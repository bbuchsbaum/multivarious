This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: R/**/*.R, R/**/*.r, *.Rmd, *.rmd, DESCRIPTION, tests/**/*.R, tests/**/*.r
- Files matching patterns in .gitignore are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
R/
  all_generic.R
  bi_projector_union.R
  bi_projector.R
  bootstrap.R
  classifier.R
  composed_projector.R
  cPCA.R
  cv.R
  discriminant_projector.R
  geneig.R
  multiblock.R
  nystrom_embedding.R
  pca.R
  pre_process.R
  projector.R
  regress.R
  svd.R
  twoway_projector.R
  utils.R
tests/
  testthat/
    test_biprojector_union.R
    test_bootstrap.R
    test_classifier.R
    test_cpca.R
    test_cross_projector.R
    test_cv.R
    test_discriminant_projector.R
    test_geneig.R
    test_nystrom.R
    test_pca.R
    test_preprocess.R
    test_projector.R
    test_regress.R
    test_svd.R
  testthat.R
DESCRIPTION
README.Rmd
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="R/bootstrap.R">
#' Fast, Exact Bootstrap for PCA Results from `pca` function
#'
#' Performs bootstrap resampling for Principal Component Analysis (PCA) based on
#' the method described by Fisher et al. (2016), optimized for high-dimensional
#' data (p >> n). This version is specifically adapted to work with the output
#' object generated by the provided `pca` function (which returns a `bi_projector`
#' object of class 'pca').
#'
#' @param x An object of class 'pca' as returned by the provided `pca` function.
#'   It's expected to contain loadings (`v`), scores (`s`), singular values (`sdev`),
#'   left singular vectors (`u`), and pre-processing info (`preproc`).
#' @param nboot The number of bootstrap resamples to perform (default: 100).
#' @param k The number of principal components to bootstrap (default: all
#'   components available in the fitted PCA model `x`). Must be less than or
#'   equal to the number of components in `x`.
#' @param parallel Logical flag indicating whether to use parallel processing
#'   via the `future` framework (default: FALSE). Requires the `future.apply` package
#'   and a configured `future` backend (e.g., `future::plan(future::multisession)`).
#' @param cores The number of cores to use for parallel processing if `parallel = TRUE`
#'   (default: `future::availableCores()`). This is used if no `future` plan is set.
#' @param seed An integer value for the random number generator seed for
#'   reproducibility (default: NULL, no seed is set).
#' @param epsilon A small positive value added to standard deviations before
#'   division to prevent division by zero or instability (default: 1e-15).
#' @param ... Additional arguments (currently ignored).
#'
#' @return A `list` object of class `bootstrap_pca_result` containing:
#'   \item{E_Vb}{Matrix (p x k) of the estimated bootstrap means of the principal components (loadings V^b = coefficients).}
#'   \item{sd_Vb}{Matrix (p x k) of the estimated bootstrap standard deviations of the principal components (loadings V^b).}
#'   \item{z_loadings}{Matrix (p x k) of the bootstrap Z-scores for the loadings, calculated as `E_Vb / sd_Vb`.}
#'   \item{E_Scores}{Matrix (n x k) of the estimated bootstrap means of the principal component scores (S^b).}
#'   \item{sd_Scores}{Matrix (n x k) of the estimated bootstrap standard deviations of the principal component scores (S^b).}
#'   \item{z_scores}{Matrix (n x k) of the bootstrap Z-scores for the scores, calculated as `E_Scores / sd_Scores`.}
#'   \item{E_Ab}{Matrix (k x k) of the estimated bootstrap means of the internal rotation matrices A^b.}
#'   \item{Ab_array}{Array (k x k x nboot) containing all the bootstrap rotation matrices A^b.}
#'   \item{Scores_array}{Array (n x k x nboot) containing all the bootstrap score matrices (S^b, with NAs for non-sampled subjects).}
#'   \item{nboot}{The number of bootstrap samples used (successful ones).}
#'   \item{k}{The number of components bootstrapped.}
#'   \item{call}{The matched call to the function.}
#'
#' @details
#' This function implements the fast bootstrap PCA algorithm proposed by
#' Fisher et al. (2016), adapted for the output structure of the provided `pca` function.
#' The `pca` function returns an object containing:
#' \itemize{
#'   \item `v`: Loadings (coefficients, p x k) - equivalent to V in SVD Y = U D V'. Note the transpose difference from `prcomp`.
#'   \item `s`: Scores (n x k) - calculated as U %*% D.
#'   \item `sdev`: Singular values (vector of length k) - equivalent to d.
#'   \item `u`: Left singular vectors (n x k).
#' }
#'
#' The bootstrap algorithm works by resampling the *subjects* (rows) and recomputing
#' the SVD on a low-dimensional representation. Specifically, it computes the SVD
#' of the resampled matrix `D U' P^b`, where `Y = U D V'` is the SVD of the original
#' (pre-processed) data, and `P^b` is a resampling matrix operating on the subjects (columns of U').
#'
#' The SVD of the resampled low-dimensional matrix is `svd(D U' P^b) = A^b S^b (R^b)'`.
#' The bootstrap principal components (loadings) are then calculated as `V^b = V A^b`,
#' and the bootstrap scores are `Scores^b = R^b S^b`.
#'
#' Z-scores are provided as `mean / sd`.
#'
#' **Important Note:** The algorithm assumes the data `Y` used for the *original* SVD (`Y = U D V'`)
#' was appropriately centered (or pre-processed according to `x$preproc`). The bootstrap
#' samples are generated based on the components derived from this pre-processed data.
#'
#' @references
#' Fisher, Aaron, Brian Caffo, Brian Schwartz, and Vadim Zipunnikov. 2016.
#' "Fast, Exact Bootstrap Principal Component Analysis for P > 1 Million."
#' \emph{Journal of the American Statistical Association} 111 (514): 846–60.
#' \doi{10.1080/01621459.2015.1062383}.
#'
#' @export
#' @family pca bootstrap
#' @examples
#' # --- Assuming the pca and svd_wrapper functions provided are loaded ---
#' # Also assuming helper functions like center(), pass(), prep(), init_transform()
#' # are defined and available. Create simplified mocks if needed:
#' center <- function() structure(list(scale=FALSE, center=TRUE), class=c("center", "prepper"))
#' pass <- function() structure(list(scale=FALSE, center=FALSE), class=c("pass", "prepper"))
#' prep <- function(preproc) {
#'    xfun <- function(X) scale(X, center = preproc$center, scale = preproc$scale)
#'    # Add proper reverse logic if needed for projection
#'    return(structure(list(transform = xfun, reverse = function(X) X), class="prep"))
#' }
#' init_transform <- function(proc, X) proc$transform(X)
#' bi_projector <- function(v, s, sdev, u, preproc, classes, method) {
#'     structure(list(v=v, s=s, sdev=sdev, u=u, preproc=preproc, method=method),
#'               class=c(classes, "bi_projector", "projector"))
#' }
#' coefficients.projector <- function(x,...) x$v
#' scores.projector <- function(x,...) x$s
#' sdev.projector <- function(x,...) x$sdev
#' # --- End Mock Helpers ---
#'
#' # Simulate data (p=50, n=20)
#' set.seed(123)
#' p_dim <- 50
#' n_obs <- 20
#' Y_mat <- matrix(rnorm(p_dim * n_obs), nrow = p_dim, ncol = n_obs)
#' # Transpose for pca function input (n x p)
#' X_mat <- t(Y_mat)
#'
#' # Perform PCA using the provided pca function
#' # Use center() pre-processing
#' pca_res <- pca(X_mat, ncomp = 5, preproc = center(), method = "fast")
#'
#' # Run bootstrap on the pca result
#' boot_res <- bootstrap_pca(pca_res, nboot = 50, k = 5, seed = 456)
#'
#' # Explore results
#' print(dim(boot_res$z_loadings)) # p x k Z-scores for loadings (coefficients)
#' print(dim(boot_res$z_scores))   # n x k Z-scores for scores
#'
#' # Plot std dev for first loading vector (V)
#' plot(boot_res$sd_Vb[, 1], ylab = "Bootstrap SD", main = "Loading Vector 1 Variability")
#'
#' # Plot std dev for first score vector (S)
#' plot(boot_res$sd_Scores[, 1], ylab = "Bootstrap SD", main = "Score Vector 1 Variability")
#'
#' \dontrun{
#' # Example with parallel processing (requires future & future.apply)
#' # library(future)
#' # library(future.apply)
#' # plan(multisession) # Set up parallel backend
#' boot_res_parallel <- bootstrap_pca(pca_res, nboot = 100, k = 5, seed = 456,
#'                                  parallel = TRUE)
#' }
#' @importFrom stats sd
#' @importFrom stats cov
bootstrap_pca <- function(x, nboot = 100, k = NULL,
                          parallel = FALSE, cores = NULL,
                          seed = NULL, epsilon = 1e-15, ...) {

  # --- Input Validation and Setup ---
  if (!inherits(x, "pca") || !inherits(x, "bi_projector")) {
      warning("Input 'x' is not of class 'pca' and 'bi_projector' as returned by the 'pca' function. Ensure it has components 'v', 's', 'sdev', and 'u'.")
  }
  required_comps <- c("v", "s", "sdev", "u")
  if (!all(required_comps %in% names(x))) {
      stop("Input object 'x' is missing required components: ",
           paste(required_comps[!required_comps %in% names(x)], collapse=", "))
  }

  # Determine dimensions from the pca object structure
  p <- nrow(x$v)       # Number of features/dimensions (from loadings V)
  n <- nrow(x$s)       # Number of subjects/observations (from scores S = UD)
  k_max <- ncol(x$v)   # Max components available in x

  if (is.null(k)) {
      k <- k_max
      message("Parameter 'k' not specified. Using k = ", k, " components from the PCA object.")
  }

  if (k > k_max) {
    stop("Requested k (", k, ") exceeds the number of components available in the PCA object (", k_max, ").")
  }
   if (k > n) {
      warning("Requested k (", k, ") exceeds the number of observations (", n, "). Results might be unstable.")
  }
  if (k <= 0 || !is.numeric(k) || k != round(k)) stop("k must be a positive integer.")

  # Seed handling with future_lapply requires specific argument
  # if (!is.null(seed)) withr::local_seed(seed) # Apply seed locally before loop if not parallel

  # Get original PCA results (up to k components) from the 'pca' object
  V <- x$v[, 1:k, drop = FALSE]        # Loadings (p x k)
  Scores_UD <- x$s[, 1:k, drop = FALSE] # Scores (n x k) - Note: x$s is U*D
  d <- x$sdev[1:k]                     # Standard deviations (singular values)
  U_svd <- x$u[, 1:k, drop = FALSE]    # Left singular vectors (n x k) - directly available

  # --- Sanity check: Scores_UD should approximate U_svd %*% diag(d) ---
  # reconstruct_scores <- U_svd %*% diag(d, nrow=k, ncol=k)
  # if (max(abs(Scores_UD - reconstruct_scores)) > sqrt(.Machine$double.eps)) {
  #    warning("Internal consistency check failed: x$s does not seem to be x$u %*% diag(x$sdev). Ensure 'pca' object structure is correct.")
  # }
  # --- End Sanity Check ---

  # Calculate the matrix D U' needed for resampling (k x n)
  DUt <- diag(d, nrow = k, ncol = k) %*% t(U_svd) # Correct: D %*% U'

  # --- Bootstrap Core Function ---
  # (Identical to the previous version, operates on derived DUt, n, k)
  svd_one_bootstrap_sample <- function(iter) {
    idx <- sample.int(n, size = n, replace = TRUE)
    DUtPb <- DUt[, idx, drop = FALSE]

    sv <- tryCatch(
      svd(DUtPb, nu = k, nv = k),
      error = function(e) {
        warning("SVD failed for bootstrap sample ", iter, ". Returning NULL. Error: ", e$message, call. = FALSE)
        return(NULL)
      }
    )
    if (is.null(sv)) return(NULL)

    actual_k_svd <- min(k, length(sv$d)) # Number of non-zero singular values returned

    # Handle cases where SVD returns fewer than k components robustly
    Ab <- matrix(0.0, nrow = k, ncol = k)
    Rb <- matrix(0.0, nrow = n, ncol = k) # R^b refers to the right vectors of svd(DUtPb)
    Sb_vals <- numeric(k)

    if (actual_k_svd > 0) {
        Ab[1:k, 1:actual_k_svd] <- sv$u[, 1:actual_k_svd, drop = FALSE]
        Rb[, 1:actual_k_svd] <- sv$v[, 1:actual_k_svd, drop = FALSE]
        Sb_vals[1:actual_k_svd] <- sv$d[1:actual_k_svd]
    } else {
        warning("SVD returned zero components for bootstrap sample ", iter, ". Results for this sample will be zero/NA.", call. = FALSE)
    }

    # --- Sign Correction ---
    diag_Ab <- diag(Ab)
    signs <- sign(diag_Ab)
    signs[signs == 0] <- 1
    Ab <- sweep(Ab, 2, signs, "*")
    Rb <- sweep(Rb, 2, signs, "*")

    # --- Calculate Bootstrap Scores (S^b = R^b S^b_vals) ---
    Scores_b_unordered <- sweep(Rb, 2, Sb_vals, "*")

    # Reconstruct full n x k score matrix with NAs for non-sampled subjects
    Scores_b <- matrix(NA_real_, nrow = n, ncol = k)
    sampled_indices_map <- split(seq_along(idx), idx) # Map original index to positions in bootstrap sample
    for (orig_idx_str in names(sampled_indices_map)) {
       orig_idx <- as.integer(orig_idx_str)
       rows_in_boot <- sampled_indices_map[[orig_idx_str]]
       # Average scores if an original subject was selected multiple times
       Scores_b[orig_idx, ] <- colMeans(Scores_b_unordered[rows_in_boot, , drop = FALSE])
    }

    return(list(Ab = Ab, Scores = Scores_b))
  }

  # --- Run Bootstrap Loop ---
  if (parallel) {
    if (!requireNamespace("future.apply", quietly = TRUE)) {
      stop("Package 'future.apply' needed for parallel processing. Please install it.", call. = FALSE)
    }
    # Setup future plan if not already set, using specified cores or default
    if (is.null(future::plan("list")[[1]]$workers)) { # Check if a plan with workers is set
        num_cores <- if (!is.null(cores)) cores else future::availableCores()
        message("Setting future plan to multisession with ", num_cores, " workers.")
        future::plan(future::multisession, workers = num_cores)
        # Ensure plan is reset on exit if we set it here
        existing_plan <- future::plan("list")
        on.exit(future::plan(existing_plan), add = TRUE)
    }
    # Use future_lapply - seed needs to be handled via future.seed argument
    res_list <- future.apply::future_lapply(1:nboot, svd_one_bootstrap_sample, future.seed = seed)
  } else {
    # Set seed locally if not running in parallel
    if (!is.null(seed)) withr::local_seed(seed)
    res_list <- lapply(1:nboot, svd_one_bootstrap_sample)
  }

  # Filter out NULL results
  failed_samples <- sum(sapply(res_list, is.null))
  if (failed_samples > 0) {
      warning("SVD failed for ", failed_samples, " out of ", nboot, " bootstrap samples. These were excluded.", call.=FALSE)
      res_list <- res_list[!sapply(res_list, is.null)]
      nboot_actual <- length(res_list)
      if (nboot_actual == 0) stop("SVD failed for all bootstrap samples.")
  } else {
      nboot_actual <- nboot
  }


  # --- Aggregate Results ---
  Ab_array <- simplify2array(lapply(res_list, `[[`, "Ab"))       # k x k x nboot_actual
  Scores_array <- simplify2array(lapply(res_list, `[[`, "Scores")) # n x k x nboot_actual

  # --- Calculate Bootstrap Moments and Z-scores ---

  # 1. Scores
  E_Scores <- apply(Scores_array, 1:2, mean, na.rm = TRUE)
  sd_Scores <- apply(Scores_array, 1:2, sd, na.rm = TRUE)
  sd_Scores[is.na(sd_Scores) | sd_Scores < epsilon] <- epsilon # Handle NAs and apply epsilon
  z_scores <- E_Scores / sd_Scores

  # 2. Loadings (Vb = V Ab)
  E_Ab <- apply(Ab_array, 1:2, mean)
  E_Vb <- V %*% E_Ab

  # Calculate Variance/SD of Loadings Vb using Cov(Ab_k)
  Var_Vb <- matrix(NA_real_, nrow = p, ncol = k)
  if (p > 0 && !is.null(rownames(V))) rownames(Var_Vb) <- rownames(V)
  if (k > 0 && !is.null(colnames(V))) colnames(Var_Vb) <- colnames(V)

  if (nboot_actual <= 1) {
      warning("Cannot compute variance with <= 1 successful bootstrap sample. Returning NA for loading SD and Z-scores.", call.=FALSE)
      sd_Vb <- Var_Vb # Matrix of NAs
      z_loadings <- Var_Vb # Matrix of NAs
  } else {
      for (ki in 1:k) {
        # Transpose needed: Ab_array is k x k x nboot, we need nboot x k for cov
        Ab_k_samples <- t(Ab_array[, ki, ])
        Cov_Ab_k <- cov(Ab_k_samples)

        # Efficient calculation using rowSums
        V_Cov_k <- V %*% Cov_Ab_k
        Term_k <- V_Cov_k * V
        Var_Vb[, ki] <- rowSums(Term_k)
      }

      Var_Vb[Var_Vb < 0 & !is.na(Var_Vb)] <- 0 # Set small negative variances to 0
      sd_Vb <- sqrt(Var_Vb)
      sd_Vb[is.na(sd_Vb) | sd_Vb < epsilon] <- epsilon # Handle NAs and apply epsilon
      z_loadings <- E_Vb / sd_Vb
  }

  # --- Prepare Return Object ---
  ret <- list(
    E_Vb = E_Vb,
    sd_Vb = sd_Vb,
    z_loadings = z_loadings,
    E_Scores = E_Scores,
    sd_Scores = sd_Scores,
    z_scores = z_scores,
    E_Ab = E_Ab,
    Ab_array = Ab_array,
    Scores_array = Scores_array,
    nboot = nboot_actual,
    k = k,
    call = match.call()
  )

  class(ret) <- c("bootstrap_pca_result", "list")
  return(ret)
}

# Keep the print method as it is generic enough
#' Print method for bootstrap_pca_result
#'
#' @param x An object of class `bootstrap_pca_result`.
#' @param ... Additional arguments passed to `print`.
#' @export
#' @keywords internal
print.bootstrap_pca_result <- function(x, ...) {
  cat("Fast Bootstrap PCA Result\n")
  cat("--------------------------\n")
  cat("Call:\n")
  print(x$call)
  cat("\n")
  cat("Number of bootstrap samples (successful):", x$nboot, "\n")
  cat("Number of components bootstrapped (k):", x$k, "\n")
  cat("\n")
  cat("Output contains bootstrap estimates for:\n")
  cat(" - Loadings (Vb = coefficients): Mean (E_Vb), SD (sd_Vb), Z-scores (z_loadings)\n")
  cat(" - Scores (Sb): Mean (E_Scores), SD (sd_Scores), Z-scores (z_scores)\n")
  cat(" - Internal rotation matrices (Ab): Mean (E_Ab), Full array (Ab_array)\n")
  cat(" - Full bootstrap scores array (Scores_array)\n")
  invisible(x)
}
</file>

<file path="R/cv.R">
#' Compute reconstruction-based error metrics
#'
#' Given two numeric matrices \code{Xtrue} and \code{Xrec}, compute:
#'  \itemize{
#'    \item MSE  (\code{"mse"})
#'    \item RMSE (\code{"rmse"})
#'    \item R^2  (\code{"r2"})
#'    \item MAE  (\code{"mae"})
#'  }
#'
#' @param Xtrue Original data matrix, shape (n x p).
#' @param Xrec  Reconstructed data matrix, shape (n x p).
#' @param metrics Character vector of metric names, e.g. \code{c("mse","rmse","r2","mae")}.
#' @param by_column Logical, if TRUE calculate R2 metric per column and average (default: FALSE).
#' @return A one-row \code{tibble} with columns matching \code{metrics}.
#' @export
measure_reconstruction_error <- function(Xtrue, Xrec, metrics = c("mse","rmse","r2"), by_column = FALSE) {
  stopifnot(is.matrix(Xtrue), is.matrix(Xrec))
  stopifnot(all(dim(Xtrue) == dim(Xrec)))
  
  diff <- Xtrue - Xrec
  sq   <- diff^2
  mse  <- mean(sq, na.rm = TRUE)
  rmse <- sqrt(mse)
  
  # R^2 calculation
  if (by_column) {
    # Column-wise R^2: Calculate R^2 for each column, then average
    col_means_true <- colMeans(Xtrue, na.rm = TRUE)
    sstot_col <- colSums(sweep(Xtrue, 2, col_means_true, "-")^2, na.rm = TRUE)
    ssres_col <- colSums(sq, na.rm = TRUE)
    
    # Avoid division by zero or near-zero for columns with no variance
    r2_col <- ifelse(sstot_col < 1e-15, NA_real_, 1 - ssres_col / sstot_col)
    
    r2 <- mean(r2_col, na.rm = TRUE) # Average the per-column R^2 values
  } else {
    # Original R^2: 1 - SSres / SStot (using grand mean)
    sstot <- sum((Xtrue - mean(Xtrue, na.rm = TRUE))^2, na.rm = TRUE)
    ssres <- sum(sq, na.rm = TRUE)
    r2 <- if (sstot < 1e-15) NA_real_ else (1 - ssres / sstot)
  }
  
  mae <- mean(abs(diff), na.rm = TRUE)
  
  out_vals <- list()
  for (m in metrics) {
    val <- switch(m,
                  "mse"  = mse,
                  "rmse" = rmse,
                  "r2"   = r2,
                  "mae"  = mae,
                  stop("Unknown metric: ", m)
    )
    out_vals[[m]] <- val
  }
  tibble::as_tibble(out_vals)
}

#' Compute inter-block transfer error metrics for a cross_projector
#'
#' We measure how well the model can transfer from X->Y or Y->X, e.g. "x2y.mse".
#'
#' The metric names are of the form "x2y.mse", "x2y.rmse", "y2x.r2", etc.
#'
#' @param Xtrue The X block test data
#' @param Ytrue The Y block test data
#' @param model The fitted \code{cross_projector}
#' @param metrics A character vector like \code{c("x2y.mse","y2x.r2")}
#' @return A 1-row tibble with columns for each requested metric
#' @export
measure_interblock_transfer_error <- function(Xtrue, Ytrue, model,
                                              metrics = c("x2y.mse")) {
  
  # Initialize output with NA for all requested metrics for predictable shape
  out_list <- stats::setNames(rep(NA_real_, length(metrics)), metrics)
  
  # Helper to do X->Y:
  do_x2y <- function() {
    requested_x2y <- grep("^x2y\\.", metrics, value = TRUE)
    if (length(requested_x2y) == 0) return()
    
    base_metrics <- gsub("^x2y\\.", "", requested_x2y)
    
    tryCatch({
      Ypred <- transfer(model, Xtrue, source = "X", target = "Y")
      # Assuming measure_reconstruction_error handles missing metrics gracefully
      subres <- measure_reconstruction_error(Ytrue, Ypred, metrics = base_metrics)
      # Update out_list only for successfully computed metrics
      for (nm in names(subres)) {
        out_list[[paste0("x2y.", nm)]] <- subres[[nm]]
      }
    }, error = function(e) {
      warning("Error during X->Y transfer or measurement: ", conditionMessage(e))
      # NAs for these metrics remain in out_list
    })
  }
  
  # Helper to do Y->X:
  do_y2x <- function() {
    requested_y2x <- grep("^y2x\\.", metrics, value = TRUE)
    if (length(requested_y2x) == 0) return()
    
    base_metrics <- gsub("^y2x\\.", "", requested_y2x)
    
    tryCatch({
      Xpred <- transfer(model, Ytrue, source = "Y", target = "X")
      subres <- measure_reconstruction_error(Xtrue, Xpred, metrics = base_metrics)
      for (nm in names(subres)) {
        out_list[[paste0("y2x.", nm)]] <- subres[[nm]]
      }
    }, error = function(e) {
      warning("Error during Y->X transfer or measurement: ", conditionMessage(e))
      # NAs remain
    })
  }
  
  # If user asked for x2y.* metrics
  if (any(startsWith(metrics, "x2y."))) {
      do_x2y()
  }
  # If user asked for y2x.* metrics
  if (any(startsWith(metrics, "y2x."))) {
      do_y2x()
  }
  
  tibble::as_tibble(out_list)
}



#' Generic cross-validation engine
#'
#' For each fold (train/test indices):
#'   \enumerate{
#'     \item Subset \code{data[train, ]}
#'     \item Fit a model with \code{.fit_fun(train_data, ...)}
#'     \item Evaluate with \code{.measure_fun(model, test_data, ...)}
#'   }
#'
#' @param data A matrix or data.frame of shape (n x p).
#' @param folds A list of folds, each a list with \code{$train} and \code{$test}.
#' @param .fit_fun Function: signature \code{function(train_data, ...)\{\}}. Returns a fitted model.
#' @param .measure_fun Function: signature \code{function(model, test_data, ...)\{\}}. Returns a tibble or named list/vector of metrics.
#' @param fit_args A list of additional named arguments passed to \code{.fit_fun}.
#' @param measure_args A list of additional named arguments passed to \code{.measure_fun}.
#' @param backend Character string: "serial" (default) or "future" for parallel execution using the `future` framework.
#' @param ... Currently ignored (arguments should be passed via `fit_args` or `measure_args`).
#' @return A tibble with columns:
#'   \item{fold}{integer fold index}
#'   \item{model}{list of fitted models}
#'   \item{metrics}{list of metric tibbles/lists}
#' @importFrom dplyr bind_rows
#' @importFrom tibble tibble
#' @export
cv_generic <- function(data, folds, .fit_fun, .measure_fun, 
                       fit_args = list(), measure_args = list(), 
                       backend = c("serial", "future"), ...) {
  
  backend <- match.arg(backend)
  
  # Function to process a single fold
  process_fold <- function(i, data, folds, .fit_fun, .measure_fun, fit_args, measure_args) {
    train_idx <- folds[[i]]$train
    test_idx  <- folds[[i]]$test
    
    train_data <- data[train_idx, , drop=FALSE]
    test_data  <- data[test_idx, , drop=FALSE]
    
    # Call fit function with arguments, wrapped in tryCatch
    model_or_error <- tryCatch({
      do.call(.fit_fun, c(list(train_data), fit_args))
    }, error = function(e) {
      warning("Fitting failed for fold ", i, ": ", conditionMessage(e))
      e # Return the error object
    })
    
    # Call measure function with arguments, wrapped in tryCatch
    metrics_or_error <- if (inherits(model_or_error, "error")) {
      # If fitting failed, create an error entry for metrics
      tibble::tibble(error = paste("Fit failed:", conditionMessage(model_or_error)))
    } else {
      # If fitting succeeded, try measuring
      tryCatch({
        do.call(.measure_fun, c(list(model_or_error, test_data), measure_args))
      }, error = function(e) {
        warning("Measuring failed for fold ", i, ": ", conditionMessage(e))
        # Return a tibble containing the error message
        tibble::tibble(error = conditionMessage(e)) 
      })
    }
    
    # Ensure metrics is a list or coercible, handle potential error tibble
    metrics_tibble <- if (tibble::is_tibble(metrics_or_error)) {
      metrics_or_error
    } else if (is.list(metrics_or_error) && !is.data.frame(metrics_or_error)) {
      # Original handling for list output from measure_fun
      tibble::as_tibble(metrics_or_error)
    } else {
      # Handle vector or other coercible types
      tibble::as_tibble(as.list(metrics_or_error))
    }
    
    # Store model or error object
    model_result <- if (inherits(model_or_error, "error")) list(NULL) else list(model_or_error)
    
    tibble::tibble(
      fold    = i,
      model   = model_result, # Store model or NULL if error
      metrics = list(metrics_tibble) # Store metrics tibble or error tibble
    )
  }
  
  # Execute based on backend
  if (backend == "future") {
      if (!requireNamespace("future.apply", quietly = TRUE)) {
          stop("Package 'future.apply' required for backend='future'. Please install it.", call. = FALSE)
      }
      # future_lapply automatically handles exports typically
      res_list <- future.apply::future_lapply(seq_along(folds), process_fold, 
                                                data=data, folds=folds, 
                                                .fit_fun=.fit_fun, .measure_fun=.measure_fun,
                                                fit_args=fit_args, measure_args=measure_args,
                                                future.seed = TRUE) # Use future's RNG seeding
  } else {
      res_list <- lapply(seq_along(folds), process_fold, 
                         data=data, folds=folds, 
                         .fit_fun=.fit_fun, .measure_fun=.measure_fun,
                         fit_args=fit_args, measure_args=measure_args)
  }
  
  dplyr::bind_rows(res_list)
}




# cv.bi_projector <- function(x,
#                                   folds,
#                                   max_comp = 5,
#                                   fit_fun  = NULL,
#                                   measure  = c("mse","rmse","r2"),
#                                   measure_fun = NULL,
#                                   return_models = FALSE,
#                                   ...) # Capture extra args for fit/measure fun
# {
#   # 1) Provide a default fit_fun if none:
#   if (is.null(fit_fun)) {
#     
#     
#     fit_fun <- function(X_train, ncomp = 2, ...) {
#       pr <- prep(center()) # Use the standard preprocessor framework
#       Xtr <- init_transform(pr, X_train)
#       svdres <- svd(Xtr, nu = ncomp, nv = ncomp)
#       bi_projector(
#         v     = svdres$v,
#         s     = svdres$u %*% diag(svdres$d[1:ncomp]),
#         sdev  = svdres$d[1:ncomp],
#         preproc = pr # Pass the properly initialized preprocessor
#       )
#     }
#   }
#   
#   # If formula, convert
#   if (rlang::is_formula(fit_fun)) {
#     formula_fun <- rlang::as_function(fit_fun)
#     .fit_fun <- function(X_train, ncomp, ...) {
#       # pass X_train as .x, plus ncomp
#       formula_fun(X_train, ncomp=ncomp, ...)
#     }
#   } else if (is.function(fit_fun)) {
#     .fit_fun <- fit_fun
#   } else {
#     stop("`fit_fun` must be a function or one-sided formula.")
#   }
#   
#   # Capture extra arguments intended for fit_fun or measure_fun
#   extra_args <- list(...)
#   
#   # If measure_fun not provided, define default with measure_reconstruction_error
#   if (!is.null(measure_fun)) {
#     .measure_fun <- measure_fun
#   } else {
#     .measure_fun <- function(model, X_test, ...) {
#       # Reconstruct requires the model's preprocessor state to be applied
#       # reconstruct_new handles projection and reverse_transform internally.
#       # Original and correct logic: compare X_test to the original-space reconstruction
#       # provided by reconstruct_new (which now consistently includes reverse_transform)
#       X_rec <- reconstruct_new(model, X_test, comp = 1:ncomp(model)) # Use all comps in model_k
#       measure_reconstruction_error(X_test, X_rec, metrics = measure)
#     }
#   }
#   
#   # We'll do our own cross-validation loop manually, 
#   # because we want to fit multiple ncomp per fold.
#   results_list <- vector("list", length(folds))
#   
#   for (i in seq_along(folds)) {
#     train_idx <- folds[[i]]$train
#     test_idx  <- folds[[i]]$test
#     
#     X_train <- x[train_idx, , drop=FALSE]
#     X_test  <- x[test_idx, , drop=FALSE]
#     
#     # Fit model once with max components
#     model_full <- tryCatch({
#         do.call(.fit_fun, c(list(X_train, ncomp=max_comp), extra_args))
#       }, error = function(e) {
#           warning("Fitting model failed for fold ", i, ": ", e$message)
#           return(NULL)
#       })
#       
#     if (is.null(model_full)) {
#         comp_list <- list(tibble::tibble(comp=1:max_comp, error=NA)) # Placeholder if fit failed
#     } else {
#         # Store metrics for each truncated component level
#         comp_list <- vector("list", max_comp)
#         for (k in seq_len(max_comp)) {
#           model_k <- tryCatch({ 
#               truncate(model_full, k)
#             }, error = function(e) {
#                 warning("Truncating model failed for fold ", i, " k=", k, ": ", e$message)
#                 return(NULL)
#             })
# 
#           if(is.null(model_k)) {
#              metrics_k <- tibble::tibble(comp_error = "Truncation failed")
#           } else {
#               metrics_k <- tryCatch({ 
#                  do.call(.measure_fun, c(list(model_k, X_test), extra_args))
#                 }, error = function(e) {
#                     warning("Measuring metrics failed for fold ", i, " k=", k, ": ", e$message)
#                     tibble::tibble(comp_error = paste("Measure failed:", conditionMessage(e)))
#                 })
#           }
# 
#           # Store a row that includes comp index and each metric
#           comp_list[[k]] <- tibble::tibble(comp = k) %>% 
#              dplyr::bind_cols(tibble::as_tibble(metrics_k)) # Ensure metrics_k is tibble
#         }
#     }
#     
#     comp_tib <- dplyr::bind_rows(comp_list)  # shape: (max_comp x (1 + #metrics))
#     
#     # We only store one "row" in the results for fold i,
#     # with a nested tibble in column "component_metrics".
#     fold_result <- tibble::tibble(
#       fold              = i,
#       component_metrics = list(comp_tib)
#     )
#     if (return_models) {
#         fold_result$model_full <- list(model_full)
#     }
#     results_list[[i]] <- fold_result
#   }
#   
#   raw_results <- dplyr::bind_rows(results_list)
#   
#   # Wrap in cv_fit
#   ret <- structure(
#     list(results=raw_results),
#     class="cv_fit"
#   )
#   ret
# }

###############################################################################
## S3 summary.cv_fit -- we'll add a new approach
###############################################################################

# @export
# summary.cv_fit <- function(object, ...) {
#   df <- object$results
#   
#   # If we have "component_metrics" nested, unnest that. 
#   # If we have a "metrics" column (older style), handle that too.
#   
#   if ("component_metrics" %in% names(df)) {
#     # new approach with per-component
#     # we unnest, then summarize across folds for each comp
#     df_unnest <- tidyr::unnest(df, cols="component_metrics")
#     # columns might be: fold, comp, mse, rmse, r2, ... or comp_error
#     # we want group_by comp, then compute mean
#     metric_cols <- setdiff(names(df_unnest), c("fold","comp"))
#     # Summarize only numeric metrics, ignoring potential error columns
#     numeric_metric_cols <- metric_cols[sapply(df_unnest[metric_cols], is.numeric)]
#     if (length(numeric_metric_cols) == 0) {
#       return(tibble::tibble(note="No numeric metrics found to summarize."))
#     }
#     out <- df_unnest %>%
#       dplyr::group_by(.data$comp) %>%
#       dplyr::summarize(
#         dplyr::across(dplyr::all_of(numeric_metric_cols), ~ mean(.x, na.rm=TRUE)),
#         .groups="drop"
#       )
#     return(out)
#     
#   } else if ("metrics" %in% names(df)) {
#     # old style - handle potential error columns here too
#     df_unnest <- tidyr::unnest(df, cols="metrics")
#     metric_cols <- setdiff(names(df_unnest), c("fold","model"))
#     # Summarize only numeric metrics
#     numeric_metric_cols <- metric_cols[sapply(df_unnest[metric_cols], is.numeric)]
#     if (length(numeric_metric_cols) == 0) {
#       return(tibble::tibble(note="No numeric metrics found to summarize."))
#     }
#     out <- df_unnest %>%
#       dplyr::summarize(
#         dplyr::across(dplyr::all_of(numeric_metric_cols), ~ mean(.x, na.rm=TRUE))
#       )
#     return(out)
#   } else {
#     # No metrics at all
#     tibble::tibble(note="No metrics found.")
#   }
# }

###############################################################################
## The rest: print.cv_fit, plot.cv_fit remain the same or updated to handle multi-comp
###############################################################################

#' @export
# print.cv_fit <- function(x, ...) {
#   df <- x$results
#   cat(crayon::bold(crayon::blue("Cross-validation fit object\n")))
#   cat(crayon::silver("  Number of folds: "), nrow(df), "\n", sep="")
#   
#   if ("component_metrics" %in% names(df)) {
#     cat(crayon::silver("  Contains per-component metrics in 'component_metrics' column.\n"))
#     cat(crayon::silver("  Use summary() to see aggregated metrics.\n"))
#   } else if ("metrics" %in% names(df)) {
#     df_unnest <- tryCatch(tidyr::unnest(df, cols="metrics"), error=function(e) NULL)
#     if (!is.null(df_unnest)) {
#       metric_cols <- setdiff(names(df_unnest), c("fold","model"))
#       cat(crayon::silver("  Metrics: "), paste(metric_cols, collapse=", "), "\n", sep="")
#     }
#     cat(crayon::silver("  Use summary() to see aggregated metrics.\n"))
#     cat(crayon::silver("  Use plot() to visualize metrics by fold.\n"))
#   } else {
#     cat(crayon::silver("  No known metrics columns found.\n"))
#   }
#   
#   invisible(x)
# }

#' @export
# plot.cv_fit <- function(x, metric=NULL, ...) {
#   if (!requireNamespace("ggplot2", quietly=TRUE)) {
#     stop("`plot.cv_fit` requires ggplot2. Please install it.")
#   }
#   
#   df <- x$results
#   
#   if ("component_metrics" %in% names(df)) {
#     # new multi-comp approach
#     df_unnest <- tidyr::unnest(df, cols="component_metrics")
#     # possible columns: fold, comp, mse, rmse, r2, ...
#     numeric_cols <- setdiff(names(df_unnest)[sapply(df_unnest, is.numeric)], c("fold","comp"))
#     if (length(numeric_cols) == 0) {
#       stop("No numeric metrics found to plot.")
#     }
#     # If user didn't specify metric, pick first
#     if (is.null(metric)) {
#       metric <- numeric_cols[1]
#       message("Metric not specified, plotting first numeric metric found: ", metric)
#     }
#     if (!metric %in% names(df_unnest)) {
#       stop("Requested metric '", metric, "' not found among numeric columns: ",
#            paste(numeric_cols, collapse=", "))
#     }
#     # We'll do a line plot: x=comp, y=metric, color=fold
#     p <- ggplot2::ggplot(df_unnest, ggplot2::aes(x=.data$comp, y=.data[[metric]])) +
#       ggplot2::stat_summary(fun = mean, geom = "line", na.rm = TRUE, color="red", linewidth=1) +
#       ggplot2::stat_summary(fun.data = mean_se, geom = "ribbon", na.rm = TRUE, alpha=0.2, fill="red") + 
#       ggplot2::labs(x="Components", y=metric, color="Fold") +
#       ggplot2::theme_minimal() +
#       ggplot2::ggtitle("Cross-validation performance by components",
#                        subtitle="Red line = Mean across folds, Ribbon = +/- 1 SE")
#     return(p)
#     
#   } else if ("metrics" %in% names(df)) {
#     # old single-model approach
#     df_unnest <- tidyr::unnest(df, cols="metrics")
#     numeric_cols <- setdiff(names(df_unnest)[sapply(df_unnest, is.numeric)], "fold")
#     if (length(numeric_cols) == 0) {
#       stop("No numeric metrics found to plot.")
#     }
#     if (is.null(metric)) {
#       metric <- numeric_cols[1]
#     }
#     if (!metric %in% names(df_unnest)) {
#       stop("Requested metric '", metric, "' not found among numeric columns: ",
#            paste(numeric_cols, collapse=", "))
#     }
#     p <- ggplot2::ggplot(df_unnest, ggplot2::aes(x=factor(.data$fold), y=.data[[metric]])) +
#       ggplot2::geom_col(fill="steelblue", alpha=0.7) +
#       ggplot2::labs(x="Fold", y=metric, title=paste("Cross-validation:", metric)) +
#       ggplot2::theme_minimal()
#     return(p)
#     
#   } else {
#     stop("No metrics found to plot.")
#   }
# }
</file>

<file path="tests/testthat/test_biprojector_union.R">
library(testthat)
library(multivarious)

test_that("bi_projector_union concatenates bi_projector instances", {
  X1 <- matrix(rnorm(5 * 5), 5, 5)
  X2 <- matrix(rnorm(5 * 5), 5, 5)
  
  fit1 <- pca(X1)
  fit2 <- pca(X2)
  
  combined_fit <- bi_projector_union(list(fit1, fit2))
  
  expect_equal(dim(combined_fit$v), c(5, ncomp(fit1) + ncomp(fit2)))
  expect_equal(dim(combined_fit$s), c(5, ncomp(fit1) + ncomp(fit2)))
  expect_equal(length(combined_fit$sdev), ncomp(fit1) + ncomp(fit2))
  expect_s3_class(combined_fit, "bi_projector_union")
})

test_that("bi_projector_union with custom outer_block_indices", {
  X1 <- matrix(rnorm(5 * 5), 5, 5)
  X2 <- matrix(rnorm(5 * 5), 5, 5)
  
  fit1 <- pca(X1)
  fit2 <- pca(X2)
  
  outer_block_indices <- list(1:5, 6:10)
  
  combined_fit <- bi_projector_union(list(fit1, fit2), outer_block_indices = outer_block_indices)
  
  expect_equal(dim(combined_fit$v), c(5, ncomp(fit1) + ncomp(fit2)))
  expect_equal(dim(combined_fit$s), c(5, ncomp(fit1) + ncomp(fit2)))
  expect_equal(length(combined_fit$sdev), ncomp(fit1) + ncomp(fit2))
  expect_equal(combined_fit$outer_block_indices, outer_block_indices)
  expect_s3_class(combined_fit, "bi_projector_union")
})

test_that("bi_projector_union fails with non-bi_projector instances", {
  X1 <- matrix(rnorm(5 * 5), 5, 5)
  X2 <- matrix(rnorm(5 * 5), 5, 5)
  
  fit1 <- pca(X1)
  non_fit <- list(v = matrix(rnorm(5 * 5), 5, 5), s = matrix(rnorm(5 * 5), 5, 5))
  
  expect_error(bi_projector_union(list(fit1, non_fit)))
})

test_that("bi_projector_union fails with incorrect outer_block_indices length", {
  X1 <- matrix(rnorm(5 * 5), 5, 5)
  X2 <- matrix(rnorm(5 * 5), 5, 5)
  
  fit1 <- pca(X1)
  fit2 <- pca(X2)
  
  outer_block_indices <- list(1:4, 5:9) # Incorrect length
  
  expect_error(bi_projector_union(list(fit1, fit2), outer_block_indices = outer_block_indices))
})
</file>

<file path="tests/testthat/test_bootstrap.R">
# tests/testthat/test_bootstrap_pca.R
context("bootstrap_pca")

set.seed(123)                                       # reproducible randomness
library(multivarious)

# --- helper to build a tiny pca object quickly ------------------------------
make_toy_pca <- function(n = 25L, p = 12L, noise = .15, k = 2L) {
  s1     <- rnorm(n, sd = 3)                        # strong latent factor
  load1  <- runif(p, -1, 1)
  X      <- outer(s1, load1) + matrix(rnorm(n * p, 0, noise), n, p)
  pca(X, ncomp = k, preproc = center(), method = "fast")
}

# reuse in all tests ---------------------------------------------------------
toy_pca  <- make_toy_pca()
p        <- nrow(toy_pca$v); n <- nrow(toy_pca$s); k <- ncol(toy_pca$v)
nboot    <- 40L                                    # keep tests snappy
boot_res <- bootstrap_pca(toy_pca, nboot = nboot, k = k, seed = 999)

# -------------  1. structural integrity  ------------------------------------
test_that("bootstrap_pca returns object of correct class and shape", {

  expect_s3_class(boot_res, "bootstrap_pca_result")

  # core slots must exist
  expect_true(all(c("E_Vb", "sd_Vb", "z_loadings",
                    "E_Scores", "sd_Scores", "z_scores",
                    "Ab_array", "Scores_array") %in% names(boot_res)))

  # dimensions
  expect_equal(dim(boot_res$E_Vb),       c(p, k))
  expect_equal(dim(boot_res$sd_Vb),      c(p, k))
  expect_equal(dim(boot_res$z_loadings), c(p, k))

  expect_equal(dim(boot_res$E_Scores),   c(n, k))
  expect_equal(dim(boot_res$sd_Scores),  c(n, k))
  expect_equal(dim(boot_res$z_scores),   c(n, k))

  expect_equal(dim(boot_res$Ab_array),     c(k, k, nboot))
  expect_equal(dim(boot_res$Scores_array), c(n, k, nboot))

  # bookkeeping scalars
  expect_equal(boot_res$nboot, nboot)
  expect_equal(boot_res$k,     k)
})

# -------------  2. Z‑score identity check  ----------------------------------
test_that("Z‑scores equal mean divided by SD", {

  # helper that checks element‑wise equality up to 1e‑12 (machine precision wiggle)
  identical_Z <- function(E, SD, Z) {
    delta <- abs(E / SD - Z)
    all(delta[is.finite(delta)] < 1e-12)
  }

  expect_true(identical_Z(boot_res$E_Vb,    boot_res$sd_Vb,    boot_res$z_loadings))
  expect_true(identical_Z(boot_res$E_Scores,boot_res$sd_Scores,boot_res$z_scores))
})

# -------------  3. statistical faithfulness ---------------------------------
test_that("bootstrap means track dominant component and SD hierarchy is sensible", {

  # correlation between original and bootstrap‑mean loadings for PC1
  cor1 <- cor(toy_pca$v[, 1], boot_res$E_Vb[, 1])
  expect_true(abs(cor1) >= 0.9)

  # dominant component should be estimated more precisely
  expect_true(median(boot_res$sd_Vb[, 1]) < median(boot_res$sd_Vb[, 2]))
})
</file>

<file path="tests/testthat/test_cross_projector.R">
# tests/testthat/test_cross_projector.R
context("cross_projector core operators")

library(multivarious)
library(MASS)          # for ginv()

set.seed(1)

# helper that makes an *arbitrary but exact* latent relationship
make_blocks <- function(n  = 40L,
                        pX = 6L,
                        pY = 5L,
                        d  = 3L) {

  Vx <- qr.Q(qr(matrix(rnorm(pX * d), pX, d)))          # orthonormal pX x d
  Vy <- qr.Q(qr(matrix(rnorm(pY * d), pY, d)))          # orthonormal pY x d
  F  <- matrix(rnorm(n * d), n, d)                      # latent scores

  list(
    X  = F %*% t(Vx),                                   # exact factor model
    Y  = F %*% t(Vy),
    Vx = Vx,
    Vy = Vy,
    F  = F
  )
}


# =========================================================================
# 1. ‑‑ Constructor + accessor sanity -------------------------------------
# =========================================================================
test_that("cross_projector stores and reports correct shapes & coefficients", {

  dims <- make_blocks()
  preproc <- prep(pass())
  Xp <- init_transform(preproc, dims$X)
  cp   <- cross_projector(dims$Vx, dims$Vy, preproc_x = preproc, preproc_y = preproc)

  ## classes
  expect_s3_class(cp, c("cross_projector", "projector"))

  ## shape() respects block argument
  expect_equal(shape(cp, "X"), c(nrow(dims$Vx), ncol(dims$Vx)))
  expect_equal(shape(cp, "Y"), c(nrow(dims$Vy), ncol(dims$Vy)))

  ## coef() returns identical matrices
  expect_equal(coef(cp, "X"), dims$Vx)
  expect_equal(coef(cp, "Y"), dims$Vy)
})


# =========================================================================
# 2. ‑‑ project() and partial_project() -----------------------------------
# =========================================================================
test_that("project and partial_project yield expected factor scores", {

  dims <- make_blocks()
  preproc <- prep(pass())
  Xp <- init_transform(preproc, dims$X)
  cp   <- cross_projector(dims$Vx, dims$Vy, preproc_x = preproc, preproc_y = preproc)

  ## ----------   full projection on X   ----------
  F_hat <- project(cp, dims$X, source = "X")            # n x d
  expect_equal(dim(F_hat), c(nrow(dims$X), ncol(dims$Vx)))
  expect_true(max(abs(F_hat - dims$F)) < 1e-12)         # exact because Vx orthonormal

  ## ----------   vector input automatically reshapes   ----------
  one_vec <- dims$X[1, ]
  sc1     <- project(cp, one_vec, source = "X")          # 1 x d
  expect_equal(as.numeric(sc1), as.numeric(F_hat[1, ]))

  ## ----------   partial projection with LS inverse   ----------
  cols      <- c(2, 4, 5)                                # pick 3 out of 6 features
  X_sub     <- dims$X[, cols, drop = FALSE]
  v_sub     <- dims$Vx[cols, , drop = FALSE]
  ls_expect <- X_sub %*% v_sub %*% ginv(crossprod(v_sub))  # analytical expectation

  sc_sub <- partial_project(cp, X_sub, colind = cols,
                            source = "X", least_squares = TRUE)

  expect_equal(sc_sub, ls_expect, tolerance = 1e-3)
})


# =========================================================================
# 3. ‑‑ transfer() reconstructs other block with low error ----------------
# =========================================================================
test_that("transfer converts X‑>Y (and Y‑>X) with low reconstruction error", {

  dims <- make_blocks()
  preproc <- prep(pass())
  Xp <- init_transform(preproc, dims$X)
  cp   <- cross_projector(dims$Vx, dims$Vy, preproc_x = preproc, preproc_y = preproc)

  ## X  →  latent  →  Y
  Y_hat <- transfer(cp, dims$X, from = "X", to = "Y",
                    opts = list(ls_rr = TRUE))

  mse_xy <- mean((Y_hat - dims$Y)^2)
  expect_lt(mse_xy, 1e-10)                               # essentially exact

  ## Y  →  latent  →  X
  X_hat <- transfer(cp, dims$Y, from = "Y", to = "X",
                    opts = list(ls_rr = TRUE))

  mse_yx <- mean((X_hat - dims$X)^2)
  expect_lt(mse_yx, 1e-4)
})
</file>

<file path="tests/testthat/test_cv.R">
library(testthat)
library(tibble) # Needed for dummy_eval
# Assuming measure_reconstruction_error and cv_generic/cv.bi_projector are loaded
# May need to load the package being tested, e.g., using devtools::load_all()

set.seed(1)

# ------------------------------------------------------------------
# helper: 5x identity "model" guarantees perfect reconstruction
perfect_fit  <- function(train_data, ...)   "identity"
perfect_eval <- function(model, test_data, ...) {
  # Reconstruct identically
  Xrec <- test_data   # perfect
  # Assuming measure_reconstruction_error is available in the test env
  measure_reconstruction_error(test_data, Xrec, metrics = c("mse","r2"))
}

# Helper k-fold function (as defined in the test)
kfold_split <- function(n, k = 5) {
  idx <- sample(rep(1:k, length.out = n))
  lapply(1:k, function(j) list(train = which(idx != j),
                               test  = which(idx == j)))
}

test_that("cv_generic yields zero-error on identity model", {
  X      <- matrix(rnorm(100 * 4), 100, 4)
  folds  <- kfold_split(nrow(X))
  
  # Assuming cv_generic is available in the test env
  res <- cv_generic(
    data          = X,
    folds         = folds,
    .fit_fun      = perfect_fit,
    .measure_fun  = perfect_eval
  )
  
  # every fold must have zero mse and r2 == 1
  metrics <- do.call(rbind, lapply(res$metrics, as.data.frame))
  expect_true(all(metrics$mse < 1e-12))
  expect_true(all(abs(metrics$r2 - 1) < 1e-12))
})

# ------------------------------------------------------------------

error_fit <- function(train_data, ...) {
  stop("boom")             # deliberate failure
}

dummy_eval <- function(model, test_data, ...) {
  tibble::tibble(dummy = 0) # won't be reached
}

test_that("cv_generic gracefully records fit errors", {
  X     <- matrix(rnorm(20), 10, 2)
  folds <- list(list(train = 1:8, test = 9:10))
  
  # Assuming cv_generic is available
  res <- cv_generic(X, folds, error_fit, dummy_eval)
  
  expect_true(grepl("Fit failed", res$metrics[[1]]$error[1]))
  expect_null(res$model[[1]])
})

# ------------------------------------------------------------------

# Need prep, center, bi_projector, cv.bi_projector, reconstruct_new, truncate.bi_projector
# Also assumes required dplyr/tidyr functions are available

# test_that("cv.bi_projector produces monotonically improving MSE", {
#   # Requires functions from the package (prep, center, bi_projector, etc.)
#   # Requires dplyr and tidyr
#   skip_if_not_installed("dplyr")
#   skip_if_not_installed("tidyr")
#   
#   set.seed(1)
#   X     <- scale(matrix(rnorm(60 * 6), 60, 6))   # centred, unit-var
#   folds <- list(
#     list(train = 1:40, test = 41:60),
#     list(train = 21:60, test = 1:20)
#   )
#   
#   # Assuming cv.bi_projector is available
#   res <- cv.bi_projector(
#     x            = X,
#     folds        = folds,
#     max_comp     = 4,
#     measure      = "mse",
#     return_models= FALSE
#   )
#   
#   # Unnest and average MSE across folds per comp
#   # Ensure necessary packages are loaded for pipe operators
#   mses <- tidyr::unnest(res$results, component_metrics) |>
#           dplyr::group_by(comp) |>
#           dplyr::summarise(mse = mean(mse, na.rm = TRUE)) |>
#           dplyr::arrange(comp)
#           
#   # Check if 'mse' column exists (might be comp_error if truncation failed)
#   if (!"mse" %in% names(mses)) {
#     fail("MSE column not found in results, check for errors during CV.")
#   }
#   
#   # Expect strictly decreasing (or at least non-increasing) curve
#   # Allow for small tolerance due to numerical precision
#   expect_true(all(diff(mses$mse) <= 1e-8))
# })
</file>

<file path="tests/testthat/test_discriminant_projector.R">
# tests/testthat/test_discriminant_projector.R
context("discriminant_projector")

library(multivarious)
library(MASS)          # for lda()

# -------------------------------------------------------------------------
# helper : two–class toy data ‒ clearly separated signal  +  pure noise
# -------------------------------------------------------------------------
set.seed(42)
n_per  <- 25L                      # per–class observations
p_sig  <- 2L                       # informative dimensions
p_noise<- 8L                       # noise dimensions
p      <- p_sig + p_noise

X_signal <- rbind(
  cbind(matrix(rnorm(n_per * p_sig ,  mean = -3), n_per, p_sig),
        matrix(rnorm(n_per * p_noise), n_per) ),
  cbind(matrix(rnorm(n_per * p_sig ,  mean =  3), n_per, p_sig),
        matrix(rnorm(n_per * p_noise), n_per) )
)
Y_signal <- factor(rep(c("A", "B"), each = n_per))

X_noise  <- X_signal                       # same features
Y_noise  <- factor(sample(Y_signal))       # labels unrelated to X

# -------------------------------------------------------------------------
# 1. Constructor integrity -------------------------------------------------
# -------------------------------------------------------------------------
test_that("discriminant_projector constructor stores consistent state", {

  lda_fit  <- lda(X_signal, grouping = Y_signal)
  # Initialize the default preprocessor (pass() does nothing, but follows pattern)
  preproc <- prep(pass())
  Xp <- init_transform(preproc, X_signal)
  
  dp       <- discriminant_projector(
                v      = lda_fit$scaling,
                s      = X_signal %*% lda_fit$scaling,
                sdev   = lda_fit$svd,
                preproc = preproc, # <-- Pass initialized preproc
                labels = Y_signal,
                Sigma  = lda_fit$covariance)

  # class & counts
  expect_s3_class(dp, c("discriminant_projector", "bi_projector"))
  expect_equal(dp$counts, table(Y_signal, dnn = NULL))

  # shape coherence
  expect_equal(dim(dp$v)[2], length(dp$sdev))
  expect_equal(dim(dp$s), c(length(Y_signal), dim(dp$v)[2]))
})

# -------------------------------------------------------------------------
# 2. Prediction engine (LDA & Euclidean) -----------------------------------
# -------------------------------------------------------------------------
test_that("predict.discriminant_projector produces sensible classes & probabilities", {

  # Initialize the default preprocessor
  preproc <- prep(pass())
  Xp <- init_transform(preproc, X_signal)
  
  dp <- {
    lda_fit <- lda(X_signal, grouping = Y_signal)
    discriminant_projector(v      = lda_fit$scaling, 
                           s      = X_signal %*% lda_fit$scaling,
                           sdev   = lda_fit$svd, 
                           preproc = preproc, # <-- Pass initialized preproc
                           labels = Y_signal,
                           Sigma  = lda_fit$covariance)
  }

  ## --- LDA method --------------------------------------------------------
  preds_lda  <- predict(dp, X_signal, method = "lda", type = "class")
  acc_lda    <- mean(preds_lda == Y_signal)
  expect_true(acc_lda > .90)                     # > 90 % on clearly separated data

  probs_lda  <- predict(dp, X_signal, method = "lda", type = "prob")
  expect_equal(dim(probs_lda), c(length(Y_signal), nlevels(Y_signal)))
  expect_true(all(abs(rowSums(probs_lda) - 1) < 1e-10))

  ## --- Euclidean method --------------------------------------------------
  preds_euc  <- predict(dp, X_signal, method = "euclid", type = "class")
  acc_euc    <- mean(preds_euc == Y_signal)
  expect_true(acc_euc > .80)                     # slightly laxer threshold

})

# -------------------------------------------------------------------------
# 3. Permutation test reacts to signal vs noise ----------------------------
# -------------------------------------------------------------------------
test_that("perm_test.discriminant_projector yields small p for signal and large p for noise", {

  # Initialize the default preprocessor
  preproc1 <- prep(pass())
  preproc2 <- prep(pass())
  initialized_proc <- init_transform(preproc1, X_signal)
  initialized_proc_noise <- init_transform(preproc2, X_noise) # Need one for noise data too
  
  ## fit on signal ---------------------------------------------------------
  lda_sig <- lda(X_signal, grouping = Y_signal)
  dp_sig  <- discriminant_projector(v      = lda_sig$scaling, 
                                    s      = X_signal %*% lda_sig$scaling,
                                    sdev   = lda_sig$svd, 
                                    preproc = preproc1, # <-- Pass initialized preproc
                                    labels = Y_signal,
                                    Sigma  = lda_sig$covariance)

  pt_sig  <- perm_test(dp_sig, X_signal, nperm = 150)
  expect_true(pt_sig$p.value < 0.05)             # evidence of separation

  ## fit on pure‑noise labels ---------------------------------------------
  lda_noise <- lda(X_noise, grouping = Y_noise)
  dp_noise  <- discriminant_projector(v      = lda_noise$scaling, 
                                      s      = X_noise %*% lda_noise$scaling,
                                      sdev   = lda_noise$svd, 
                                      preproc = preproc2, # <-- Pass initialized preproc for noise
                                      labels = Y_noise,
                                      Sigma  = lda_noise$covariance)

  pt_noise <- perm_test(dp_noise, X_noise, nperm = 150)
  expect_true(pt_noise$p.value > 0.10)           # no real separation
})
</file>

<file path="tests/testthat.R">
library(testthat)
library(multivarious)

test_check("multivarious")
</file>

<file path="R/cPCA.R">
#' Contrastive PCA++ (cPCA++)
#
#' Performs Contrastive PCA++ (cPCA++) to find directions that capture variation
#' enriched in a "foreground" dataset relative to a "background" dataset.
#' This implementation follows the cPCA++ approach which directly solves the
#' generalized eigenvalue problem Rf v = lambda Rb v, where Rf and Rb are
#' the covariance matrices of the foreground and background data, centered
#' using the *background mean*.
#'
#' @references
#' Salloum, R., Kuo, C. C. J. (2022). cPCA++: An efficient method for contrastive feature learning. Pattern Recognition, 124, 108378. (Algorithm 1)
#'
#' @param X_f A numeric matrix representing the foreground dataset (samples x features).
#' @param X_b A numeric matrix representing the background dataset (samples x features).
#'        `X_f` and `X_b` must have the same number of features (columns).
#' @param ncomp Integer. The number of contrastive components to compute. Defaults to `min(ncol(X_f))`, 
#'        but will be capped by the effective rank based on sample sizes.
#' @param center_background Logical. If TRUE (default), both `X_f` and `X_b` are centered using the
#'        column means of `X_b`. If FALSE, it assumes data is already appropriately centered.
#' @param lambda Shrinkage intensity for covariance estimation (0 <= lambda <= 1).
#'        Defaults to 0 (no shrinkage). Uses `corpcor::cov.shrink`. Can help stabilize
#'        results if `Rb` is ill-conditioned or singular.
#' @param method A character string specifying the primary computation method. Options include:
#'    - `"geigen"` (Default): Use `geneig` from the `geigen` package.
#'    - `"primme"`: Use `geneig` with the PRIMME library backend (requires special `geigen` build).
#'    - `"sdiag"`: Use `geneig` with a spectral decomposition method.
#'    - `"corpcor"`: Use a corpcor-based whitening approach followed by standard PCA.
#' @param strategy Controls the GEVD approach when `method` is not `"corpcor"`. Options include:
#'    - `"auto"` (Default): Chooses based on dimensions (feature vs. sample space).
#'    - `"feature"`: Forces direct computation via `p x p` covariance matrices.
#'    - `"sample"`: Forces sample-space computation via SVD and a smaller GEVD (efficient for large `p`).
#' @param ... Additional arguments passed to the underlying computation functions 
#'    (`geigen::geneig` or `irlba::irlba` based on `method` and `strategy`).
#'
#' @details
#' **Preprocessing:** Following the cPCA++ paper, if `center_background = TRUE`, both `X_f` and `X_b`
#' are centered by subtracting the column means calculated *only* from the background data `X_b`.
#' This is crucial for isolating variance specific to `X_f`.
#'  
#' **Core Algorithm (methods "geigen", "primme", "sdiag", strategy="feature"):**
#' 1. Center `X_f` and `X_b` using the mean of `X_b`.
#' 2. Compute potentially shrunk \eqn{p \times p} covariance matrices `Rf` (from centered `X_f`) and `Rb` (from centered `X_b`) using `corpcor::cov.shrink`.
#' 3. Solve the generalized eigenvalue problem `Rf v = lambda Rb v` for the top `ncomp` eigenvectors `v` using `geigen::geneig`. These eigenvectors are the contrastive principal components (loadings).
#' 4. Compute scores by projecting the centered foreground data onto the eigenvectors: `S = X_f_centered %*% v`.
#'
#' **Core Algorithm (Large-D / Sample Space Strategy, strategy="sample"):**
#' When \eqn{p \gg n}, forming \eqn{p \times p} matrices `Rf` and `Rb` is infeasible. The "sample" strategy follows cPCA++ §3.2:
#' 1. Center `X_f` and `X_b` using the mean of `X_b`.
#' 2. Compute the SVD of centered \eqn{X_b = Ub Sb Vb^T} (using `irlba` for efficiency).
#' 3. Project centered `X_f` into the background's principal subspace: `Zf = X_f_centered %*% Vb`.
#' 4. Form small \eqn{r \times r} matrices: `Rf_small = cov(Zf)` and `Rb_small = (1/(n_b-1)) * Sb^2`.
#' 5. Solve the small \eqn{r \times r} GEVD: `Rf_small w = lambda Rb_small w` using `geigen::geneig`.
#' 6. Lift eigenvectors back to feature space: `v = Vb %*% w`.
#' 7. Compute scores: `S = X_f_centered %*% v`.
#'
#' **Alternative Algorithm (method "corpcor"):**
#' 1. Center `X_f` and `X_b` using the mean of `X_b`.
#' 2. Compute `Rb` and its inverse square root `Rb_inv_sqrt`.
#' 3. Whiten the foreground data: `X_f_whitened = X_f_centered %*% Rb_inv_sqrt`.
#' 4. Perform standard PCA (`stats::prcomp`) on `X_f_whitened`.
#' 5. The returned `v` and `s` are the loadings and scores *in the whitened space*. The loadings are *not* the generalized eigenvectors `v`. A specific class `corpcor_pca` is added to signal this.
#'
#' @return A `bi_projector`-like object with classes `c("cPCAplus", "<method_class>", "bi_projector")` containing:
#' \describe{
#'   \item{v}{Loadings matrix (features x ncomp). Interpretation depends on `method` (see Details).}
#'   \item{s}{Scores matrix (samples_f x ncomp).}
#'   \item{sdev}{Vector (length ncomp). Standard deviations (sqrt of generalized eigenvalues for `geigen` methods, PCA std devs for `corpcor`).}
#'   \item{values}{Vector (length ncomp). Generalized eigenvalues (for `geigen` methods) or PCA eigenvalues (for `corpcor`).}
#'   \item{strategy}{The strategy used ("feature" or "sample") if method was not "corpcor".}
#'   \item{preproc}{The initialized `preprocessor` object used.}
#'   \item{method}{The computation method used.}
#'   \item{ncomp}{The number of components computed.}
#'   \item{nfeatures}{The number of features.}
#' }
#'
#' @examples
#' # Simulate data where foreground has extra variance in first few dimensions
#' set.seed(123)
#' n_f <- 100
#' n_b <- 150
#' n_features <- 50
#'
#' # Background: standard normal noise
#' X_b <- matrix(rnorm(n_b * n_features), nrow=n_b, ncol=n_features)
#' colnames(X_b) <- paste0("Feat_", 1:n_features)
#'
#' # Foreground: background noise + extra variance in first 5 features
#' X_f_signal <- matrix(rnorm(n_f * 5, mean=0, sd=2), nrow=n_f, ncol=5)
#' X_f_noise <- matrix(rnorm(n_f * (n_features-5)), nrow=n_f, ncol=n_features-5)
#' X_f <- cbind(X_f_signal, X_f_noise) + matrix(rnorm(n_f * n_features), nrow=n_f, ncol=n_features)
#' colnames(X_f) <- paste0("Feat_", 1:n_features)
#' rownames(X_f) <- paste0("SampleF_", 1:n_f)
#'
#' # Apply cPCA++ (requires geigen and corpcor packages)
#' # install.packages(c("geigen", "corpcor"))
#' if (requireNamespace("geigen", quietly = TRUE) && requireNamespace("corpcor", quietly = TRUE)) {
#'   # Assuming helper constructors like bi_projector are available
#'   # library(multivarious) 
#'
#'   res_cpca_plus <- cPCAplus(X_f, X_b, ncomp = 5, method = "geigen")
#'
#'   # Scores for the foreground data (samples x components)
#'   print(head(res_cpca_plus$s))
#'
#'   # Loadings (contrastive directions) (features x components)
#'   print(head(res_cpca_plus$v))
#'
#'   # Plot scores
#'   plot(res_cpca_plus$s[, 1], res_cpca_plus$s[, 2],
#'        xlab = "Contrastive Component 1", ylab = "Contrastive Component 2",
#'        main = "cPCA++ Scores (geigen method)")
#'
#'   # Example with corpcor method
#'   res_cpca_corp <- cPCAplus(X_f, X_b, ncomp = 5, method = "corpcor")
#'   print(head(res_cpca_corp$s)) # Scores in whitened space
#'   print(head(res_cpca_corp$v)) # Loadings in whitened space
#' }
#'
#' @importFrom stats prcomp
#' @importFrom Matrix crossprod
#' @importFrom stats coef prcomp
#' @importFrom corpcor pseudoinverse
#' @importFrom geigen geigen
#' @export
cPCAplus <- function(X_f, X_b, ncomp = NULL,
                     center_background = TRUE,
                     lambda = 0,
                     method = c("geigen", "primme", "sdiag", "corpcor"),
                     strategy = c("auto", "feature", "sample"),
                     ...) {

  # --- Input Validation --- 
  method <- match.arg(method)
  strategy <- match.arg(strategy)
  # Use chk or assertions for validation
  stopifnot(is.matrix(X_f), is.numeric(X_f))
  stopifnot(is.matrix(X_b), is.numeric(X_b))
  if (ncol(X_f) != ncol(X_b)) {
    stop("Foreground matrix X_f (", ncol(X_f), " features) and background matrix X_b (",
         ncol(X_b), " features) must have the same number of columns.")
  }
  if (is.null(ncomp)) {
    ncomp <- min(ncol(X_f), nrow(X_f), nrow(X_b))
    message("ncomp not provided, defaulting to min(p, n_f, n_b) = ", ncomp)
  }
  stopifnot(is.numeric(ncomp), length(ncomp) == 1, ncomp > 0, ncomp == floor(ncomp))
  stopifnot(ncomp <= ncol(X_f))
  stopifnot(is.logical(center_background), length(center_background) == 1)
  stopifnot(is.numeric(lambda), length(lambda) == 1, lambda >= 0, lambda <= 1)

  # Preserve original row/col names
  rn_f <- rownames(X_f)
  cn <- colnames(X_f)
  if (is.null(cn) && !is.null(colnames(X_b))) cn <- colnames(X_b)

  # --- Preprocessing --- 
  # Fix 2: Create proper pre_processor objects
  if (center_background) {
    mean_b <- colMeans(X_b, na.rm = TRUE)
    if(any(is.na(mean_b))) stop("NA values encountered in background mean calculation.")
    X_f_centered <- sweep(X_f, 2, mean_b, "-")
    X_b_centered <- sweep(X_b, 2, mean_b, "-")
    # Create a finalized preprocessor using the calculated means
    proc <- prep(center(cmeans = mean_b))
  } else {
    X_f_centered <- X_f
    X_b_centered <- X_b
    # Create a finalized identity preprocessor
    proc <- prep(pass())
  }

  # --- Core Computation --- 
  if (method == "corpcor") {
    if (!requireNamespace("corpcor", quietly = TRUE)) {
            stop("Package 'corpcor' needed for method='corpcor'. Please install it.", call. = FALSE)
        }

    # Compute Rb using cov.shrink on already centered data
    Rb <- corpcor::cov.shrink(X_b_centered, lambda = lambda, verbose = FALSE)

    # Compute Rb^(-1/2) using eigenvalue decomposition
    eig_Rb <- eigen(Rb, symmetric = TRUE)
    eig_vals_Rb_inv_sqrt <- ifelse(eig_Rb$values > sqrt(.Machine$double.eps), 1 / sqrt(eig_Rb$values), 0)
    # Check for potential issues
    if (sum(eig_vals_Rb_inv_sqrt > 0) < ncomp) {
        warning("Rank of background covariance (after thresholding) is less than ncomp. Results may be unreliable.")
    }
    Rb_inv_sqrt <- eig_Rb$vectors %*% diag(eig_vals_Rb_inv_sqrt, nrow=length(eig_vals_Rb_inv_sqrt)) %*% t(eig_Rb$vectors)

    # Whiten foreground data
    X_f_whitened <- X_f_centered %*% Rb_inv_sqrt

    # Perform standard PCA on the whitened data
    # Using prcomp for robustness and standard output
    pca_res <- tryCatch({
        stats::prcomp(X_f_whitened, center = FALSE, scale. = FALSE, rank. = ncomp)
        }, error = function(e){ 
            stop("Error during stats::prcomp on whitened data: ", e$message)
        })

    # Adjust ncomp if prcomp returned fewer components
    actual_ncomp <- min(ncomp, ncol(pca_res$rotation))
    if (actual_ncomp < ncomp) {
        warning("prcomp returned fewer components (", actual_ncomp, ") than requested (", ncomp, ").")
        ncomp <- actual_ncomp
    }
    if (ncomp == 0) stop("PCA on whitened data yielded zero components.")

    # Results are in the whitened space
    eigenvectors_whitened <- pca_res$rotation[, 1:ncomp, drop = FALSE]
    eigenvalues_pca <- (pca_res$sdev[1:ncomp])^2
    scores_pca <- pca_res$x[, 1:ncomp, drop = FALSE]

    # Back-transform loadings to original space
    v_orig <- Rb_inv_sqrt %*% eigenvectors_whitened

    # Recalculate scores by projecting original centered data onto back-transformed loadings
    scores <- X_f_centered %*% v_orig

    # Create structure using bi_projector constructor
    # Note: values/sdev are from PCA on whitened data, not generalized eigenvalues
     projector <- bi_projector(
            v = v_orig,                  # Loadings in original space
            s = scores,                  # Scores = projection onto original space loadings
            sdev = pca_res$sdev[1:ncomp], # Sdev from PCA on whitened data
            values = eigenvalues_pca,    # Eigenvalues from PCA on whitened data
            preproc = proc,
            classes = c("cPCAplus", "corpcor_pca", "bi_projector"),
            method_used = list(type = "cPCAplus", method = method, lambda=lambda, ncomp=ncomp) # Store extra info
        )

  } else {
    # --- Methods using geigen --- 
    if (!requireNamespace("geigen", quietly = TRUE)) {
            stop("Package 'geigen' needed for methods 'geigen', 'primme', 'sdiag'. Please install it.", call. = FALSE)
        }
    if (!requireNamespace("corpcor", quietly = TRUE)) {
            # Required for covariance estimation
            stop("Package 'corpcor' needed for covariance estimation. Please install it.", call. = FALSE)
        }
        
    # Determine strategy: feature space (direct covariance) or sample space (large D)
    p <- ncol(X_f_centered)
    n_f <- nrow(X_f_centered)
    n_b <- nrow(X_b_centered)
    use_sample_space <- FALSE
    effective_strategy <- strategy

    if (strategy == "auto") {
      # Heuristic: p > 5 * max(n_f, n_b)
      if (p > 5 * max(n_f, n_b)) {
        message("Large dimension detected (p > 5*max(n_f, n_b)). Switching to sample-space GEVD strategy.")
        use_sample_space <- TRUE
        effective_strategy <- "sample"
      } else {
        effective_strategy <- "feature"
      }
    } else if (strategy == "sample") {
      use_sample_space <- TRUE
    } # else strategy == "feature" -> use_sample_space remains FALSE

    if (lambda != 0 && use_sample_space) {
        warning("Shrinkage (lambda != 0) is specified but the sample-space strategy is selected. Shrinkage currently only applied to feature-space strategy.")
        # lambda is ignored in sample-space path computation below
    }

    # --- Compute Eigen solution based on strategy --- 
    if (use_sample_space) {
        # --- Sample Space Strategy (Large D) --- 
        message("Using sample-space strategy...")
        if (!requireNamespace("irlba", quietly = TRUE))
            stop("Package 'irlba' needed for large-D sample-space strategy. Install it.", call.=FALSE)

        # 1. Background SVD (thin)
        # Oversample slightly for stability
        r_target <- min(n_b - 1, p, ncomp + 15)
        if (r_target <= 0) stop("Target rank for background SVD is non-positive.")

        svdb <- tryCatch({
            irlba::irlba(X_b_centered, nv = r_target, nu = 0)
           }, error = function(e){ 
               stop("Error during irlba SVD of background matrix X_b: ", e$message)
           })
        
        V_b  <- svdb$v            # p  x r
        # Use adjusted degrees of freedom for covariance estimate
        Sigma2 <- (svdb$d^2) / max(1, n_b - 1)   # length r 
        actual_rank_b <- length(Sigma2)

        # 2. Small matrices
        Rb_small <- diag(Sigma2, nrow = actual_rank_b)
        
        # Foreground projected into background subspace
        Zf  <- X_f_centered %*% V_b            # n_f x r
        # Use adjusted degrees of freedom for covariance estimate
        Rf_small <- crossprod(Zf) / max(1, n_f - 1)  # r x r

        # 3. Solve GEVD in r x r space
        ncomp_geigen <- min(ncomp, actual_rank_b)
        if (ncomp_geigen < ncomp) {
            warning("Reduced ncomp from ", ncomp, " to ", ncomp_geigen, " due to background SVD rank.")
            ncomp <- ncomp_geigen
        }
        if (ncomp == 0) stop("Cannot compute components; ncomp is zero after rank adjustment.")

        geig_small <- tryCatch({
            geigen::geigen(Rf_small, Rb_small, symmetric = TRUE)
           }, error = function(e) {
               stop("Error during small GEVD using geigen: ", e$message)
           })
        
        # Ensure enough valid eigenvalues/vectors returned
        k_avail_small <- length(geig_small$values)
        actual_ncomp_small <- min(ncomp, k_avail_small)
        if (actual_ncomp_small < ncomp) {
             warning("Small GEVD returned fewer eigenvalues (", k_avail_small, ") than requested (", ncomp, "). Reducing ncomp to ", actual_ncomp_small, ".")
             ncomp <- actual_ncomp_small
        }
        if (ncomp == 0) stop("Small GEVD returned zero components.")

        eigenvalues_raw <- geig_small$values[1:ncomp]
        w <- geig_small$vectors[, 1:ncomp, drop = FALSE] # r x k
        
        # 4. Lift eigenvectors back to feature space
        v_raw <- V_b %*% w                     # p x k
        
        # Ensure real
        if (is.complex(v_raw)) {
            warning("Complex eigenvectors encountered after lifting, taking the real part.")
            v_raw <- Re(v_raw)
        }
        eigenvectors_ordered <- v_raw # Keep intermediate name
        eigenvalues_ordered <- eigenvalues_raw

    } else {
        # --- Feature Space Strategy (Standard/Small D) --- 
        message("Using feature-space strategy...")
        # Compute covariance matrices using shrunk estimates on centered data
        Rf_obj <- corpcor::cov.shrink(X_f_centered, lambda = lambda, verbose = FALSE)
        Rb_obj <- corpcor::cov.shrink(X_b_centered, lambda = lambda, verbose = FALSE)
        
        # Ensure plain matrix for geigen by removing S3 class
        Rf <- Rf_obj
        Rb <- Rb_obj
        class(Rf) <- "matrix"
        class(Rb) <- "matrix"

        # Solve the generalized eigenvalue problem: Rf v = lambda Rb v
        geigen_res <- tryCatch({
            geigen::geigen(A = Rf, B = Rb, ...)
            }, error = function(e) {
               stop("Error during geigen::geigen: ", e$message) 
            })

        # Extract results
        k_avail <- length(geigen_res$values)
        actual_ncomp <- min(ncomp, k_avail)
         if (actual_ncomp < ncomp) {
            warning("geigen returned fewer eigenvalues (", k_avail, ") than requested (", ncomp, "). Reducing ncomp to ", actual_ncomp, ".")
            ncomp <- actual_ncomp
        }
        if (ncomp == 0) stop("Generalized eigenvalue decomposition returned zero components.")

        eigenvalues_raw <- geigen_res$values[1:ncomp]
        eigenvectors_raw <- geigen_res$vectors[, 1:ncomp, drop = FALSE]

        # Ensure eigenvectors are real
        if (is.complex(eigenvectors_raw)) {
            warning("Complex eigenvectors encountered, taking the real part.")
            eigenvectors_raw <- Re(eigenvectors_raw)
        }
        eigenvectors_ordered <- eigenvectors_raw # Keep intermediate name
        eigenvalues_ordered <- eigenvalues_raw
    }

    # --- Post-process results common to both GEVD strategies --- 
    
    # Order by decreasing eigenvalue magnitude
    ord <- order(abs(eigenvalues_ordered), decreasing = TRUE)
    eigenvalues <- abs(eigenvalues_ordered[ord]) # Use abs value, should be positive
    eigenvectors <- eigenvectors_ordered[, ord, drop = FALSE]

    # Apply sign flip for consistency
    if (is.numeric(eigenvectors) && nrow(eigenvectors) > 0) { # Check if eigenvectors were successfully extracted
        signs <- apply(eigenvectors, 2, function(v) {
            if(all(is.na(v) | v == 0)) return(1) # Handle all zero/NA vectors
            # Flip based on element with largest absolute value
            sign_val <- base::sign(v[which.max(abs(v))])
            # Ensure sign is not 0 if max abs value is 0 (should not happen with check above)
            if (sign_val == 0) 1 else sign_val
            })
        signs[signs == 0] <- 1 # Handle cases where max element is 0
        eigenvectors <- sweep(eigenvectors, 2, signs, FUN = "*")
    } else {
        warning("Could not apply sign flip: eigenvectors are not numeric or empty.")
    }

    # Compute scores: projection of centered foreground data onto eigenvectors
    scores <- X_f_centered %*% eigenvectors

    # Create a bi_projector instance
    projector <- bi_projector(
            v = eigenvectors,         # Generalized eigenvectors (Loadings)
            s = scores,               # Scores = Projection onto loadings
            sdev = sqrt(pmax(eigenvalues, 0)), # Ensure non-negative before sqrt
            values = eigenvalues,     # Generalized eigenvalues
            preproc = proc,
            classes = c("cPCAplus", paste0(effective_strategy, "_pca"), "bi_projector"),
            method_used = list(type = "cPCAplus", method = method, strategy = effective_strategy, lambda=lambda, ncomp=ncomp) 
        )
  }

  # --- Final Touches --- 
  # Assign names
  colnames(projector$v) <- paste0("cPC", 1:ncomp)
  colnames(projector$s) <- paste0("cPC", 1:ncomp)
  if (!is.null(cn)) {
      rownames(projector$v) <- cn
  }
  if (!is.null(rn_f)) {
      rownames(projector$s) <- rn_f
  }

  return(projector)
}
</file>

<file path="tests/testthat/test_cpca.R">
library(dplyr)
library(testthat)
library(multivarious) # Load the package

# Helper function to prepare Iris data for cPCA context
# X_f: Between-group means (Foreground, 3 samples x 4 features)
# X_b: Original centered data (Background, 150 samples x 4 features)
prepare_iris_data <- function() {
  data(iris)
  X_full <- iris[, 1:4] # Numeric features only

  # Calculate between-group means (X_f)
  X_f <- X_full %>%
    bind_cols(Species = iris$Species) %>% # Add species for grouping
    group_by(Species) %>%
    summarise(across(where(is.numeric), mean), .groups = 'drop') %>% # Use .groups
    select(-Species) %>% # Remove Species column after summarising
    as.matrix()
  rownames(X_f) <- levels(iris$Species) # Assign species names as rownames

  # Use original data centered by overall mean as X_b (common use case)
  overall_means <- colMeans(X_full)
  # Note: cPCAplus internally centers by background mean if center_background=TRUE
  # Here we just return the raw data for X_b
  X_b <- as.matrix(X_full)

  return(list(X_f = X_f, X_b = X_b, n_f = nrow(X_f), n_b = nrow(X_b), p = ncol(X_f)))
}

test_that("cPCAplus basic structure and methods ('geigen' family)", {
  iris_data <- prepare_iris_data()
  X_f <- iris_data$X_f
  X_b <- iris_data$X_b
  n_components <- 2 # Test fewer components than features/samples

  # Test a geigen family method
  method <- "geigen" # Can also test "primme", "sdiag" if available/desired

  # --- Test with lambda = 0 ---
  result_lambda_0 <- cPCAplus(X_f, X_b, ncomp = n_components, lambda = 0, method = method)

  # Check classes
  expect_s3_class(result_lambda_0, "cPCAplus")
  expect_s3_class(result_lambda_0, "bi_projector")

  # Check dimensions using accessors
  expect_equal(ncomp(result_lambda_0), n_components)
  expect_equal(nrow(coef(result_lambda_0)), iris_data$p) # features x ncomp
  expect_equal(ncol(coef(result_lambda_0)), n_components)
  expect_equal(nrow(scores(result_lambda_0)), iris_data$n_f) # n_f_samples x ncomp
  expect_equal(ncol(scores(result_lambda_0)), n_components)

  # Check values/sdev length
  expect_length(result_lambda_0$values, n_components)
  expect_length(result_lambda_0$sdev, n_components)
  expect_equal(result_lambda_0$sdev, sqrt(pmax(result_lambda_0$values, 0)))

  # Check preprocessor class (use correct name)
  expect_s3_class(result_lambda_0$preproc, "pre_processor")

  # Check stored method info
  expect_equal(result_lambda_0$method_used$method, method)
  expect_equal(result_lambda_0$method_used$ncomp, n_components)

  # --- Test with lambda > 0 ---
  result_lambda_pos <- cPCAplus(X_f, X_b, ncomp = n_components, lambda = 0.1, method = method)

  # Basic structure checks again
  expect_s3_class(result_lambda_pos, "cPCAplus")
  expect_equal(ncomp(result_lambda_pos), n_components)
  expect_length(result_lambda_pos$values, n_components)

  # Check if results differ due to lambda (eigenvalues should change)
  # Use tolerance due to potential numerical noise
  expect_false(isTRUE(all.equal(result_lambda_0$values, result_lambda_pos$values)))

  # --- Test Strategy Force ---
  # Force "feature" strategy (should work for small iris data)
  result_feat_strat <- cPCAplus(X_f, X_b, ncomp = n_components, lambda = 0, method = method, strategy = "feature")
  expect_equal(result_feat_strat$method_used$strategy, "feature")
  # Compare with auto strategy result (should be identical here as auto chooses feature)
  expect_equal(coef(result_lambda_0), coef(result_feat_strat))
  expect_equal(result_lambda_0$values, result_feat_strat$values)

})

test_that("cPCAplus method 'corpcor'", {
  iris_data <- prepare_iris_data()
  X_f <- iris_data$X_f
  X_b <- iris_data$X_b
  n_components <- 2

  # --- Test corpcor method ---
  result_corpcor <- cPCAplus(X_f, X_b, ncomp = n_components, lambda = 0.1, method = "corpcor")

  # Check classes (includes corpcor_pca)
  expect_s3_class(result_corpcor, "cPCAplus")
  expect_s3_class(result_corpcor, "corpcor_pca")
  expect_s3_class(result_corpcor, "bi_projector")

  # Check dimensions
  expect_equal(ncomp(result_corpcor), n_components)
  expect_equal(nrow(coef(result_corpcor)), iris_data$p)
  expect_equal(nrow(scores(result_corpcor)), iris_data$n_f)

  # Check values/sdev length (these are from PCA on whitened data)
  expect_length(result_corpcor$values, n_components)
  expect_length(result_corpcor$sdev, n_components)
  expect_equal(result_corpcor$values, result_corpcor$sdev^2)

  # Check preprocessor
  expect_s3_class(result_corpcor$preproc, "pre_processor")

  # Check stored method info
  expect_equal(result_corpcor$method_used$method, "corpcor")

})

# Test centering option
test_that("cPCAplus center_background = FALSE works", {
  iris_data <- prepare_iris_data()
  # Pre-center data using background mean
  mean_b <- colMeans(iris_data$X_b)
  X_f_cent <- sweep(iris_data$X_f, 2, mean_b, "-")
  X_b_cent <- sweep(iris_data$X_b, 2, mean_b, "-")

  res_no_center <- cPCAplus(X_f_cent, X_b_cent, ncomp = 2, center_background = FALSE, method = "geigen")

  expect_s3_class(res_no_center, "cPCAplus")
  expect_equal(ncomp(res_no_center), 2)
  # Check preprocessor class
  expect_s3_class(res_no_center$preproc, "pre_processor")

  # Compare with default centering (should be numerically close)
  res_center <- cPCAplus(iris_data$X_f, iris_data$X_b, ncomp = 2, center_background = TRUE, method = "geigen")
  expect_equal(res_no_center$values, res_center$values, tolerance = 1e-6)
  # Check cosine similarity of eigenvectors (more robust than direct comparison)
  v1 <- coef(res_no_center)
  v2 <- coef(res_center)
  cos_sim <- abs(diag(crossprod(v1, v2))) # Absolute cosine similarity
  expect_true(all(cos_sim > 0.999)) # Expect vectors to point in same/opposite direction
})


# TODO: Add tests for strategy="sample" if feasible (requires p >> n data)
# TODO: Add tests for edge cases (e.g., ncomp=1, singular Rb)
</file>

<file path="tests/testthat/test_geneig.R">
library(testthat)
library(Matrix) # For diagonal matrix operations and checks

# Define a known symmetric matrix A and a positive definite matrix B
A <- matrix(c(4, 1, 1, 2), nrow=2, byrow=TRUE)
B <- matrix(c(6, 2, 2, 5), nrow=2, byrow=TRUE)  # B must be symmetric and positive definite


test_that("geigen method returns correct results", {
  result <- geneig(A = A, B = B, ncomp=2, method="geigen")
  expect_equal(dim(result$vectors), c(2, 2))
  expect_equal(length(result$values), 2)
  # Eigenvalues can be complex for geigen, check real part if needed or just existence
  # expect_true(all(Re(result$values) > 0)) # This might not hold for general A
})

test_that("robust method equivalent test using geigen", {
  # Changed method from "robust" to "geigen" as "robust" is removed
  result <- geneig(A = A, B = B, ncomp=2, method="geigen")
  expect_equal(dim(result$vectors), c(2, 2))
  expect_equal(length(result$values), 2)
  # Cannot guarantee positive values for geigen
  # expect_true(all(result$values > 0))
})

test_that("sdiag method equivalent test using geigen", {
  # Changed method from "sdiag" to "geigen" as "sdiag" is removed
  result <- geneig(A = A, B = B, ncomp=2, method="geigen")
  expect_equal(dim(result$vectors), c(2, 2))
  expect_equal(length(result$values), 2)
  # Cannot guarantee positive values for geigen
  # expect_true(all(result$values > 0))
})

test_that("primme method returns correct results", {
  skip_if_not_installed("PRIMME")  # Skip if PRIMME is not available
  result <- geneig(A = A, B = B, ncomp=2, method="primme", which="LA")
  expect_equal(dim(result$vectors), c(2, 2))
  expect_equal(length(result$values), 2)
  expect_true(all(result$values > 0))
})

test_that("non-square matrices are handled", {
  non_square_A <- matrix(1:6, nrow=2)
  non_square_B <- matrix(1:6, nrow=2)
  
  expect_error(geneig(A = non_square_A, B = non_square_B, ncomp=2, method="geigen"))
})

test_that("negative and very small eigenvalues in B are handled in geigen", {
  B_with_negative <- matrix(c(4, 1, 1, -2), nrow=2, byrow=TRUE)
  # Changed method from "sdiag" to "geigen"
  result <- geneig(A = A, B = B_with_negative, ncomp=2, method="geigen")
  expect_equal(dim(result$vectors), c(2, 2))
  # Cannot guarantee positive values for geigen
  # expect_true(all(result$values > 0))
})
</file>

<file path="tests/testthat/test_projector.R">
context("projector")
library(testthat)
library(multivarious)

test_that("can construct a projector object", {
  v <- matrix(rnorm(10*5), 10, 5)
  proj <- projector(v)
  expect_s3_class(proj, "projector")
  expect_equal(ncomp(proj), 5)
  expect_equal(shape(proj), c(10, 5))
})

test_that("can project data onto subspace", {
  mat1 <- matrix(rnorm(10*10), 10, 10)
  v <- matrix(rnorm(10*5), 10, 5)

  preproc <- prep(pass())
  Xp <- init_transform(preproc, mat1)

  proj <- projector(v, preproc=preproc)
  pdat <- project(proj, mat1)
  expect_equal(dim(pdat), c(10, 5))
})

test_that("can partially project data onto subspace", {
  mat1 <- matrix(rnorm(10*10), 10, 10)
  v <- matrix(rnorm(10*5), 10, 5)

  preproc <- prep(pass())
  Xp <- init_transform(preproc, mat1)
  proj <- projector(v, preproc=preproc)
  
  
  pdat <- partial_project(proj, mat1[, 1:5], 1:5) 
  expect_equal(dim(pdat), c(10, 5))
})

test_that("can compute inverse projection", {
  v <- matrix(rnorm(10*5), 10, 5)
  proj <- projector(v)
  inv_proj <- inverse_projection(proj)
  expect_equal(dim(inv_proj), c(5, 10))
})

test_that("can compute partial inverse projection", {
  v <- matrix(rnorm(10*5), 10, 5)
  proj <- projector(v)
  inv_proj <- partial_inverse_projection(proj, 1:5)
  expect_equal(dim(inv_proj), c(5, 10))
})

test_that("can truncate a projector", {
  v <- matrix(rnorm(10*5), 10, 5)
  proj <- projector(v)
  proj_trunc <- truncate(proj, 3)
  expect_equal(ncomp(proj_trunc), 3)
  expect_equal(shape(proj_trunc), c(10, 3))
})

test_that("can create and use a partial projector", {
  v <- matrix(rnorm(10*5), 10, 5)
  proj <- projector(v)
  
  placeholder_orig_data <- matrix(0, nrow=2, ncol=10)
  Xp <- init_transform(proj$preproc, placeholder_orig_data)
  
  partial_proj <- partial_projector(proj, 1:7)
  expect_s3_class(partial_proj, "partial_projector")
  expect_equal(shape(partial_proj), c(7, 5))
  
  mat1_partial <- matrix(rnorm(10*7), 10, 7)
  pdat <- project(partial_proj, mat1_partial)
  expect_equal(dim(pdat), c(10, 5))
})
</file>

<file path="tests/testthat/test_svd.R">
test_that("can run a simple svd with 1 component", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- svd_wrapper(mat1, 1)
  
  expect_equal(ncomp(pres),1)
})

test_that("can run a simple svd with 1 rsvd", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- svd_wrapper(mat1, 1, method="rsvd")
  
  expect_equal(ncomp(pres),1)
})
</file>

<file path="tests/testthat/test_nystrom.R">
library(testthat)
library(multivarious)

# test_that("nystrom_approx returns a bi_projector object", {
#   X <- matrix(rnorm(50 * 10), 50, 10)
#   nystrom_res <- nystrom_approx(X)
#   expect_s3_class(nystrom_res, "bi_projector")
# })
# 
# test_that("nystrom_approx returns the correct number of components", {
#   X <- matrix(rnorm(50 * 10), 50, 10)
#   nystrom_res_3 <- nystrom_approx(X, ncomp=3)
#   expect_equal(ncol(nystrom_res_3$v), 3)
#   
#   nystrom_res_5 <- nystrom_approx(X, ncomp=5)
#   expect_equal(ncol(nystrom_res_5$v), 5)
# })
# 
# test_that("nystrom_approx handles custom kernel functions", {
#   X <- matrix(rnorm(50 * 10), 50, 10)
#   
#   linear_kernel <- function(x, y) {
#     x %*% t(y)
#   }
#   
#   nystrom_res_linear <- nystrom_approx(X, kernel_func=linear_kernel)
#   expect_s3_class(nystrom_res_linear, "bi_projector")
#   
#   rbf_kernel <- function(x, y, sigma=1) {
#     size_x <- nrow(x)
#     size_y <- nrow(y)
#     
#     dist_matrix <- matrix(0, nrow=size_x, ncol=size_y)
#     for (i in 1:size_x) {
#       for (j in 1:size_y) {
#         dist_matrix[i, j] <- sum((x[i, ] - y[j, ])^2)
#       }
#     }
#     
#     exp(-dist_matrix / (2 * sigma^2))
#   }
#   
#   nystrom_res_rbf <- nystrom_approx(X, kernel_func=rbf_kernel)
#   expect_s3_class(nystrom_res_rbf, "bi_projector")
# })
# 
# test_that("nystrom_approx handles custom landmarks", {
#   X <- matrix(rnorm(50 * 10), 50, 10)
#   landmarks <- c(1, 5, 10, 15, 20,25,27)
#   
#   nystrom_res_landmarks <- nystrom_approx(X, landmarks=landmarks)
#   expect_s3_class(nystrom_res_landmarks, "bi_projector")
# })
# 
# test_that("nystrom_approx handles custom nlandmarks", {
#   X <- matrix(rnorm(50 * 10), 50, 10)
#   
#   nystrom_res_nlandmarks <- nystrom_approx(X, nlandmarks=5)
#   expect_s3_class(nystrom_res_nlandmarks, "bi_projector")
# })
</file>

<file path="tests/testthat/test_regress.R">
test_that("can run a regress analysis with one y variable", {
  mat1 <- matrix(rnorm(100*15), 100, 15)
  y <- rnorm(100)
  reg <- regress(mat1, y, preproc=pass(), method="lm")
  expect_true(!is.null(reg))
  
  recon <- reconstruct(reg)
  expect_true(!is.null(recon))
})

test_that("can run a regress analysis with one y variable and intercept", {
  mat1 <- matrix(rnorm(100*15), 100, 15)
  y <- rnorm(100)
  reg <- regress(mat1, y, preproc=pass(), method="lm", intercept=TRUE)
  expect_true(!is.null(reg))
  
  recon <- reconstruct(reg)
  expect_true(!is.null(recon))
  
  expect_true(ncol(reg$v) == ncol(mat1)+1)
  
})

test_that("can run a regress analysis with multiple y variables", {
  mat1 <- matrix(rnorm(100*15), 100, 15)
  y <- cbind(rnorm(100), rnorm(100), rnorm(100))
  reg <- regress(mat1, y, preproc=pass(), method="lm")
  recon <- reconstruct(reg)
  expect_true(!is.null(reg))
  expect_true(!is.null(recon))
})

test_that("can run a regress analysis with multiple y variables and ridge", {
  mat1 <- matrix(rnorm(100*15), 100, 15)
  y <- cbind(rnorm(100), rnorm(100), rnorm(100))
  reg <- regress(mat1, y, preproc=pass(), method="mridge")
  recon <- reconstruct(reg)
  expect_true(!is.null(reg))
  expect_true(!is.null(recon))
})

test_that("can run a regress analysis with multiple y variables and enet", {
  mat1 <- matrix(rnorm(100*15), 100, 15)
  y <- cbind(rnorm(100), rnorm(100), rnorm(100))
  reg <- regress(mat1, y, preproc=pass(), method="enet", alpha=.3, lambda=.01)
  recon <- reconstruct(reg)
  expect_true(!is.null(reg))
  expect_true(!is.null(recon))
})
</file>

<file path="tests/testthat/test_pca.R">
test_that("can run a simple pca analysis", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1)
  
  proj <- project(pres, mat1)
  s <- scores(pres)
  
  expect_equal(proj ,s)
  expect_equal(sdev(pres)[1:length(pres$d)], svd(scale(mat1,center=TRUE, scale=FALSE))$d[1:length(pres$d)])
})

test_that("can project variables using pca result", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1)
  
  pv <- project_vars(pres, mat1)
  expect_equal(pv * (nrow(mat1) - 1), coefficients(pres), tolerance = 1e-6)
})

test_that("can reconstruct a PCA and recover X", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1, preproc = pass())
  
  recon <- reconstruct(pres)
  expect_equal(recon, mat1, tolerance = 1e-3, ignore_attr = TRUE)
})

test_that("can reconstruct a PCA and recover X after centering", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1, preproc = center())
  
  recon <- reconstruct(pres)
  expect_equal(recon, mat1, tolerance = 1e-3, ignore_attr = TRUE)
})

test_that("can reconstruct a PCA and recover X after standardizing", {
  mat1 <- matrix(rnorm(10*15, mean=5, sd=3), 10, 15) # Add mean/sd variation
  # Ensure no zero variance columns
  mat1[,1] <- mat1[,1] + 1:10 
  pres <- pca(mat1, preproc = standardize())
  
  recon <- reconstruct(pres)
  expect_equal(recon, mat1, tolerance = 1e-3, ignore_attr = TRUE)
})

test_that("can reconstruct a PCA and recover X with pass() preproc", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1, preproc = pass())
  
  recon <- reconstruct(pres)
  expect_equal(recon, mat1, tolerance = 1e-3, ignore_attr = TRUE)
})

test_that("can reconstruct a PCA and recover X after colscale(type='z')", {
  mat1 <- matrix(rnorm(10*15, mean=5, sd=3), 10, 15) # Add mean/sd variation
  # Ensure no zero variance columns
  mat1[,1] <- mat1[,1] + 1:10 
  
  # colscale(type='z') scales by 1/sd, does not center
  pres <- pca(mat1, preproc = colscale(type='z'))
  
  recon <- reconstruct(pres)
  expect_equal(recon, mat1, tolerance = 1e-3, ignore_attr = TRUE)
})

test_that("can reconstruct a PCA and recover X after colscale(type='unit')", {
  mat1 <- matrix(rnorm(10*15, mean=2, sd=2), 10, 15) # Add mean/sd variation
  # Ensure no zero variance columns
  mat1[,1] <- mat1[,1] + 1:10 
  
  # colscale(type='unit') scales columns to unit variance
  pres <- pca(mat1, preproc = colscale(type='unit'))
  
  recon <- reconstruct(pres)
  expect_equal(recon, mat1, tolerance = 1e-3, ignore_attr = TRUE)
})

test_that("can compute pca residuals", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1, ncomp=2)
  
  mat1_centered <- scale(mat1, center=TRUE, scale=FALSE)
  resid_vals <- residuals(pres, ncomp=2, xorig=mat1_centered)
  expect_true(all.equal(dim(resid_vals), dim(mat1)))
})

test_that("can truncate a pca", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1, ncomp=4)
  pres2 <- truncate(pres, 2)

  expect_true(ncomp(pres2) == 2)
  expect_equal(length(pres2$sdev), 2)
  expect_equal(ncol(coef(pres2)), 2)
  expect_equal(ncol(scores(pres2)), 2)
})

test_that("can compute permutation test statistics", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1, ncomp=4)
  ptest <- perm_test(pres, mat1, nperm = 50, parallel = FALSE)
  expect_s3_class(ptest, "perm_test")
  expect_s3_class(ptest, "perm_test_pca")
  expect_true(!is.null(ptest$component_results))
  expect_true(nrow(ptest$component_results) > 0)
})
</file>

<file path="tests/testthat/test_preprocess.R">
context("pre-processing")
library(magrittr)
library(testthat)

test_that("can preprocess a matrix no center, no scale", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pp <- pass() %>% prep()
  X <- pp$init(mat1)
  x2 <- pp$reverse_transform(X)
  expect_equal(mat1,x2)
  expect_equal(X, mat1)
})

test_that("can preprocess a matrix center only", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pp <- center() %>% prep()
  Xp <- pp$init(mat1)
  x2 <- pp$reverse_transform(Xp)
  expect_equal(mat1,x2)
  expect_true(all(mat1 != Xp))
})

test_that("can apply a centering transform", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pp <- center()
  x <- prep(pp)
  x2 <- init_transform(x,mat1)
  x3 <- apply_transform(x,mat1)
  expect_equal(x2,x3)
})

test_that("can apply a scaling transform", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pp <- standardize()
  x <- prep(pp)
  x2 <- init_transform(x, mat1)
  x3 <- apply_transform(x, mat1)
  expect_equal(x2,x3)
 
})

test_that("can preprocess a matrix with column scaling", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  wts <- 2:16
  pp <- colscale(type="weights", weights=wts)
  x <- prep(pp)
  xinit <- init_transform(x, mat1)
  xrev <- reverse_transform(x, xinit)
  expect_equal(mat1,xrev)
})

# test_that("can reset a prepper with `fresh`", {
#   mat1 <- matrix(rnorm(10*15), 10, 15)
#   pp <- center()
#   x <- prep(pp, mat1)
#   
# })



test_that("can reverse transform a matrix after standardization", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pp <- standardize()
  x <- prep(pp)
  x1 <- init_transform(x,mat1)
  x2 <- reverse_transform(x, x1)
  expect_equal(mat1,x2)
})



test_that("can compose two pre-processors", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  x <- center() %>% colscale(type="z") %>% prep()
  
  x1 <- init_transform(x,mat1)
  x2 <- reverse_transform(x, x1)
  expect_equal(mat1,x2)

})



test_that("can preprocess a matrix with a colind", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pp <- center() %>% prep()
  
  x <- init_transform(pp,mat1)
  ret <- apply_transform(pp, mat1[,1:2], colind=1:2)
  
  expect_equal(ret, x[,1:2])
})

test_that("can concatenate two pre-processors", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  mat2 <- matrix(rnorm(10*15), 10, 15)
  p <- center()
  
  proclist <- lapply(1:2, function(i) {
    fresh(p) %>% prep()
  })
  
  m1 <- init_transform(proclist[[1]], mat1)
  m2 <- init_transform(proclist[[2]], mat2)
  proc <- concat_pre_processors(proclist, list(1:15, 16:30))
  
  a1 <- apply_transform(proc, cbind(mat1,mat2))
  a2 <- apply_transform(proc, mat1, colind=1:15)
  a3 <- apply_transform(proc, mat2, colind=16:30)
  
  pres<- pca(cbind(mat1,mat2), ncomp=12)
  proj <- multiblock_biprojector(pres$v, s=pres$s, sdev=pres$sdev, proc, block_indices=list(1:15, 16:30))
  p1 <- project_block(proj, m1, 1)
  p2 <- project_block(proj, m2, 2)
  
  expect_true(!is.null(p1))
  expect_true(!is.null(p2))
  
  proc$transform(mat1, colind=1:15)
  proc$transform(mat2, colind=16:30)
  
  proj <- multiblock_projector(pres$v, proc, block_indices=list(1:15, 16:30))
  p1 <- project_block(proj, m1, 1)
  p2 <- project_block(proj, m2, 2)
  
  expect_true(!is.null(p1))
  expect_true(!is.null(p2))
  
})

test_that("concat_pre_processors handles complex colind across different block types", {
  set.seed(123) # for reproducibility
  mat1 <- matrix(rnorm(10*5, mean=10), 10, 5)
  mat2 <- matrix(rnorm(10*7, mean=20, sd=5), 10, 7)
  mat3 <- matrix(rnorm(10*3, mean=0, sd=1), 10, 3)
  
  p1 <- center() %>% prep()       # Center only
  p2 <- standardize() %>% prep()  # Center and scale
  p3 <- pass() %>% prep()         # No-op
  
  # Initialize individual processors
  m1_init <- init_transform(p1, mat1)
  m2_init <- init_transform(p2, mat2)
  m3_init <- init_transform(p3, mat3)
  
  proclist <- list(p1, p2, p3)
  block_indices <- list(1:5, 6:12, 13:15)
  
  proc_concat <- concat_pre_processors(proclist, block_indices)
  
  # --- Test apply_transform --- 
  # Select columns: 2nd and 4th from block 1, 2nd and 3rd from block 2, 2nd from block 3
  colind_global <- c(2, 4, 7, 8, 14)
  
  # Create the input matrix corresponding to these global columns
  test_mat_apply <- cbind(mat1[, c(2, 4)], mat2[, c(2, 3)], mat3[, 2, drop=FALSE])
  
  # Apply the concatenated transform
  result_apply <- apply_transform(proc_concat, test_mat_apply, colind = colind_global)
  
  # Manually apply transforms to corresponding original blocks and columns
  manual_res1 <- apply_transform(p1, mat1[, c(2, 4), drop = FALSE], colind = c(2, 4)) # Local indices: 2, 4
  manual_res2 <- apply_transform(p2, mat2[, c(2, 3), drop = FALSE], colind = c(2, 3)) # Local indices: 2, 3
  manual_res3 <- apply_transform(p3, mat3[, 2, drop = FALSE], colind = 2)        # Local index: 2
  
  # Combine manual results in the order defined by colind_global
  expected_apply <- cbind(manual_res1[, 1, drop=FALSE],  # Corresponds to global col 2
                        manual_res1[, 2, drop=FALSE],  # Corresponds to global col 4
                        manual_res2[, 1, drop=FALSE],  # Corresponds to global col 7
                        manual_res2[, 2, drop=FALSE],  # Corresponds to global col 8
                        manual_res3[, 1, drop=FALSE]) # Corresponds to global col 14
  
  expect_equal(result_apply, expected_apply, tolerance = 1e-7)
  
  # --- Test reverse_transform --- 
  # Input for reverse is the result from apply_transform
  test_mat_reverse <- result_apply
  
  # Reverse using concatenated processor
  result_reverse <- reverse_transform(proc_concat, test_mat_reverse, colind = colind_global)
  
  # Manually reverse transforms using the outputs from the manual forward transforms
  manual_rev1 <- reverse_transform(p1, manual_res1, colind = c(2, 4))
  manual_rev2 <- reverse_transform(p2, manual_res2, colind = c(2, 3))
  manual_rev3 <- reverse_transform(p3, manual_res3, colind = 2)
  
  # Combine manual reverse results in the order defined by colind_global
  expected_reverse <- cbind(manual_rev1[, 1, drop=FALSE], 
                            manual_rev1[, 2, drop=FALSE], 
                            manual_rev2[, 1, drop=FALSE], 
                            manual_rev2[, 2, drop=FALSE], 
                            manual_rev3[, 1, drop=FALSE])
                            
  # The result of reverse should match the original input subset 'test_mat_apply'
  expect_equal(result_reverse, test_mat_apply, tolerance = 1e-7)
})

# 
# test_that("can preprocess a block projector", {
#   mat1 <- matrix(rnorm(10*15), 10, 15)
#   mat2 <-  matrix(rnorm(10*10), 10, 10)
#   pca1 <- pca(mat1, ncomp=4)
#   pca2 <- pca(mat2, ncomp=2)
#   
#   bm <- block_projector(list(pca1,pca2))
#   pp <- pre_processor(bm,center=FALSE, scale=FALSE)
#   pdat <- pre_process(pp)
#   expect_equal(ncol(pdat), 6)
#   expect_equal(project(bm), pdat)
# })
# 
# test_that("can preprocess a block projector with newdata", {
#   mat1 <- matrix(rnorm(10*15), 10, 15)
#   mat2 <-  matrix(rnorm(10*10), 10, 10)
#   pca1 <- pca(mat1, ncomp=4)
#   pca2 <- pca(mat2, ncomp=2)
#   
#   bm <- block_projector(list(pca1,pca2))
#   pp <- pre_processor(bm,center=FALSE, scale=FALSE)
#   
#   mat3 <- cbind(mat1,mat2)
#   pdat <- pre_process(pp,mat3)
#   
#   expect_equal(ncol(pdat), 6)
#   expect_equal(project(bm), pdat)
# })
# 
# test_that("can preprocess a block projector with newdata from a sub-block", {
#   mat1 <- matrix(rnorm(10*15), 10, 15)
#   mat2 <-  matrix(rnorm(10*10), 10, 10)
#   pca1 <- pca(mat1, ncomp=4)
#   pca2 <- pca(mat2, ncomp=2)
#   
#   bm <- block_projector(list(pca1,pca2))
#   pp <- pre_processor(bm,center=FALSE, scale=FALSE)
#   
#   mat3 <- cbind(mat2)
#   pdat <- pre_process(pp,mat3, block_index=2)
#   
#   expect_equivalent(project(bm, block_index=2), unclass(pdat))
# })
</file>

<file path="README.Rmd">
---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# multivarious

<!-- badges: start -->
[![Codecov test coverage](https://codecov.io/gh/bbuchsbaum/multivarious/branch/master/graph/badge.svg)](https://app.codecov.io/gh/bbuchsbaum/multivarious?branch=master)
<!-- badges: end -->


This package is intended to provide some basic abstractions and default implementations of basic computational infrastructure for multivariate component-based modeling such as principal components analysis.

The main idea is to model multivariate decompositions as involving projections from an input data space to a lower dimensional component space. This idea is encapsulated by the `projector` class and the `project` function. Support for two-way mapping (row projection and column projection) is provided by the derived class `bi-projector`. Generic functions for common operations are included:

- `project` for mapping from input space into (usually) reduced-dimensional output space
- `partial_project` for mapping a subset of input space into output space
- `project_vars` for mapping new variables ("supplementary variables") to output space
- `reconstruct` for reconstructing input data from its low-dimensional representation
- `residuals` for extracting residuals of a fit with `n` components.


## Installation

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("bbuchsbaum/multivarious")
```
## Example

This is a basic example which shows you how to solve a common problem:

```{r example}
library(multivarious)
## basic example code
```
</file>

<file path="R/nystrom_embedding.R">
#' Nyström approximation for kernel-based decomposition (Unified Version)
#'
#' Approximate the eigen-decomposition of a large kernel matrix K using either
#' the standard Nyström method (Williams & Seeger, 2001) or the Double Nyström method 
#' (Lim et al., 2015, Algorithm 3). 
#'
#' The Double Nyström method introduces an intermediate step that reduces the
#' size of the decomposition problem, potentially improving efficiency and scalability.
#'
#' @param X A numeric matrix or data frame of size (N x D), where N is number of samples.
#' @param kernel_func A kernel function with signature `kernel_func(X, Y, ...)`.
#'   If NULL, defaults to a linear kernel: `X %*% t(Y)`.
#' @param ncomp Number of components (eigenvectors/eigenvalues) to return. 
#'   Cannot exceed the number of landmarks. Default capped at `length(landmarks)`.
#' @param landmarks A vector of row indices (1-based, from X) specifying the landmark points.
#'   If NULL, `nlandmarks` points are sampled uniformly at random.
#' @param nlandmarks The number of landmark points to sample if `landmarks` is NULL. Default is 10.
#' @param preproc A pre-processing pipeline object (e.g., from `prep()`) or a pre-processing function 
#'   (default `pass()`) to apply before computing the kernel.
#' @param method Either "standard" (the classic single-stage Nyström) or "double" (the two-stage Double Nyström method).
#' @param center Logical. If TRUE, attempts kernel centering. Default FALSE. 
#'   **Note:** True kernel centering (required for equivalence to Kernel PCA) is 
#'   computationally expensive and **not fully implemented**. Setting `center=TRUE` currently only 
#'   issues a warning. For results equivalent to standard PCA, use a linear kernel 
#'   and center the input data `X` (e.g., via `preproc`). See Details.
#' @param l Intermediate rank for the double Nyström method. Ignored if `method="standard"`.
#'   Typically, `l < length(landmarks)` to reduce complexity.
#' @param use_RSpectra Logical. If TRUE, use `RSpectra::svds` for partial SVD. Recommended for large problems.
#' @param ... Additional arguments passed to `kernel_func`.
#'
#' @return A `bi_projector` object with class "nystrom_approx" and additional fields:
#' \describe{
#'   \item{\code{v}}{The eigenvectors (N x ncomp) approximating the kernel eigenbasis.}
#'   \item{\code{s}}{The scores (N x ncomp) = v * diag(sdev), analogous to principal component scores.}
#'   \item{\code{sdev}}{The square roots of the eigenvalues.}
#'   \item{\code{preproc}}{The pre-processing pipeline used.}
#'   \item{\code{meta}}{A list containing parameters and intermediate results used (method, landmarks, kernel_func, etc.).}
#' }
#'
#' @details 
#' **Kernel Centering:** Standard Kernel PCA requires the kernel matrix K to be centered 
#' in the feature space (Schölkopf et al., 1998). This implementation currently 
#' **does not perform kernel centering** by default (`center=FALSE`) due to computational complexity. 
#' Consequently, with non-linear kernels, the results approximate the eigen-decomposition 
#' of the *uncentered* kernel matrix, and are not strictly equivalent to Kernel PCA. 
#' If using a linear kernel, centering the input data `X` (e.g., using `preproc=prep(center())`) 
#' yields results equivalent to standard PCA, which is often sufficient.
#' 
#' **Standard Nyström:** Uses the method from Williams & Seeger (2001), including the 
#' `sqrt(m/N)` scaling for eigenvectors and `N/m` for eigenvalues (`m` landmarks, `N` samples).
#' 
#' **Double Nyström:** Implements Algorithm 3 from Lim et al. (2015).
#'
#' @references
#' Schölkopf, B., Smola, A., & Müller, K. R. (1998). Nonlinear component analysis as a kernel eigenvalue problem. 
#' *Neural computation*, 10(5), 1299-1319.
#' 
#' Williams, C. K. I., & Seeger, M. (2001). Using the Nyström Method to Speed Up Kernel Machines. 
#' In *Advances in Neural Information Processing Systems 13* (pp. 682-688).
#' 
#' Lim, D., Jin, R., & Zhang, L. (2015). An Efficient and Accurate Nystrom Scheme for Large-Scale Data Sets. 
#' *Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence* (pp. 2765-2771).
#'
#' @importFrom RSpectra svds
#' @importFrom stats rnorm
#' @export
#'
#' @examples
#' set.seed(123)
#' # Smaller example matrix
#' X <- matrix(rnorm(1000*300), 1000, 300)
#' 
#' # Standard Nyström
#' res_std <- nystrom_approx(X, ncomp=5, nlandmarks=50, method="standard")
#' print(res_std)
#' 
#' # Double Nyström
#' res_db <- nystrom_approx(X, ncomp=5, nlandmarks=50, method="double", l=20)
#' print(res_db)
#' 
#' # Projection (using standard result as example)
#' scores_new <- project(res_std, X[1:10,])
#' head(scores_new)
nystrom_approx <- function(X, kernel_func=NULL, ncomp=NULL, 
                           landmarks=NULL, nlandmarks=10, preproc=pass(), 
                           method=c("standard","double"), 
                           center=FALSE, # Added center argument
                           l=NULL, use_RSpectra=TRUE, ...) {
  
  method <- match.arg(method)
  
  # Basic checks
  chk::chkor(chk::vld_matrix(X), chk::vld_s4_class(X, "Matrix"))
  N <- nrow(X)
  
  # If no landmarks given, sample them
  if (is.null(landmarks)) {
    if (nlandmarks > N) {
      warning("Number of landmarks requested exceeds number of samples. Using N landmarks.")
      nlandmarks <- N
    }
    if (nlandmarks <= 0) {
        stop("'nlandmarks' must be positive.")
    }
    landmarks <- sort(sample(N, nlandmarks))
  }
  m <- length(landmarks)
  
  # Validate ncomp
  if (is.null(ncomp)) {
      ncomp <- min(m, ncol(X)) # Default ncomp capped by landmarks
  } else {
      chk::chk_whole_number(ncomp)
      chk::chk_gt(ncomp, 0)
      if (ncomp > m) {
          warning(sprintf("Requested ncomp (%d) exceeds number of landmarks (%d). Setting ncomp = %d.", ncomp, m, m))
          ncomp <- m
      }
  }
  
  # Ensure kernel_func is valid
  if (!is.null(kernel_func) && !is.function(kernel_func)) {
    stop("kernel_func must be a function or NULL.")
  }
  
  # Default to linear kernel if none provided
  if (is.null(kernel_func)) {
    kernel_func <- function(X, Y, ...) X %*% t(Y)
  }
  
  # Handle preproc argument (accept function or prep object)
  if (!inherits(preproc, "pre_processor")) {
      if (is.function(preproc)) {
          proc <- prep(preproc)
      } else {
          stop("'preproc' must be a 'prep' object or a pre-processing function.")
      }
  } else {
      proc <- preproc
  }
  
  # Placeholder check for centering - not implemented yet
  if (center) {
      warning("Kernel centering (center=TRUE) is requested but not yet implemented. Proceeding with uncentered kernel.")
      # Future: implement kernel centering logic here or within kernel calls
  }
  
  X_preprocessed <- init_transform(proc, X)
  
  # Determine sets
  non_landmarks <- setdiff(seq_len(N), landmarks)
  
  X_l <- X_preprocessed[landmarks, , drop=FALSE]         
  X_nl <- if (length(non_landmarks) > 0) X_preprocessed[non_landmarks, , drop=FALSE] else matrix(0, 0, ncol(X_preprocessed))
  
  # Compute kernel submatrices
  K_mm <- kernel_func(X_l, X_l, ...)
  K_nm <- if (length(non_landmarks) > 0) kernel_func(X_nl, X_l, ...) else matrix(0, 0, m)
  
  # Function for partial SVD or full eigen if needed, with re-orthogonalization for eigen
  low_rank_decomp <- function(M, k) {
    # Check if k exceeds dimensions or if RSpectra should not be used
    if (k >= nrow(M) || !use_RSpectra) {
      # full eigen decomposition
      eig <- eigen(M, symmetric=TRUE)
      # Select top k and re-orthogonalize
      vecs <- eig$vectors[, 1:k, drop=FALSE]
      # Re-orthogonalize using QR
      vecs_ortho <- qr.Q(qr(vecs))
      return(list(d = eig$values[1:k], v = vecs_ortho))
    } else {
      # partial SVD using RSpectra
      sv <- tryCatch(RSpectra::svds(M, k=k, nu=0, nv=k), # only need V
                     error = function(e) {
                         warning(sprintf("RSpectra::svds failed: %s. Falling back to eigen().", e$message))
                         NULL
                     })
      
      if (is.null(sv)) {
         # Fallback to eigen if svds fails
         eig <- eigen(M, symmetric=TRUE)
         vecs <- eig$vectors[, 1:k, drop=FALSE]
         vecs_ortho <- qr.Q(qr(vecs))
         return(list(d = eig$values[1:k], v = vecs_ortho))
      } else {
          # svds returns U,D,V with M = U D V^T
          # For symmetric M, eigenvectors are V (or U)
          # Eigenvalues are d^2 (or d if M is PD and svd(sqrt(M)) was used, but here it's svd(M))
          # sv$v are right singular vectors = eigenvectors for symmetric M
          # We assume sv$v is already orthonormal
          return(list(d = sv$d^2, v = sv$v)) # lambda = sigma^2
      }
    }
  }
  
  meta_info <- list(method = method, 
                    landmarks = landmarks, 
                    kernel_func = kernel_func, 
                    ncomp = ncomp, 
                    center = center,
                    extra_args = list(...)) # Store extra args passed to kernel
  
  if (method == "standard") {
    # Standard Nyström with proper scaling
    eig_mm <- eigen(K_mm, symmetric=TRUE)
    lambda_mm_raw <- eig_mm$values
    U_mm_raw <- eig_mm$vectors
    
    # Filter out near-zero eigenvalues using scaled epsilon
    eps <- max(lambda_mm_raw) * .Machine$double.eps * 100 
    keep <- which(lambda_mm_raw > eps)
    if (length(keep) == 0) stop("No significant eigenvalues found in K_mm.")
    
    # Keep only up to ncomp components
    keep <- keep[seq_len(min(ncomp, length(keep)))]
    lambda_mm <- lambda_mm_raw[keep]
    U_mm <- U_mm_raw[, keep, drop=FALSE]
    k_eff <- length(lambda_mm) # Effective number of components
    
    # Approximate eigenvalues of full K: lambda_hat = (N/m) * lambda_mm
    lambda_hat <- (N / m) * lambda_mm
    sdev <- sqrt(lambda_hat)
    
    # Approximate eigenvectors U_hat = sqrt(m/N) * [U_mm ; K_nm %*% U_mm %*% diag(1/lambda_mm)]
    scaling_factor_u <- sqrt(m / N)
    inv_lambda_mm_mat <- diag(1 / lambda_mm, nrow = k_eff, ncol = k_eff)
    U_mm_scaled <- U_mm * scaling_factor_u
    U_nm_scaled <- (K_nm %*% (U_mm %*% inv_lambda_mm_mat)) * scaling_factor_u
    
    U_full <- matrix(0, N, k_eff)
    U_full[landmarks, ] <- U_mm_scaled
    if (length(non_landmarks) > 0) {
      U_full[non_landmarks, ] <- U_nm_scaled
    }
    
    # Scores s = U_hat * diag(sdev_hat)
    # s <- U_full %*% diag(sdev, nrow=k_eff) # Old way
    s <- sweep(U_full, 2, sdev, "*") # Efficient way
    
    # Store results
    out <- bi_projector(
      v = U_full, 
      s = s, 
      sdev = sdev, 
      preproc = proc, 
      classes = c("nystrom_approx", "standard"),
      meta = c(meta_info, list( 
        lambda_mm = lambda_mm, # Eigenvalues of K_mm used
        U_mm = U_mm           # Eigenvectors of K_mm used
      )),
      ...
    )
    return(out)
    
  } else { # method == "double"
    # Double Nyström Method
    if (is.null(l)) stop("For method='double', you must specify intermediate rank 'l'.")
    if (l <= 0 || l > m) stop("Intermediate rank 'l' must be > 0 and <= number of landmarks.")
    if (l < ncomp) {
        warning(sprintf("Intermediate rank l (%d) is less than final ncomp (%d). Setting l = ncomp.", l, ncomp))
        l <- ncomp
    }
    
    # 1. Approximate principal subspace of K_mm to rank l
    approx_l <- low_rank_decomp(K_mm, l)
    lambda_l_raw <- approx_l$d
    V_S_l_raw <- approx_l$v
    
    # Filter near-zero eigenvalues
    eps_l <- max(lambda_l_raw) * .Machine$double.eps * 100
    keep_l <- which(lambda_l_raw > eps_l)
    if (length(keep_l) == 0) stop("No significant eigenvalues found in K_mm first stage (rank l). Reduce l?")
    
    l_eff <- length(keep_l)
    if (l_eff < ncomp) {
        warning(sprintf("Effective rank after first stage (%d) is less than requested ncomp (%d). Proceeding with %d components.", l_eff, ncomp, l_eff))
        ncomp <- l_eff # Adjust final ncomp down
    }
    lambda_l <- lambda_l_raw[keep_l]
    V_S_l <- V_S_l_raw[, keep_l, drop=FALSE]
    
    # Compute K_(X,L) for all X once - potentially large N x m matrix
    K_all_landmarks <- kernel_func(X_preprocessed, X_l, ...)
    
    # Construct W = K_all_landmarks * (V_S_l * Lambda_l^{-1/2}) 
    # Using robust diag for the inverse sqrt lambda matrix
    inv_sqrt_lambda_l_mat <- diag(1 / sqrt(lambda_l), nrow = l_eff, ncol = l_eff)
    W <- K_all_landmarks %*% (V_S_l %*% inv_sqrt_lambda_l_mat)
    
    # 2. Compute K_W = W^T W (l x l) and do final low-rank approximation to ncomp
    K_W <- crossprod(W)
    approx_k <- low_rank_decomp(K_W, ncomp) # ncomp might have been adjusted down
    lambda_k_raw <- approx_k$d
    V_k_raw <- approx_k$v
    
    # Filter near-zero eigenvalues for final step
    eps_k <- max(lambda_k_raw) * .Machine$double.eps * 100
    keep_k <- which(lambda_k_raw > eps_k)
    if (length(keep_k) == 0) stop("No significant eigenvalues found in second stage (K_W).")
    
    k_eff <- length(keep_k)
    if (k_eff < ncomp) {
        warning(sprintf("Effective rank after second stage (%d) is less than requested ncomp (%d). Final components: %d.", k_eff, ncomp, k_eff))
    }
    lambda_k <- lambda_k_raw[keep_k]
    V_k <- V_k_raw[, keep_k, drop=FALSE]
    
    # Final eigenvalues = lambda_k 
    sdev <- sqrt(lambda_k)
    
    # Final eigenvectors U_k = W * V_k * Lambda_k^{-1/2}
    inv_sqrt_lambda_k_mat <- diag(1 / sqrt(lambda_k), nrow = k_eff, ncol = k_eff)
    U_k <- W %*% (V_k %*% inv_sqrt_lambda_k_mat)
    
    # Scores = U_k * diag(sdev)
    # s <- U_k %*% diag(sdev, nrow=k_eff) # Old way
    s <- sweep(U_k, 2, sdev, "*") # Efficient way
    
    # Store intermediate results needed for projection
    out <- bi_projector(
      v = U_k, 
      s = s,
      sdev = sdev,
      preproc = proc,
      classes = c("nystrom_approx", "double"),
      meta = c(meta_info, list(
          # Intermediate results needed for projection:
          V_S_l = V_S_l, # U_mm in standard Nystrom context (eigenvectors of K_mm subset)
          inv_sqrt_lambda_l = inv_sqrt_lambda_l_mat, # Diagonal matrix
          V_k = V_k, # Eigenvectors of K_W
          inv_sqrt_lambda_k = inv_sqrt_lambda_k_mat, # Diagonal matrix
          # Store original eigenvalues for reference if needed?
          lambda_l_raw = lambda_l_raw,
          lambda_k_raw = lambda_k_raw
        )
      ),
      ...
    )
    return(out)
  }
}


#' Project new data using a Nyström approximation model
#'
#' @param x A `nystrom_approx` object (inheriting from `bi_projector`).
#' @param new_data New data matrix to project.
#' @param ... Additional arguments (currently ignored).
#' @return A matrix of projected scores.
#' @export
project.nystrom_approx <- function(x, new_data, ...) {
  # Extract necessary info from the meta slot
  meta <- x$meta
  if (is.null(meta) || is.null(meta$method)) {
      stop("Nystrom model object 'x' appears incomplete or corrupted (missing meta information).")
  }
  
  kernel_func <- meta$kernel_func
  landmarks <- meta$landmarks
  # X_l <- x$X_landmarks # This might not be stored directly anymore
  # Need to recompute X_l from original X if not stored, or store it.
  # Let's assume X_l IS stored or reconstructable via preproc if needed
  # We need X_l for K(new, X_l). It should be retrievable if preproc is identity or stored.
  # For now, assume it was stored (though it wasn't explicitly in constructor output). 
  # It is better to store X_l in the object.
  # Let's modify the constructor to store X_l in meta.
  if (is.null(meta$X_landmarks)) {
     stop("Cannot project: Landmark data (X_l) not found in the model object.")
  }
  X_l <- meta$X_landmarks
  
  # Use the *projector* object for reprocess to get consistent preprocessing
  new_data_p <- reprocess(x, new_data)
  K_new_landmark <- kernel_func(new_data_p, X_l, meta$extra_args)
  
  if (meta$method == "double") {
    # Double Nyström projection:
    # approx_scores(new) = K_new_landmark * V_S_l * inv_sqrt_lambda_l * V_k * inv_sqrt_lambda_k * diag(sdev)
    # where sdev = sqrt(lambda_k)
    
    if (is.null(meta$V_S_l) || is.null(meta$inv_sqrt_lambda_l) || is.null(meta$V_k) || is.null(meta$inv_sqrt_lambda_k)){
        stop("Double Nystrom model object 'x' is missing necessary components for projection.")
    }
    
    # Combine projection matrices
    proj_matrix <- meta$V_S_l %*% meta$inv_sqrt_lambda_l %*% meta$V_k %*% meta$inv_sqrt_lambda_k
    # approx_eigenvectors <- K_new_landmark %*% proj_matrix
    # approx_scores <- sweep(approx_eigenvectors, 2, x$sdev, "*")
    
    # Or, more directly: Scores = K(new,L) * ProjMatrix * diag(sdev)
    scores <- K_new_landmark %*% (proj_matrix %*% diag(x$sdev, nrow=length(x$sdev), ncol=length(x$sdev)))
    
  } else { # method == "standard"
    # Standard Nyström projection:
    # approx_scores(new) = K_new_landmark * U_mm * Lambda_mm^{-1/2} * diag(sdev_hat)
    # where sdev_hat = sqrt( (N/m) * lambda_mm )
    
    if (is.null(meta$U_mm) || is.null(meta$lambda_mm)) {
        stop("Standard Nystrom model object 'x' is missing necessary components for projection.")
    }
    
    lambda_mm <- meta$lambda_mm
    U_mm <- meta$U_mm
    k_eff <- length(lambda_mm)
    
    # Ensure lambda_mm > 0 before sqrt
    lambda_mm[lambda_mm <= 0] <- .Machine$double.eps # Replace non-positive with small value
    
    # Calculate the projection matrix: U_mm * Lambda_mm^{-1/2} * diag(sdev_hat)
    # inv_sqrt_lambda_mm_mat <- diag(1 / sqrt(lambda_mm), nrow = k_eff, ncol = k_eff)
    # proj_matrix <- U_mm %*% inv_sqrt_lambda_mm_mat %*% diag(x$sdev, nrow=k_eff, ncol=k_eff)
    
    # Simpler: Proj = U_mm * diag( 1/lambda_mm * sdev_hat ) = U_mm * diag( sqrt(N/m / lambda_mm) )
    N <- nrow(x$v) # Get N from the stored eigenvectors dimension
    m <- length(meta$landmarks)
    proj_vector <- sqrt((N/m) / lambda_mm) # Vector for diagonal matrix
    
    # Handle potential Inf/NaN if lambda_mm was zero
    proj_vector[!is.finite(proj_vector)] <- 0 
    
    proj_matrix <- U_mm %*% diag(proj_vector, nrow=k_eff, ncol=k_eff)
    
    scores <- K_new_landmark %*% proj_matrix
  }
  
  scores
}
</file>

<file path="R/bi_projector_union.R">
#' A Union of Concatenated `bi_projector` Fits
#'
#' This function combines a set of `bi_projector` fits into a single `bi_projector` instance.
#' The new instance's weights and associated scores are obtained by concatenating the weights
#' and scores of the input fits.
#'
#' @param fits A list of `bi_projector` instances with the same row space. These instances
#'   will be combined to create a new `bi_projector` instance.
#' @param outer_block_indices An optional list of indices for the outer blocks. If not provided,
#'   the function will compute the indices based on the dimensions of the input fits.
#'
#' @examples
#'
#' X1 <- matrix(rnorm(5*5), 5, 5)
#' X2 <- matrix(rnorm(5*5), 5, 5)
#'
#' bpu <- bi_projector_union(list(pca(X1), pca(X2)))
#'
#' @return A new `bi_projector` instance with concatenated weights, scores, and other
#'   properties from the input `bi_projector` instances.
#' @export
#' @import chk
bi_projector_union <- function(fits, outer_block_indices=NULL) {
  chk::chk_all(fits, chk::chk_s3_class, "bi_projector")
  
  if (is.null(outer_block_indices)) {
    nv <- sapply(fits, function(f) shape(f)[1])
    offsets <- cumsum(c(1, nv))
    outer_block_indices <- lapply(1:length(nv), function(i) {
      seq(offsets[i], offsets[i] + nv[i]-1)
    })
  } else {
    nv <- sapply(fits, function(f) shape(f)[1])
    chk::chk_equal(nv, sapply(outer_block_indices, length))
  }
  
  v <- do.call(cbind, lapply(fits, coef))
  s <- do.call(cbind, lapply(fits, scores))
  sdev <- sapply(fits, sdev)
  
  cpreproc <- concat_pre_processors(lapply(fits, "[[", "preproc"), outer_block_indices)
    
  ret <- bi_projector(
    v=v,
    s=s,
    sdev=sdev,
    preproc=cpreproc,
    fits=fits,
    outer_block_indices=outer_block_indices,
    classes="bi_projector_union"
  )
    
}


#' @export
print.bi_projector_union <- function(x, ...) {
  cat("A bi_projector_union object with the following properties:\n\n")
  
  cat("Combined bi_projector instances:\n")
  num_instances <- length(x$fits)
  cat("  Number of instances: ", num_instances, "\n")
  
  cat("\nDimensions of the weights (v) matrix:\n")
  cat("  Rows: ", nrow(x$v), " Columns: ", ncol(x$v), "\n")
  
  cat("\nDimensions of the scores (s) matrix:\n")
  cat("  Rows: ", nrow(x$s), " Columns: ", ncol(x$s), "\n")
  
  cat("\nLength of the standard deviations (sdev) vector:\n")
  cat("  Length: ", length(x$sdev), "\n")
  
  cat("\nPreprocessing information:\n")
  print(x$preproc, ...)
  
  cat("\nOuter block indices:\n")
  print(x$outer_block_indices, ...)
  
  invisible(x)
}
</file>

<file path="R/composed_projector.R">
#' Compose Multiple Partial Projectors
#'
#' Creates a `composed_partial_projector` object that applies partial projections sequentially.
#' If multiple projectors are composed, the column indices (colind) used at each stage must be considered.
#'
#' @param ... A sequence of projectors that implement `partial_project()`, optionally named.
#' @return A `composed_partial_projector` object.
#' @export
#'
compose_partial_projector <- function(...) {
  args <- list(...)
  arg_names <- names(args)
  if (is.null(arg_names)) {
    arg_names <- paste0("stage_", seq_along(args))
  } else {
    # Ensure unnamed stages get default names
    unnamed_idx <- which(arg_names == "")
    if (length(unnamed_idx) > 0) {
      arg_names[unnamed_idx] <- paste0("stage_", unnamed_idx)
    }
  }
  # Check for duplicate names
  if (anyDuplicated(arg_names)) {
    stop("Duplicate stage names provided: ", 
         paste(arg_names[duplicated(arg_names)], collapse = ", "))
  }
  names(args) <- arg_names # Assign final names back to the list
  
  lapply(args, function(p) chk::chk_s3_class(p, "projector"))
  
  if (length(args) == 1) {
    return(args[[1]])
  }
  
  shapelist <- lapply(args, shape)
  for (i in 2:length(args)) {
    chk::chk_equal(shapelist[[i-1]][2], shapelist[[i]][1])
  }
  
  out <- structure(
    list(projectors = args),
    class = c("composed_partial_projector", "composed_projector", "projector"),
    .cache = new.env(parent = emptyenv()),
    stage_names = arg_names,
    index_map = NULL # Placeholder for lineage tracking
  )
  out
}


#' Partial Project Through a Composed Partial Projector
#'
#' Applies `partial_project()` through each projector in the composition.
#' If `colind` is a single vector, it applies to the first projector only. Subsequent projectors apply full columns.
#' If `colind` is a list, each element specifies the `colind` for the corresponding projector in the chain.
#'
#' @param x A `composed_partial_projector` object.
#' @param new_data The input data matrix or vector.
#' @param colind A numeric vector or a list of numeric vectors/NULLs. 
#'   If a single vector, applies to the first projector only. 
#'   If a list, its length should ideally match the number of projectors. 
#'   `colind[[i]]` specifies the column indices (relative to the *input* of stage `i`) 
#'   to use for the partial projection at stage `i`. A `NULL` entry means use full projection 
#'   for that stage. If the list is shorter than the number of stages, `NULL` (full projection) 
#'   is assumed for remaining stages. If a single numeric vector is provided, it is treated 
#'   as `list(colind, NULL, NULL, ...)` for backward compatibility (partial only at first stage).
#' @param ... Additional arguments passed to `partial_project()` or `project()` methods.
#'
#' @return The partially projected data after all projectors are applied.
#' @export
partial_project.composed_partial_projector <- function(x, new_data, colind = NULL, ...) {
  projs <- x$projectors
  n_proj <- length(projs)
  
  if (n_proj == 0) return(new_data) # No projectors, return data as is
  
  # Standardize colind format to a list (user provides indices relative to ORIGINAL input)
  if (!is.list(colind)) {
    if (is.null(colind)) {
      colind_list_orig <- rep(list(NULL), n_proj)
    } else {
      chk::chk_vector(colind)
      chk::chk_numeric(colind)
      colind_list_orig <- c(list(colind), rep(list(NULL), n_proj - 1))
    }
  } else {
    if (length(colind) > n_proj) {
      warning("Length of 'colind' list is greater than the number of stages. Extra elements ignored.")
      colind_list_orig <- colind[1:n_proj]
    } else if (length(colind) < n_proj) {
      colind_list_orig <- c(colind, rep(list(NULL), n_proj - length(colind)))
    } else {
      colind_list_orig <- colind
    }
    lapply(colind_list_orig, function(ci) {
      if (!is.null(ci)) {
        chk::chk_vector(ci)
        chk::chk_numeric(ci)
      }
    })
  }
  
  # Handle input data shape
  if (is.vector(new_data)) {
    new_data <- matrix(new_data, nrow = 1)
  }
  chk::vld_matrix(new_data)
  
  # Initialize mapping: Input columns for stage 1 are original columns 1:p
  # This assumes the user provides data corresponding ONLY to the original cols 
  # specified in colind_list_orig[[1]] if it's not NULL.
  p_initial <- shape(projs[[1]])[1]
  current_input_to_orig_map <- 1:p_initial

  # Apply projectors sequentially
  current_data <- new_data
  for (i in 1:n_proj) {
    proj <- projs[[i]]
    stage_colind_orig <- colind_list_orig[[i]] # User-specified ORIGINAL indices
    
    if (is.null(stage_colind_orig)) {
      # Full projection for this stage
      # Input data (`current_data`) columns correspond to `current_input_to_orig_map`
      current_data <- project(proj, current_data, ...)
      # Output map retains all output dimensions, but lineage isn't tracked here yet
      # For now, assume the output columns map 1:k relative to the output
      # A full lineage map would be needed to track origin precisely.
      # Placeholder: The new map just indicates the dimension
      current_input_to_orig_map <- 1:shape(proj)[2] 
    } else {
      # Partial projection for this stage
      
      # 1. Find which columns of `current_data` correspond to `stage_colind_orig`
      #    We need the positions (local indices) within `current_data` that match 
      #    the original indices provided by the user.
      local_indices_to_use <- which(current_input_to_orig_map %in% stage_colind_orig)
      
      if (length(local_indices_to_use) == 0) {
           stop("Stage ", i, ": None of the requested original column indices (", 
                paste(stage_colind_orig, collapse=","), ") were found in the input data to this stage (",
                "which corresponds to original indices: ", paste(current_input_to_orig_map, collapse=","),").")
      }
      
      # 2. Check if the provided `current_data` actually contains these columns
      #    The user must provide input `new_data` that aligns with the first non-NULL colind.
      #    For subsequent stages, `current_data` *is* the input.
      #    The number of columns in the input data for this stage MUST match the number 
      #    of *available* input columns passed from the previous stage.
      #    The `partial_project` call below needs data ONLY for the `local_indices_to_use`.
      if (i == 1) {
          # First stage: ncol(new_data) must match length(stage_colind_orig)
           chk::chk_equal(ncol(current_data), length(stage_colind_orig))
           data_for_partial_proj <- current_data # User supplied correctly subsetted data
           # The local indices *for the projector* are simply 1:length(stage_colind_orig)
           # because the data is already subsetted. But the projector's `partial_project`
           # needs the indices relative to *its* full input space (original indices).
           # --> This implies partial_project methods need to accept indices relative to original.
           # Let's assume partial_project(proj, data_subset, colind_orig) works.
           # We pass the subsetted data and the ORIGINAL indices it corresponds to.
           current_data <- partial_project(proj, data_for_partial_proj, stage_colind_orig, ...)

      } else {
          # Subsequent stages: Select the necessary columns from the output of the previous stage
          data_for_partial_proj <- current_data[, local_indices_to_use, drop = FALSE]
          # The `partial_project` method needs indices relative to *its* full input dimension.
          # The full input dimension corresponds to the output of the previous stage.
          # The `local_indices_to_use` are exactly those indices.
          current_data <- partial_project(proj, data_for_partial_proj, local_indices_to_use, ...)
      }
      
      # 3. Update the map for the next stage's input
      # The output columns map 1:k relative to the output.
      # Lineage isn't fully tracked without a proper index_map structure.
      # Placeholder: The new map just indicates the dimension
      current_input_to_orig_map <- 1:shape(proj)[2]
    }
  }
  
  current_data
}


#' @export
project.composed_projector <- function(x, new_data, ...) {
  if (is.vector(new_data)) {
    new_data <- matrix(new_data, nrow=1)
  }
  
  chk::vld_matrix(new_data)
  
  # Apply each projector in sequence
  for (proj in x$projectors) {
    new_data <- project(proj, new_data, ...)
  }
  
  new_data
}

#' @export
print.composed_projector <- function(x, ...) {
  n_proj <- length(x$projectors)
  stage_names <- attr(x, "stage_names", exact = TRUE)
  if (is.null(stage_names)) { # Fallback if attribute missing somehow
      stage_names <- paste0("stage_", seq_len(n_proj))
  }
  
  cat("Composed projector object:\n")
  cat("  Number of projectors: ", n_proj, "\n")
  
  if (n_proj > 0) {
      cat(" Pipeline:\n")
      shapes <- lapply(x$projectors, shape)
      # Determine max name length for alignment
      max_name_len <- max(nchar(stage_names))
      
      # Print initial input dimension
      # cat(sprintf("  %-*s %4d vars\n", max_name_len + 2, "Input", shapes[[1]][1]))
      
      for (i in 1:n_proj) {
          stage_name <- stage_names[i]
          in_dim <- shapes[[i]][1]
          out_dim <- shapes[[i]][2]
          # Right-align stage number
          stage_label <- sprintf("%-*s", max_name_len, stage_name)
          cat(sprintf("  %2d. %s : %4d -> %4d\n", i, stage_label, in_dim, out_dim))
      }
  }
  invisible(x)
}

#' Get Coefficients of a Composed Projector
#'
#' Calculates the effective coefficient matrix that maps from the original 
#' input space (of the first projector) to the final output space (of the 
#' last projector). This is done by multiplying the coefficient matrices 
#' of all projectors in the sequence.
#'
#' @param object A `composed_projector` object.
#' @param ... Currently unused.
#'
#' @return A matrix representing the combined coefficients.
#' @export
coef.composed_projector <- function(object, ...) {
  cache_env <- attr(object, ".cache", exact = TRUE)
  use_caching <- !is.null(cache_env) && is.environment(cache_env)
  key <- "combined_coef"
  
  if (use_caching && !is.null(cache_env[[key]])) {
    return(cache_env[[key]])
  }
  
  projs <- object$projectors
  if (length(projs) == 0) {
    # Or should it return NULL or an identity matrix of appropriate size?
    # Returning NULL seems safest if there are no projectors.
    return(NULL) 
  }
  
  # Get coefficient matrix from the first projector
  combined_coef <- coef(projs[[1]])
  
  # Sequentially multiply by subsequent coefficient matrices
  if (length(projs) > 1) {
    for (i in 2:length(projs)) {
      # Ensure coef method exists for this projector stage
      stage_coef <- coef(projs[[i]]) 
      if (is.null(combined_coef) || is.null(stage_coef)) {
          warning(paste("Cannot compute combined coefficient matrix because stage", 
                        i, "or previous stages lack coefficients."))
          return(NULL)
      }
      # Matrix multiplication: V_combined = V_prev %*% V_stage_i
      combined_coef <- combined_coef %*% stage_coef
    }
  }
  
  # Store in cache if available
  if (use_caching) {
    cache_env[[key]] <- combined_coef
  }
  
  combined_coef
}

#' Compose Projectors Sequentially (Pipe Operator)
#' 
#' This infix operator provides syntactic sugar for composing projectors sequentially.
#' It is an alias for \code{\link{compose_partial_projector}}.
#' 
#' @param lhs The left-hand side projector (or a composed projector).
#' @param rhs The right-hand side projector to add to the sequence.
#' 
#' @return A \code{composed_partial_projector} object representing the combined sequence.
#' @export
#' @rdname compose_partial_projector
`%>>%` <- function(lhs, rhs) {
   # Check if lhs is already a composed projector
   # Ensure rhs is a list containing the single projector
   if (!is.list(rhs) || length(rhs) != 1 || !inherits(rhs[[1]], "projector")) {
       # If rhs is a projector, wrap it in a list for consistent handling
       if (inherits(rhs, "projector")) {
           rhs_list <- list(rhs)
           names(rhs_list) <- paste0("stage_", length(list(...)) + 1) # Needs context or better default naming
           # Better: Use a placeholder name if none provided in original call context
           rhs_list <- list(stage_new = rhs) 
           # Need to check if name 'stage_new' collides! This logic needs care.
       } else {
           stop("`rhs` must be a projector object.")
       }
   } else {
       # Assume rhs was provided like list(new_stage_name = projector_obj)
       rhs_list <- rhs
   }
   
   lhs_projs <- if (inherits(lhs, "composed_projector")) lhs$projectors else list(lhs)
   lhs_names <- if (inherits(lhs, "composed_projector")) attr(lhs, "stage_names") else "stage_1"
   
   rhs_projs <- rhs_list
   rhs_names <- names(rhs_list)
   if (is.null(rhs_names) || rhs_names == "") rhs_names <- paste0("stage_", length(lhs_projs) + 1)
   
   # Combine projectors and check for name collisions
   all_projs <- c(lhs_projs, rhs_projs)
   all_names <- c(lhs_names, rhs_names)
   
   # Ensure unique names - simple appending suffix for now
   if (anyDuplicated(all_names)) {
       warning("Duplicate stage names detected during composition with `%>>%`. Appending suffixes.")
       all_names <- make.unique(all_names, sep = ".")
   }
   
   # Use do.call with named list
   do.call(compose_partial_projector, setNames(all_projs, all_names))
}

#' @export
#' @rdname variables_used
variables_used.composed_projector <- function(x, tol = 1e-8, ...) {
    combined_coef <- coef(x)
    if (is.null(combined_coef)) {
        warning("Cannot determine variables used: combined coefficients are NULL.")
        return(numeric(0))
    }
    
    # Find rows (original variables) with any non-zero coefficient across all components
    row_sums_sq <- rowSums(combined_coef^2)
    used_indices <- which(row_sums_sq > tol^2)
    
    sort(unique(used_indices))
}

#' @export
#' @rdname vars_for_component
vars_for_component.composed_projector <- function(x, k, tol = 1e-8, ...) {
    combined_coef <- coef(x)
    if (is.null(combined_coef)) {
        warning("Cannot determine variables used: combined coefficients are NULL.")
        return(numeric(0))
    }
    
    # Check if component k is valid
    if (k < 1 || k > ncol(combined_coef)) {
        stop("Component index 'k' (", k, ") is out of bounds [1..", ncol(combined_coef), "].")
    }
    
    # Find rows (original variables) with non-zero coefficient for component k
    component_k_coefs <- combined_coef[, k]
    used_indices <- which(abs(component_k_coefs) > tol)
    
    sort(unique(used_indices))
}

#' Compute the Inverse Projection for a Composed Projector
#'
#' Calculates the pseudo-inverse of the composed projector, mapping from the 
#' final output space back towards the original input space. This is computed
#' by multiplying the pseudo-inverses of the individual projector stages in 
#' reverse order: `V_k+ %*% ... %*% V_2+ %*% V_1+`.
#' 
#' Requires that each stage implements the `inverse_projection` method.
#'
#' @param x A `composed_projector` object.
#' @param ... Additional arguments passed to the underlying `inverse_projection` methods.
#'
#' @return A matrix representing the combined pseudo-inverse.
#' @export
inverse_projection.composed_projector <- function(x, ...) {
    cache_env <- attr(x, ".cache", exact = TRUE)
    use_caching <- !is.null(cache_env) && is.environment(cache_env)
    key <- "combined_inv_proj"
    
    if (use_caching && !is.null(cache_env[[key]])) {
        return(cache_env[[key]])
    }
    
    projs <- x$projectors
    n_proj <- length(projs)
    
    if (n_proj == 0) {
        warning("Cannot compute inverse projection: no projectors in the composition.")
        return(NULL)
    }
    
    # Get inverse projection from the *last* projector first
    combined_inv_proj <- inverse_projection(projs[[n_proj]], ...)
    
    # Sequentially pre-multiply by preceding inverse projection matrices
    if (n_proj > 1) {
        # Iterate from second-to-last down to first
        for (i in (n_proj - 1):1) {
            stage_inv_proj <- inverse_projection(projs[[i]], ...)
            
            if (is.null(combined_inv_proj) || is.null(stage_inv_proj)) {
                warning(paste("Cannot compute combined inverse projection because stage", 
                              i, "or subsequent stages lack an inverse projection."))
                return(NULL)
            }
            
            # Matrix multiplication: V_combined+ = V_{i+1...n}+ %*% V_i+
            combined_inv_proj <- combined_inv_proj %*% stage_inv_proj
        }
    }
    
    # Store in cache if available
    if (use_caching) {
        cache_env[[key]] <- combined_inv_proj
    }
    
    combined_inv_proj
}

#' Reconstruct Data from Scores using a Composed Projector
#'
#' Maps scores from the final latent space back towards the original input space
#' using the composed projector's combined inverse projection. Requires scores
#' to be provided explicitly.
#' 
#' Attempts to apply the `reverse_transform` of the *first* stage's preprocessor
#' to return data in the original units. If the first stage preprocessor is 
#' unavailable or invalid, a warning is issued, and data is returned in the 
#' (potentially) preprocessed space of the first stage.
#'
#' @param x A `composed_projector` object.
#' @param scores A numeric matrix of scores (observations x components) in the 
#'   final latent space of the composed projector.
#' @param comp Numeric vector of component indices (columns of `scores`, rows of 
#'   `inverse_projection`) to use for reconstruction. Defaults to all components.
#' @param rowind Numeric vector of row indices (observations in `scores`) to 
#'   reconstruct. Defaults to all rows.
#' @param colind Numeric vector of original variable indices (columns of the final 
#'   reconstructed matrix) to return. Defaults to all original variables.
#' @param ... Additional arguments (currently unused).
#'
#' @return A matrix representing the reconstructed data, ideally in the original 
#'   data space.
#' @export
reconstruct.composed_projector <- function(x, scores, comp = NULL, rowind = NULL, colind = NULL, ...) {
  chk::chk_s3_class(x, "composed_projector")
  chk::chk_matrix(scores)

  projs <- x$projectors
  n_proj <- length(projs)
  
  if (n_proj == 0) {
      warning("Cannot reconstruct: no projectors in the composition.")
      # If no projectors, reconstruction is just the input scores?
      # Or should return error? Let's return scores for now.
      return(scores)
  }

  # --- Argument Validation --- 
  n_comp_final <- shape(projs[[n_proj]])[2] # Num components output by LAST stage
  n_vars_orig <- shape(projs[[1]])[1]      # Num variables input to FIRST stage

  # Validate score dimensions against final stage output
  chk::chk_equal(ncol(scores), n_comp_final)

  # Default component/row/col indices
  if (is.null(comp)) comp <- 1:n_comp_final
  if (is.null(rowind)) rowind <- 1:nrow(scores)
  if (is.null(colind)) colind <- 1:n_vars_orig

  # Validate indices
  chk::chk_vector(comp)
  chk::chk_vector(rowind)
  chk::chk_vector(colind)
  chk::chk_subset(comp, 1:n_comp_final)
  chk::chk_subset(rowind, 1:nrow(scores))
  chk::chk_subset(colind, 1:n_vars_orig)
  
  # Subset initial scores based on requested components and rows
  # Note: 'comp' refers to components in the *final* space
  current_recon <- scores[rowind, comp, drop = FALSE]
  
  # --- Iterative Reconstruction --- 
  # Iterate backwards from the last stage to the first
  for (i in n_proj:1) {
      proj_i <- projs[[i]]
      preproc_i <- proj_i$preproc
      inv_proj_i <- inverse_projection(proj_i)
      
      if (is.null(inv_proj_i)) {
          stop("Cannot reconstruct: Stage ", i, " (", names(projs)[i],
               ") lacks an inverse_projection method or its result is NULL.")
      }
      
      # --- Corrected Order --- 
      # 1. Apply the linear inverse projection of stage i FIRST
      #    Input: current_recon (in the output space of stage i)
      #    Output: reconstruction in the input space of stage i (before preproc i)
      num_comps_in_current <- ncol(current_recon)
      if (num_comps_in_current > nrow(inv_proj_i)) {
          stop("Dimension mismatch during reconstruction at stage ", i,
               ": Input components (", num_comps_in_current,
               ") exceed available rows in inverse projection (", nrow(inv_proj_i), ")")
      }
      # Select rows of V_i+ corresponding to the components in current_recon
      inv_proj_i_sub <- inv_proj_i[1:num_comps_in_current, , drop = FALSE]
      # Apply the linear inverse projection
      current_recon <- current_recon %*% inv_proj_i_sub
      
      # 2. Reverse the pre-processing of stage i SECOND
      #    Input: result from step 1 (in input space of stage i, but potentially still centered/scaled)
      #    Output: reconstruction in the input space of stage i (output space of stage i-1)
      if (!is.null(preproc_i) && inherits(preproc_i, "pre_processor")) {
          # Pass the result of the inverse projection to reverse_transform
          current_recon <- reverse_transform(preproc_i, current_recon)
      } # else: If no preprocessor, current_recon is already correct for this step
      
      # ------------------------
  }
  
  # --- Final Column Selection --- 
  # After the loop, current_recon is in the original variable space
  # Select the requested original columns
  final_recon <- current_recon[, colind, drop = FALSE]
  
  return(final_recon)
}

#' Truncate a Composed Projector
#' 
#' Reduces the number of output components of the composed projector by 
#' truncating the *last* stage in the sequence. 
#' 
#' Note: This implementation currently only supports truncating the final stage.
#' Truncating intermediate stages would require re-computing subsequent stages 
#' or combined attributes and is not yet implemented.
#' 
#' @param x A `composed_projector` object.
#' @param ncomp The desired number of final output components.
#' @param ... Currently unused.
#' 
#' @return A new `composed_projector` object with the last stage truncated.
#' @export
truncate.composed_projector <- function(x, ncomp, ...) {
    chk::chk_s3_class(x, "composed_projector")
    chk::chk_number(ncomp, x_arg = "ncomp")
    chk::chk_gt(ncomp, 0)
    
    projs <- x$projectors
    n_proj <- length(projs)
    
    if (n_proj == 0) {
        warning("Cannot truncate: no projectors in the composition.")
        return(x)
    }
    
    # Truncate the last projector
    last_proj <- projs[[n_proj]]
    last_proj_truncated <- truncate(last_proj, ncomp)
    
    # Replace the last projector with the truncated version
    projs_new <- projs
    projs_new[[n_proj]] <- last_proj_truncated
    
    # Re-compose (this also updates attributes like stage_names)
    # Need !!! to splice the list elements as arguments
    # Also need to preserve original names
    new_comp <- do.call(compose_partial_projector, setNames(projs_new, attr(x, "stage_names")))
    
    # Clear the cache of the new object
    cache_env <- attr(new_comp, ".cache", exact = TRUE)
    if (!is.null(cache_env) && is.environment(cache_env)) {
        rm(list = ls(cache_env), envir = cache_env)
    }
    
    # Note: Index map is likely invalidated by truncation but not recalculated here.
    # Set index_map to NULL in the new object to indicate it's invalid?
    # attr(new_comp, "index_map") <- NULL # Optional: Explicitly invalidate
    
    return(new_comp)
}

#' Summarize a Composed Projector
#' 
#' Provides a summary of the stages within a composed projector, including 
#' stage names, input/output dimensions, and the primary class of each stage.
#' 
#' @param object A `composed_projector` object.
#' @param ... Currently unused.
#' 
#' @return A `tibble` summarizing the pipeline stages.
#' @export
#' @importFrom tibble tibble
summary.composed_projector <- function(object, ...) {
  projs <- object$projectors
  n_proj <- length(projs)
  stage_names <- attr(object, "stage_names", exact = TRUE)
  
  if (n_proj == 0) {
    return(tibble::tibble(
      stage = integer(0),
      name = character(0),
      in_dim = integer(0),
      out_dim = integer(0),
      class = character(0)
    ))
  }
  
  if (is.null(stage_names) || length(stage_names) != n_proj) {
      stage_names <- paste0("stage_", seq_len(n_proj))
  }
  
  shapes <- lapply(projs, shape)
  in_dims <- vapply(shapes, `[`, 1L, 1)
  out_dims <- vapply(shapes, `[`, 1L, 2)
  classes <- vapply(projs, function(p) class(p)[1], character(1))
  
  tibble::tibble(
     stage   = seq_len(n_proj),
     name    = stage_names,
     in_dim  = in_dims,
     out_dim = out_dims,
     class   = classes
  )
}
</file>

<file path="R/twoway_projector.R">
#' Two-way (cross) projection to latent components
#'
#' A projector that reduces two blocks of data, X and Y, yielding a pair of weights for each component.
#' This structure can be used, for example, to store weights derived from canonical correlation analysis.
#'
#' @details This class extends `projector` and therefore basic operations such as `project`, `shape`, `reprocess`,
#' and `coef` work, but by default, it is assumed that the `X` block is primary. To access `Y` block operations, an
#' additional argument `source` must be supplied to the relevant functions, e.g., `coef(fit, source = "Y")`
#'
#' @param vx the X coefficients
#' @param vy the Y coefficients
#' @param preproc_x the X pre-processor
#' @param preproc_y the Y pre-processor
#' @param ... extra parameters or results to store
#' @param classes additional class names
#' @return a cross_projector object
#' @export
#' @examples
#' # Create two scaled matrices X and Y
#' X <- scale(matrix(rnorm(10 * 5), 10, 5))
#' Y <- scale(matrix(rnorm(10 * 5), 10, 5))
#'
#' # Perform canonical correlation analysis on X and Y
#' cres <- cancor(X, Y)
#' sx <- X %*% cres$xcoef
#' sy <- Y %*% cres$ycoef
#'
#' # Create a cross_projector object using the canonical correlation analysis results
#' canfit <- cross_projector(cres$xcoef, cres$ycoef, cor = cres$cor,
#'                           sx = sx, sy = sy, classes = "cancor")
cross_projector <- function(vx, vy, preproc_x=prep(pass()), preproc_y=prep(pass()), 
                             ..., classes=NULL) {
  
  chk::chkor(chk::chk_matrix(vx), chk::chk_s4_class(vx, "Matrix"))
  chk::chkor(chk::chk_matrix(vy), chk::chk_s4_class(vy, "Matrix"))
  chk::chk_s3_class(preproc_x, "pre_processor")
  chk::chk_s3_class(preproc_y, "pre_processor")
  
  
  out <- structure(
    list(
      v=vx,
      vx=vx,
      vy=vy,
      preproc=preproc_x,
      preproc_x=preproc_x,
      preproc_y=preproc_y,
      .cache = new.env(parent = emptyenv()),
      ...),
    class= c(classes, "cross_projector", "projector")
  )
  
  out
}

#' project a cross_projector instance
#' 
#' @inheritParams project
#' @param source the source of the data (X or Y block)
#' @return the projected data
#' @export
#' @family project
project.cross_projector <- function(x, new_data, source=c("X", "Y"),...) {
  source <- match.arg(source)
  chk::vld_matrix(new_data)
  
  if (is.vector(new_data)) {
    chk::chk_equal(length(new_data), shape(x, source=source)[1])
    new_data <- matrix(new_data, byrow=TRUE, ncol=length(new_data))
  }
  
  chk::check_dim(new_data, ncol, values=nrow(coef.cross_projector(x, source=source)))
  reprocess(x, new_data, source=source) %*% coef.cross_projector(x, source=source)
}

#' Extract coefficients from a cross_projector object
#'
#' @param object the model fit
#' @param source the source of the data (X or Y block), either "X" or "Y"
#' @param ... extra args
#' @return the coefficients
#' @export
coef.cross_projector <- function(object, source=c("X", "Y"),...) {
  source <- match.arg(source)
  if (source == "X") {
    object$vx
  } else {
    object$vy
  }
}

#' reprocess a cross_projector instance
#' 
#' @inheritParams reprocess
#' @param source the source of the data (X or Y block)
#' @return the re(pre-)processed data
#' @export
#' @family reprocess
reprocess.cross_projector <- function(x, new_data, colind=NULL, source=c("X", "Y"), ...) {
  source <- match.arg(source)
  if (is.null(colind)) {
    chk::chk_equal(ncol(new_data), nrow(coef.cross_projector(x, source=source)))
    colind <- 1:ncol(new_data)
  } else {
    chk::chk_equal(length(colind), ncol(new_data)) 
  }
    
  if (source == "X") {
    apply_transform(x$preproc_x, new_data, colind)
  } else {
    apply_transform(x$preproc_y, new_data, colind)
  }
  
}

#' Partially project data for a cross_projector
#'
#' Projects new data from either the X or Y domain onto the latent subspace,
#' considering only a specified subset of original features (`colind`).
#'
#' @param x A `cross_projector` object.
#' @param new_data A numeric matrix (n x length(colind)) or vector, representing
#'   the observations corresponding to the columns specified by `colind`.
#' @param colind A numeric vector of column indices in the original data space
#'   (either X or Y domain, specified by `source`) that correspond to `new_data`'s columns.
#' @param least_squares Logical; if TRUE (default), use ridge-regularized least squares for projection.
#' @param lambda Numeric; ridge penalty (default 1e-6). Ignored if `least_squares=FALSE`.
#' @param source Character, either "X" or "Y", indicating which domain `new_data` and `colind` belong to.
#' @param ... Additional arguments (currently ignored).
#'
#' @return A numeric matrix (n x d) of factor scores in the latent subspace.
#' @export
partial_project.cross_projector <- function(x, new_data, colind,
                                            least_squares=TRUE,
                                            lambda=1e-6,
                                            source=c("X","Y"),
                                            ...) {
  source <- match.arg(source)
  
  # reprocess with colind
  nd_proc <- apply_transform(x$preproc_x, new_data, colind)
  
  # subset columns in v
  v_full  <- coef.cross_projector(x, source=source)   # shape (p x d)
  v_sub   <- v_full[colind, , drop=FALSE]     # shape (|colind| x d)
  
  if (least_squares) {
    inv_vtv_sub <- robust_inv_vTv(v_sub, lambda=lambda)
    factor_scores <- nd_proc %*% v_sub %*% inv_vtv_sub
  } else {
    factor_scores <- nd_proc %*% v_sub
  }
  factor_scores
}

#' Transfer from X domain to Y domain (or vice versa) in a cross_projector
#'
#' @inherit transfer description
#' @param x A `cross_projector` object.
#' @param new_data The data to transfer.
#' @param from Source domain ("X" or "Y").
#' @param to Target domain ("X" or "Y").
#' @param opts A list of options (see `transfer` generic).
#' @param ... Ignored.
#' @return Transferred data matrix.
#' @importFrom utils modifyList
#' @importFrom cli cli_abort
#' @export
transfer.cross_projector <- function(x, new_data,
                                     from, to,
                                     opts = list(),
                                     ...) {

  defaults <- list(cols    = NULL,   # Target columns subset
                   comps   = NULL,   # Latent components subset (currently unused in this method)
                   ls_rr   = FALSE,  # Ridge LS for forward projection?
                   lambda  = 1e-6)
  opts <- utils::modifyList(defaults, opts, keep.null = TRUE)

  # Validate from/to arguments
  # (Ideally use block_names(x) if available, but hardcoding for now)
  from <- match.arg(from, c("X", "Y"))
  to   <- match.arg(to,   c("X", "Y"))
  if (from == to)
    cli::cli_abort("{.arg from} ('{from}') and {.arg to} ('{to}') must differ.")

  # ---------- 1. preprocess & dimension sanity ----------
  px <- if (from == "X") x$preproc_x else x$preproc_y
  # Apply transform, expecting full data columns corresponding to `from` domain
  nd_proc <- apply_transform(px, new_data) 
  
  p_expected <- shape(x, source = from)[1] # Expected number of features in `from` domain
  # Check input data *after* transform matches expected feature dimension
  chk::chk_equal(ncol(nd_proc), p_expected)

  # Allow row-vector input (nd_proc might become a vector after apply_transform)
  if (is.vector(nd_proc)) nd_proc <- matrix(nd_proc, nrow = 1)

  # ---------- 2. forward projection ----------------------
  # Project the preprocessed data into the latent space
  # Note: project generic might take `opts` later, but cross_projector method uses ls_rr, lambda directly
  scores <- project(x, nd_proc, source = from,
                    least_squares = opts$ls_rr, lambda = opts$lambda)
                    
  # Subset components if requested
  if (!is.null(opts$comps)) {
      chk::chk_vector(opts$comps)
      chk::chk_subset(opts$comps, 1:ncol(scores))
      scores <- scores[, opts$comps, drop = FALSE]
  }

  # ---------- 3. back-projection to {to} ------------------
  # Get cached inverse for the target domain
  # If opts$comps was used, we need the corresponding columns of the inverse
  inv <- .cache_inv(x, to, colind=opts$comps, lambda = opts$lambda) # Pass opts$comps as colind for cache
  
  rec <- scores %*% inv # Reconstruct using (subsetted) scores and (subsetted) inverse

  # ---------- 4. undo preprocessing for target -----------
  p_to <- if (to == "X") x$preproc_x else x$preproc_y
  # Pass the target columns subset (opts$cols) to reverse_transform
  out  <- reverse_transform(p_to, rec, colind = opts$cols)

  # keep dimnames if sensible
  rownames(out) <- rownames(new_data)
  # Get target coefficient matrix to extract column names if needed
  target_coef_names <- colnames(coef(x, source = to))
  if (!is.null(opts$cols) && !is.null(target_coef_names)) {
      # Ensure opts$cols are valid indices for the target coefficient matrix
      chk::chk_vector(opts$cols)
      chk::chk_subset(opts$cols, 1:length(target_coef_names))
      colnames(out) <- target_coef_names[opts$cols]
  } else if (!is.null(target_coef_names)){
      # If reconstructing all columns, assign all names
      colnames(out) <- target_coef_names
  }

  out
}

#' Default inverse_projection method for cross_projector
#'
#' This function obtains the matrix that maps factor scores in the
#' latent space back into the original domain (X or Y). By default,
#' we assume \code{v_domain} is \emph{not} necessarily orthonormal or invertible,
#' so we use a pseudoinverse approach (e.g. MASS::ginv).
#'
#' @param x A `cross_projector` object.
#' @param domain Either \code{"X"} or \code{"Y"}, indicating which block's inverse 
#'        loading matrix we want (i.e., if you want to reconstruct data in the
#'        X space or Y space).
#' @param ... Additional arguments (currently unused, but may be used by subclasses).
#'
#' @return A matrix that, when multiplied by the factor scores, yields the
#'         reconstruction in the specified domain's original space.
#'
#' @export
#' @examples
#' # Suppose 'cp' is a cross_projector object. If we want the
#' # inverse for the Y domain:
#' #   inv_mat <- inverse_projection(cp, domain="Y")
#' # Then reconstruct:  Yhat <- Fscores %*% inv_mat
inverse_projection.cross_projector <- function(x, domain=c("X","Y"), ...) {
  domain <- match.arg(domain)
  
  # We'll retrieve the loadings for the target domain.
  v_mat <- coef.cross_projector(x, source=domain)
  if (!requireNamespace("MASS", quietly = TRUE)) {
    stop("Package 'MASS' is required for the default pseudoinverse approach.")
  }
  
  # Use ginv() for a minimal-norm pseudoinverse.
  inv_mat <- MASS::ginv(v_mat)
  
  inv_mat
}

#' Partial Inverse Projection of a Subset of the Loading Matrix in cross_projector
#'
#' This function obtains the "inverse" mapping for a columnwise subset of the loading
#' matrix in the specified domain. In practice, if \code{v_mat} is not orthonormal
#' or not square, we use a pseudoinverse approach (via \code{MASS::ginv}).
#'
#' By default, this is a minimal-norm solution for partial columns of \code{v_mat}.
#' If you need a different approach (e.g., ridge, direct solve, etc.), you can override
#' this method in your specific class or code.
#'
#' @param x A `cross_projector` object.
#' @param colind A numeric vector specifying the columns (indices) of the \emph{latent factors}
#'        or loadings to invert. Typically these correspond to a subset of canonical components
#'        or principal components, etc.
#' @param domain Either \code{"X"} or \code{"Y"}, indicating which block's partial
#'        loadings we want to invert.
#' @param ... Additional arguments (unused by default, but may be used by subclasses).
#'
#' @return A matrix of shape \code{(length(colind) x p_block)} that, when multiplied
#'         by factor scores restricted to \code{colind} columns, yields an
#'         \code{(n x p_block)} reconstruction in the original domain block.
#'
#' @export
#' @examples
#' # Suppose 'cp' is a cross_projector, and we want only columns 1:3 of
#' # the Y block factors. Then:
#' #   inv_mat_sub <- partial_inverse_projection(cp, colind=1:3, domain="Y")
#' # The shape will be (3 x pY), so factor_scores_sub (n x 3) %*% inv_mat_sub => (n x pY).
partial_inverse_projection.cross_projector <- function(x, colind, domain=c("X","Y" ), ...) {
  domain <- match.arg(domain)
  chk::chk_vector(colind)
  chk::chk_all_pos_int(colind)
  
  # Robust caching check
  cache_env <- x$.cache
  use_caching <- !is.null(cache_env) && is.environment(cache_env)
  key <- paste0("partial_inv_proj_", domain, "@", paste0(sort(unique(colind)), collapse = "_"))

  if (use_caching && !is.null(cache_env[[key]])) {
    return(cache_env[[key]])
  }
  
  # Compute if not cached or cache not available
  v_mat <- coef.cross_projector(x, source=domain)  # shape = (p_block x d_total)
  chk::chk_range(max(colind, na.rm=TRUE), c(1, ncol(v_mat)))
  chk::chk_range(min(colind, na.rm=TRUE), c(1, ncol(v_mat)))

  if (!requireNamespace("MASS", quietly = TRUE)) {
    stop("Package 'MASS' is required for the default pseudoinverse approach.")
  }
  
  v_sub <- v_mat[, colind, drop=FALSE]  # shape = (p_block x length(colind))
  inv_sub <- MASS::ginv(v_sub)  # minimal-norm pseudoinverse

  # Store in cache if available
  if (use_caching) {
    cache_env[[key]] <- inv_sub
  }
  
  inv_sub
}

#' shape of a cross_projector instance
#' 
#' @param source the source of the data (X or Y block)
#' @return the shape of the data
#' @export
#' @family shape
#' @inheritParams shape
shape.cross_projector <- function(x, source=c("X", "Y"), ...) {
  source <- match.arg(source)
  if (source == "X") {
    c(nrow(x$vx), ncol(x$vx))
  } else {
    c(nrow(x$vy), ncol(x$vy))
  }
}



#' @seealso \code{\link{perm_test}}, \code{\link{measure_interblock_transfer_error}}, 
#'   \code{\link{cross_projector}}
#' @export
perm_test.cross_projector <- function(x,
                                      X,
                                      Y,
                                      nperm = 100,
                                      measure_fun = NULL,
                                      shuffle_fun = NULL,
                                      fit_fun = NULL,
                                      stepwise = FALSE,
                                      parallel = FALSE,
                                      alternative = c("greater", "less", "two.sided"),
                                      ...) 
{
  alternative <- match.arg(alternative)
  
  # Validate dimensions
  if (nrow(X) != nrow(Y)) stop("X and Y must have the same number of rows.")
  
  # Capture extra arguments
  extra_args <- list(...)
  
  # ---------- Default Measure Function (x2y.mse) ----------
  if (is.null(measure_fun)) {
    measure_fun <- function(model_perm, X_orig, Y_perm, ...) {
      # Note: We evaluate the permuted model on the original X and *permuted* Y
      out <- measure_interblock_transfer_error(
        Xtrue = X_orig, Ytrue = Y_perm, 
        model = model_perm, metrics="x2y.mse"
      )
      out[["x2y.mse"]]  # single numeric
    }
    stat_name <- "x2y.mse (default)"
  } else {
    stat_name <- deparse(substitute(measure_fun))
  }
  
  # ---------- Default Shuffle Function (shuffle rows of Y) ----------
  if (is.null(shuffle_fun)) {
    shuffle_fun <- function(Xblock, Yblock, ...) {
      idx <- sample(nrow(Yblock))
      Yblock[idx, , drop=FALSE]
    }
  }
  
  # ---------- Default Fit Function (stats::cancor) ----------
  if (is.null(fit_fun)) {
    fit_fun <- function(Xtrain, Ytrain, preproc_x_orig, preproc_y_orig, ...) {
      if (!requireNamespace("stats", quietly = TRUE)) {
          stop("Package 'stats' (for cancor) needed for default fit_fun. Please install it.", call. = FALSE)
      }
      ccr_args <- c(list(x = Xtrain, y = Ytrain), list(...))
      ccr <- try(do.call(stats::cancor, ccr_args), silent = TRUE)
      if (inherits(ccr, "try-error")) {
        warning(sprintf("Default fit_fun (stats::cancor) failed: %s. Returning NULL.", ccr))
        return(NULL)
      }
      # Need original preprocessors to build the new projector
      cross_projector(ccr$xcoef, ccr$ycoef,
                      preproc_x = preproc_x_orig, 
                      preproc_y = preproc_y_orig)
    }
  }
  
  # ---------- Observed Statistic ----------
  # Evaluate original model on original data
  measure_args_obs <- c(list(model_perm = x, X_orig = X, Y_perm = Y), extra_args)
  obs_stat <- try(do.call(measure_fun, measure_args_obs), silent = TRUE)
  
  if (inherits(obs_stat, "try-error") || !is.numeric(obs_stat) || length(obs_stat) != 1) {
      warning(sprintf("Could not calculate observed statistic using measure_fun: %s", obs_stat))
      obs_stat <- NA_real_
  }
  if (is.na(obs_stat)) {
      warning("Observed statistic is NA. Permutation test cannot proceed meaningfully.")
      # Return early with NA results? Or let it run and return NA p-value? Let it run for now.
  }

  # ---------- Permutation Loop Function ----------
  one_perm <- function(i, ...) {
    # Shuffle Y
    shuffle_args <- c(list(Xblock = X, Yblock = Y), extra_args)
    Yperm <- do.call(shuffle_fun, shuffle_args)
    
    # Refit model on X and permuted Y, passing original preprocessors
    fit_args <- c(list(Xtrain = X, Ytrain = Yperm, 
                       preproc_x_orig = x$preproc_x, preproc_y_orig = x$preproc_y), 
                  extra_args)
    new_mod <- try(do.call(fit_fun, fit_args), silent = TRUE)
    if (inherits(new_mod, "try-error") || is.null(new_mod)) {
        warning(sprintf("Permutation %d: fit_fun failed: %s. Returning NA.", i, ifelse(is.null(new_mod), "NULL return", new_mod)))
        return(NA_real_)
    }
    
    # Calculate statistic using permuted model and permuted Y
    measure_args_perm <- c(list(model_perm = new_mod, X_orig = X, Y_perm = Yperm), extra_args)
    stat_perm <- try(do.call(measure_fun, measure_args_perm), silent = TRUE)
    
    if (inherits(stat_perm, "try-error")) {
        warning(sprintf("Permutation %d: measure_fun failed: %s. Returning NA.", i, stat_perm))
        return(NA_real_)
    }
     if (!is.numeric(stat_perm) || length(stat_perm) != 1) {
        warning(sprintf("Permutation %d: measure_fun did not return a single numeric value. Returning NA.", i))
        return(NA_real_)
    }
    stat_perm
  }
  
  # ---------- Run Permutations (Serial or Parallel) ----------
  message(sprintf("Running %d permutations for cross projector (%s)...", 
                  nperm, if(parallel) "parallel" else "serial"))
  apply_fun <- if (parallel) future.apply::future_lapply else lapply
  perm_args <- list(X = seq_len(nperm), FUN = one_perm)
  # Pass ... down to one_perm via the lapply function's ...
  if (parallel) perm_args$future.seed <- TRUE
  perm_args <- c(perm_args, extra_args) # Add ... to the apply_fun call
  
  perm_vals_list <- do.call(apply_fun, perm_args)
  perm_vals <- unlist(perm_vals_list)
  n_complete <- sum(!is.na(perm_vals))
  
  if (n_complete < nperm) {
      warning(sprintf("%d permutations failed and were excluded.", nperm - n_complete))
      perm_vals <- stats::na.omit(perm_vals)
  }
  if (n_complete == 0) {
      stop("All permutations failed. Cannot compute p-value.")
  }
  
  # ---------- P-value Calculation (Empirical with +1 smoothing) ----------
  pval <- NA # Initialize
  if (!is.na(obs_stat)) { # Only calculate p-value if observed stat is valid
      if (alternative == "greater") {
        b <- sum(perm_vals >= obs_stat, na.rm = TRUE)
        pval <- (b + 1) / (n_complete + 1)
      } else if (alternative == "less") {
        b <- sum(perm_vals <= obs_stat, na.rm = TRUE)
        pval <- (b + 1) / (n_complete + 1)
      } else { # two.sided
        b_greater <- sum(perm_vals >= obs_stat, na.rm = TRUE)
        b_less <- sum(perm_vals <= obs_stat, na.rm = TRUE)
        pval <- 2 * min((b_greater + 1) / (n_complete + 1), (b_less + 1) / (n_complete + 1))
        pval <- min(pval, 1.0) # Ensure p-value doesn't exceed 1
      }
  }
  
  # ---------- Output Structure ----------
  out <- structure(
    list(
      statistic = obs_stat,
      perm_values = perm_vals,
      p.value = pval,
      alternative = alternative,
      method = sprintf("Permutation test for cross_projector (measure: %s)", stat_name),
      nperm = n_complete, 
      call = match.call()
    ),
    class = "perm_test"
  )
  
  out
}

#' @export
print.cross_projector <- function(x,...) {
  cat("cross projector: ", paste0(class(x)), "\n")
  cat("input dim (X): ", shape(x, source="X")[1], "\n")
  cat("output dim (X): ", shape(x, source="X")[2], "\n")
  cat("input dim (Y): ", shape(x, source="Y")[1], "\n")
  cat("output dim (Y): ", shape(x, source="Y")[2], "\n")
}

# R/cross_projector-transfer.R (or within R/twoway_projector.R)
# --------------------------------------------------------------
# cache pseudoinverses once
# Not exported, internal helper
.cache_inv <- function(x, domain, colind = NULL, lambda = 1e-6) {
  # Robust caching check
  cache_env <- x$.cache
  use_caching <- !is.null(cache_env) && is.environment(cache_env)

  # Create a unique key based on domain and target columns
  key_suffix <- if (!is.null(colind)) paste0(sort(colind), collapse = "_") else "all"
  key <- paste0("inv_", domain, "@", key_suffix)
  
  # Return cached value if available
  if (use_caching && !is.null(cache_env[[key]])) {
      return(get(key, envir = cache_env, inherits = FALSE))
  }

  # Compute if not cached or cache not available
  v <- coef.cross_projector(x, source = domain)
  
  # Apply colind if provided (subsetting components)
  if (!is.null(colind)) {
      chk::chk_vector(colind)
      chk::chk_subset(colind, 1:ncol(v))
      v <- v[, colind, drop = FALSE]
  }
  
  inv <- tryCatch(MASS::ginv(v), error = function(e) NA)
  
  # Fallback to regularized inverse if ginv fails or yields non-finite values
  if (any(!is.finite(inv))) { 
    warning("MASS::ginv failed or returned non-finite values; using regularized inverse.")
    # Calculate t(v) %*% v + lambda*I
    vTv <- crossprod(v)
    reg_inv <- tryCatch(solve(vTv + diag(lambda, ncol(v))) %*% t(v), error = function(e) NA)
    if (any(!is.finite(reg_inv))) { 
        stop("Both ginv and regularized inverse failed for domain ", domain)
    }
    inv <- reg_inv
  }
  
  # Store in cache if available
  if (use_caching) {
    assign(key, inv, envir = cache_env)
  }
  
  inv
}
</file>

<file path="tests/testthat/test_classifier.R">
testthat::context("classifier")

test_that("can construct a pca classifier", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1)
  y <- rep(letters[1:4], length.out=10)
  cl <- classifier(pres, labels=y, new_data=mat1)
 
  p <- predict(cl, mat1)
  expect_true(ncol(p$prob) == 4)
  expect_true(nrow(p$prob) == 10)
})


test_that("can construct a pca classifier with colind", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  pres <- pca(mat1)
  y <- rep(letters[1:4], length.out=10)
  cl <- classifier(pres, labels=y, new_data=mat1[,1:5], colind=1:5)
  
  p <- predict(cl, mat1[,1:5])
  expect_true(ncol(p$prob) == 4)
  expect_true(nrow(p$prob) == 10)
  
  p2 <- predict(cl, mat1[1,1:5])
  expect_true(ncol(p2$prob) == 4)
  expect_true(nrow(p2$prob) == 1)
  
  p2 <- predict(cl, mat1[1,1:5], colind=1:5, metric="euclidean")
  expect_true(ncol(p2$prob) == 4)
  expect_true(nrow(p2$prob) == 1)
  
  p2 <- project(cl, mat1[1,1:5])
  expect_true(ncol(p2) == ncomp(pres))
  
})

test_that("can split a matrix", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  Y <- rep(letters[1:3], length.out=nrow(mat1))
  sm <- split_matrix(mat1,Y)
  expect_equal(3, length(sm))
})

test_that("can compute group means of rows of matrix", {
  mat1 <- matrix(rnorm(10*15), 10, 15)
  Y <- rep(letters[1:3], length.out=nrow(mat1))
  gm <- group_means(Y, mat1)
  expect_equal(nrow(gm), 3)
  
  Y <- letters[1:nrow(mat1)]
  gm <- group_means(Y, mat1)
  expect_equal(nrow(gm), length(Y))
})




test_that("feature_importance.classifier works with iris data and PCA", {
  # Data setup
  data(iris)
  X <- as.matrix(iris[, 1:4])
  labels <- iris[, 5]
  
  # Fit PCA
  pca_res <- pca(X, ncomp = 3)
  
  # Create classifier
  classifier <- classifier.projector(pca_res, labels = labels, new_data = X, knn = 1)
  
  # New data for prediction
  new_data <- X
  
  # Evaluate feature importance using the marginal approach
  importance_marginal <- feature_importance.classifier(classifier, new_data, ncomp = 2, 
                                                       true_labels = labels,
                                                       metric = "euclidean", fun = rank_score, 
                                                       approach = "marginal")
  print(importance_marginal)
  expect_true(all(importance_marginal$importance > 0))
  
  # Evaluate feature importance using the standalone approach
  importance_standalone <- feature_importance.classifier(classifier, new_data, ncomp = 2, 
                                                         true_labels = labels,
                                                         metric = "euclidean", fun = rank_score, 
                                                         approach = "standalone")
  print(importance_standalone)
  expect_true(all(importance_standalone$importance > 0))
})
</file>

<file path="R/svd.R">
#' Singular Value Decomposition (SVD) Wrapper
#'
#' Computes the singular value decomposition of a matrix using one of the specified methods.
#' It is designed to be an easy-to-use wrapper for various SVD methods available in R.
#'
#' @param X the input matrix
#' @param ncomp the number of components to estimate (default: min(dim(X)))
#' @param preproc the pre-processor to apply on the input matrix (e.g., `center()`, `standardize()`, `pass()`) 
#'              Can be a `prepper` object or a pre-processing function.
#' @param method the SVD method to use: 'base', 'fast', 'irlba', 'propack', 'rsvd', or 'svds'
#' @param q parameter passed to method `rsvd` (default: 2)
#' @param p parameter passed to method `rsvd` (default: 10)
#' @param tol minimum relative tolerance for dropping singular values (compared to the largest). Default: `.Machine$double.eps`.
#' @param ... extra arguments passed to the selected SVD function
#' @return an SVD object that extends `bi_projector`
#' @export
#' @importFrom RSpectra svds
#' @importFrom rsvd rsvd
#' @importFrom irlba irlba
#' @importFrom corpcor fast.svd
#' @importFrom svd propack.svd
#' @examples
#' # Load iris dataset and select the first four columns
#' data(iris)
#' X <- iris[, 1:4]
#'
#' # Compute SVD using the base method and 3 components
#' fit <- svd_wrapper(X, ncomp = 3, preproc = center(), method = "base")
svd_wrapper <- function(X, ncomp=min(dim(X)), 
                        preproc=pass(),
                        method=c("fast", "base", "irlba", 
                                 "propack", "rsvd", "svds"), 
                        q=2,
                        p=10,
                        tol=.Machine$double.eps,
                        ...) {
  method <- match.arg(method)
  
  proc <- prep(preproc)
  Xp <- init_transform(proc, X)
  
  # Cap ncomp based on dimensions and method requirements
  max_k <- min(dim(Xp))
  # RSpectra and irlba require k < min(dim), base/fast/propack/rsvd work up to min(dim)
  safe_k <- if (method %in% c("svds", "irlba")) max_k - 1 else max_k
  if (safe_k < 1) stop("Matrix dimensions too small for requested SVD method.")
  
  k <- min(ncomp, safe_k) # Final number of components to request
  if (k < ncomp) {
      warning(sprintf("Requested ncomp=%d reduced to k=%d due to matrix dimensions or method constraints.", ncomp, k))
  }
  if (k <= 0) stop("Cannot request 0 or negative components (k).", call. = FALSE)
  
  res <- tryCatch({ # Wrap the switch in tryCatch for robustness
    switch(method,
           base = svd(Xp, nu = k, nv = k, ...),
           fast = {
               if (!requireNamespace("corpcor", quietly = TRUE)) stop("Package 'corpcor' needed for method='fast'.", call.=FALSE)
               corpcor::fast.svd(Xp, tol = 0) # Use tol later for relative filtering
           },
           rsvd = {
               if (!requireNamespace("rsvd", quietly = TRUE)) stop("Package 'rsvd' needed for method='rsvd'.", call.=FALSE)
               rsvd::rsvd(Xp, k = k, q = q, p = p, ...)
           },
           svds = {
               if (!requireNamespace("RSpectra", quietly = TRUE)) stop("Package 'RSpectra' needed for method='svds'.", call.=FALSE)
               RSpectra::svds(Xp, k = k, ...)
            },
           propack = {
               if (!requireNamespace("svd", quietly = TRUE)) stop("Package 'svd' needed for method='propack'.", call.=FALSE)
               # propack needs neig = number of eigenvalues = k
               svd::propack.svd(Xp, neig = k, ...)
           },
           irlba = {
               if (!requireNamespace("irlba", quietly = TRUE)) stop("Package 'irlba' needed for method='irlba'.", call.=FALSE)
               irlba::irlba(Xp, nu = k, nv = k, ...)
           },
           stop("Unknown SVD method specified: ", method) # Default stop
    )
   }, error = function(e) {
       stop(sprintf("SVD computation failed for method '%s': %s", method, e$message), call. = FALSE)
   })
  
  # Filter based on relative tolerance
  if (length(res$d) == 0) stop("SVD returned zero singular values.")
  keep <- which(res$d > tol * res$d[1])
  
  if (length(keep) == 0) {
    stop("All singular values are below the relative tolerance.")
  }
  
  # Final ncomp is the minimum of originally requested (k) and kept singular values
  ncomp_final <- min(k, length(keep))
  
  d <- res$d[keep[1:ncomp_final]]
  u <- res$u[, keep[1:ncomp_final], drop=FALSE]
  v <- res$v[, keep[1:ncomp_final], drop=FALSE]
  
  bi_projector(v, s=u %*% diag(d, nrow=ncomp_final, ncol=ncomp_final), 
               sdev=d, u=u, preproc=proc, 
               classes="svd", method=method)
}

#' Calculate Standardized Scores for SVD results
#'
#' Computes standardized scores from an SVD result performed by `svd_wrapper`.
#' These scores are scaled to have approximately unit variance, assuming the original
#' data used for SVD was centered. They differ from the `s` component of the 
#' `svd` object, which contains scores scaled by singular values.
#'
#' @param x An object of class `svd`, typically from `svd_wrapper`.
#' @param ... Extra arguments (ignored).
#' @return A matrix of standardized scores (N x k) with columns having variance close to 1.
#' @export
std_scores.svd <- function(x,...) {
  N <- nrow(x$u)
  if (N <= 1) {
      warning("Cannot compute standardized scores with N <= 1. Returning raw U vectors.")
      return(x$u)
  }
  # Scale U vectors to have variance ~1
  sqrt(N - 1) * x$u 
}
</file>

<file path="R/discriminant_projector.R">
#' Construct a Discriminant Projector
#'
#' A `discriminant_projector` is an instance that extends `bi_projector` with a projection that maximizes class separation.
#' This can be useful for dimensionality reduction techniques that take class labels into account, such as Linear Discriminant Analysis (LDA).
#'
#' @param v The projection matrix (often `X %*% v`). Rows correspond to observations, columns to components.
#' @param s The score matrix (often `X %*% v`). Rows correspond to observations, columns to components.
#' @param sdev The standard deviations associated with the scores or components (e.g., singular values from LDA).
#' @param preproc A `prepper` or `pre_processor` object, or a pre-processing function (e.g., `center`, `pass`).
#' @param labels A factor or character vector of class labels corresponding to the rows of `X` (and `s`).
#' @param classes Additional S3 classes to prepend.
#' @param ... Extra arguments passed to `bi_projector`.
#' @return A `discriminant_projector` object.
#'
#' @seealso bi_projector
#' @importFrom stats setNames
#' @export
#' @examples
#' # Simulate data and labels
#' set.seed(123)
#' X <- matrix(rnorm(100 * 10), 100, 10)
#' labels <- factor(rep(1:2, each = 50))
#'
#' # Perform LDA and create a discriminant projector
#' lda_fit <- MASS::lda(X, labels)
#'
#' dp <- discriminant_projector(lda_fit$scaling, X %*% lda_fit$scaling, sdev = lda_fit$svd, 
#' labels = labels)
#' @export
discriminant_projector <- function(v, s, sdev, preproc=prep(pass()), labels, classes=NULL, ...) {
  
  chk::vld_matrix(v)
  chk::vld_matrix(s)
  chk::vld_numeric(sdev)
  chk::chk_equal(length(sdev), ncol(s))
  chk::chk_equal(ncol(v), length(sdev))
  chk::chk_equal(length(labels), nrow(s))
  
  # Ensure labels are factor and get named counts
  labels <- factor(labels)
  counts <- table(labels, dnn = NULL) 
  
  out <- bi_projector(v, s=s, sdev=sdev, preproc=preproc, labels=labels, 
                      counts=counts, classes=c(classes, "discriminant_projector"), ...)
}

#' Predict method for a discriminant_projector, supporting LDA or Euclid
#'
#' This produces class predictions or posterior-like scores for new data. We first
#' project the data into the subspace defined by \code{x$v}, then either:
#' \enumerate{
#'   \item \strong{LDA approach} (\code{method="lda"}), which uses a (simplified)
#'         linear discriminant formula or distance to class means in the subspace
#'         combined with prior probabilities. 
#'   \item \strong{Euclid approach} (\code{method="euclid"}), which uses plain
#'         Euclidean distance to each class mean in the subspace.
#' }
#' We return either a \code{type="class"} label or \code{type="prob"} posterior-like
#' matrix.
#'
#' @param object A \code{discriminant_projector} object (extending \code{bi_projector}),
#'   which has \code{x$v} for column loadings, \code{x$s} for row scores, and
#'   \code{x$labels} for class labels.
#' @param new_data A numeric matrix (or vector) with the same # of columns as
#'   the original data (unless partial). Rows=observations, columns=features.
#' @param method Either \code{"lda"} (the default discriminant approach) or
#'   \code{"euclid"} (simple nearest-mean in subspace).
#' @param type \code{"class"} (default) for predicted class labels or \code{"prob"}
#'   for posterior-like probabilities. 
#' @param ... further arguments (not used or for future expansions).
#'
#' @return 
#' If \code{type="class"}, a factor vector of length n (predicted classes).
#' If \code{type="prob"}, an (n x #classes) numeric matrix of posterior-like values, with row names matching \code{new_data} if available.
#'
#' Predict method for a discriminant_projector
#'
#' This produces class predictions or posterior-like scores for new data, based on:
#' \itemize{
#'   \item \strong{LDA approach} (\code{method="lda"}), which uses a linear discriminant 
#'         formula with a pooled covariance matrix if \code{x\$Sigma} is given, or 
#'         the identity matrix if \code{Sigma=NULL}.
#'   \item \strong{Euclid approach} (\code{method="euclid"}), which uses plain
#'         Euclidean distance to each class mean in the subspace.
#' }
#'
#' We return either a \code{type="class"} label or \code{type="prob"} posterior-like
#' matrix.
#'
#' @param object A \code{discriminant_projector} object.
#' @param new_data A numeric matrix (or vector) with the same # of columns as
#'   the original data (unless partial usage). Rows=observations, columns=features.
#' @param method Either \code{"lda"} (the default) or \code{"euclid"} (nearest-mean).
#' @param type \code{"class"} (default) for predicted class labels, or \code{"prob"}
#'   for posterior-like probabilities. 
#' @param colind (optional) if partial columns are used, specify which columns 
#'   map to the subspace. If \code{NULL}, assume full columns. 
#' @param ... further arguments (not used or for future expansions).
#'
#' @return 
#' If \code{type="class"}, a factor vector of length n (predicted classes).
#' If \code{type="prob"}, an (n x #classes) numeric matrix of posterior-like values.
#' @importFrom stats median
#' @exportS3Method predict discriminant_projector
#' @export
predict.discriminant_projector <- function(object,
                                           new_data,
                                           method = c("lda", "euclid"),
                                           type   = c("class", "prob"),
                                           colind = NULL,
                                           ...) 
{
  method <- match.arg(method)
  type   <- match.arg(type)
  
  # 1) Reprocess new_data, possibly partial columns if colind is set
  #    Note that 'reprocess' can handle colind argument if partial usage is supported
  if (is.null(colind)) {
    new_data_proc <- reprocess(object, new_data) 
  } else {
    new_data_proc <- reprocess(object, new_data, colind = colind)
  }
  
  # ensure new_data_proc is at least 2D
  if (is.vector(new_data_proc)) {
    new_data_proc <- matrix(new_data_proc, nrow = 1)
  }
  
  # Handle potential colind subsetting for v
  if (!is.null(colind)) {
      if (max(colind) > nrow(object$v) || min(colind) < 1) {
           stop(sprintf("Invalid 'colind' provided. Indices must be between 1 and %d.", nrow(object$v)))
      }
      v_use <- object$v[colind, , drop = FALSE]
  } else {
      v_use <- object$v
  }
  
  # multiply by object$v (possibly subsetted) => subspace
  scores_new <- new_data_proc %*% v_use   # shape: (n x d)
  
  # 2) Extract class info from training
  labs         <- object$labels
  class_levels <- levels(as.factor(labs))
  nclass       <- length(class_levels)
  
  # 3) Compute class means in subspace from object$s
  #    object$s has shape (n_train x d)
  class_means <- matrix(NA, nrow = nclass, ncol = ncol(object$s))
  for (i in seq_len(nclass)) {
    rows <- which(labs == class_levels[i])
    class_means[i, ] <- colMeans(object$s[rows, , drop = FALSE])
  }
  
  # 4) If we want real LDA, we can use stored Sigma or identity if not present
  if (method == "lda") {
    sigma_pooled <- object$Sigma
    if (is.null(sigma_pooled)) {
      # fallback to identity
      sigma_pooled <- diag(ncol(object$s))
    }
    inv_sigma <- solve(sigma_pooled)
    
    # Precompute (for linear discriminants):
    means_inv  <- class_means %*% inv_sigma
    means_quad <- rowSums(means_inv * class_means)
    logpri     <- log(object$counts / sum(object$counts))
    
    # Build discriminant matrix
    disc_mat <- matrix(0, nrow = nrow(scores_new), ncol = nclass)
    for (i in seq_len(nclass)) {
      dotvals      <- scores_new %*% means_inv[i, ] 
      disc_mat[,i] <- dotvals - 0.5 * means_quad[i] + logpri[i]
    }
    
    if (type == "class") {
      best_idx <- max.col(disc_mat, ties.method = "first")
      return(factor(class_levels[best_idx], levels = class_levels))
    } else {
      # Softmax rowwise
      disc_exp <- exp(disc_mat - apply(disc_mat, 1, max))
      prob_mat <- disc_exp / rowSums(disc_exp)
      colnames(prob_mat) <- class_levels
      rownames(prob_mat) <- rownames(new_data_proc) # Add row names
      return(prob_mat)
    }
    
  } else {
    # method = "euclid"
    # nearest-mean in subspace
    dist_mat <- matrix(NA, nrow = nrow(scores_new), ncol = nclass)
    for (i in seq_len(nclass)) {
      diff_ij      <- sweep(scores_new, 2, class_means[i, ], FUN = "-")
      dist_mat[,i] <- rowSums(diff_ij^2)
    }
    
    if (type == "class") {
      # pick argmin distance
      best_idx <- max.col(-dist_mat, ties.method = "first")
      return(factor(class_levels[best_idx], levels = class_levels))
    } else {
      # posterior-like = normalized inverse distance
      # Use relative epsilon based on median distance to avoid Inf for zero distances
      eps <- 1e-6 * stats::median(dist_mat[dist_mat > 0], na.rm = TRUE) 
      if (!is.finite(eps) || eps <= 0) {
         eps <- 1e-8 # Fallback if median is non-positive or NaN/Inf
      }
      
      inv_dist <- 1 / (dist_mat + eps)
      prob_mat <- inv_dist / rowSums(inv_dist)
      colnames(prob_mat) <- class_levels
      rownames(prob_mat) <- rownames(new_data_proc) # Add row names
      return(prob_mat)
    }
  }
}



#' @importFrom stats quantile predict na.omit
#' @importFrom future.apply future_lapply
#' @export
#'@family perm_test
#' @seealso \code{\link{perm_test}} for the generic permutation test function
perm_test.discriminant_projector <- function(
    x, X,
    nperm = 1000,
    measure_fun = NULL, 
    fit_fun = NULL,
    shuffle_fun = NULL,
    predict_method = c("lda", "euclid"), # Added explicit argument
    stepwise = FALSE, # Ignored
    parallel = FALSE,
    alternative = c("greater", "less", "two.sided"),
    ...) {
  
  # Match arguments
  alternative <- match.arg(alternative)
  predict_method <- match.arg(predict_method)
  
  # Ensure labels are factor and dimensions match
  y <- factor(x$labels)
  if (nrow(X) != length(y)) {
      stop(sprintf("Number of rows in X (%d) does not match number of labels in x (%d).", nrow(X), length(y)))
  }
  
  # Capture extra arguments to pass down
  extra_args <- list(...)
  
  # ---------- Default Measure Function (Accuracy) ----------
  if (is.null(measure_fun)) {
    measure_fun <- function(model_perm, X_orig, y_perm, predict_method, ...) {
      # Predict using the *permuted* model on original X
      pred_class <- predict(model_perm, X_orig, method = predict_method, type="class")
      
      # Compare predictions to the *permuted* labels (y_perm)
      valid_idx <- !is.na(y_perm)
      if (any(!valid_idx)) {
          y_perm <- y_perm[valid_idx]
          pred_class <- pred_class[valid_idx]
          if (length(y_perm) == 0) return(NA_real_) # Return NA if no valid data
      }
      # Return only the numeric accuracy value
      sum(pred_class == y_perm) / length(y_perm)
    }
    stat_name <- "accuracy (default)"
  } else {
    stat_name <- deparse(substitute(measure_fun))
  }
  
  # ---------- Default Shuffle Function ----------
  if (is.null(shuffle_fun)) {
    shuffle_fun <- function(labels, ...) sample(labels)
  }
  
  # ---------- Default Fit Function (Re-uses original preprocessing & MASS::lda) ----------
  if (is.null(fit_fun)) {
    fit_fun <- function(Xtrain_orig, labelstrain, preproc_obj, ...) {
      # Fix: Use apply_transform instead of reprocess, as preproc_obj is the processor, not the model
      Xtrain_proc <- try(apply_transform(preproc_obj, Xtrain_orig), silent = TRUE)
      if (inherits(Xtrain_proc, "try-error")) {
        # Improved error message
        stop(sprintf("Failed to apply original preprocessor transform in default fit_fun: %s", Xtrain_proc))
      }
      if (!requireNamespace("MASS", quietly = TRUE)) {
        stop("Package 'MASS' needed for default fit_fun. Please install it.", call. = FALSE)
      }
      # Pass ... down to lda if provided
      lda_args <- c(list(x=Xtrain_proc, grouping = labelstrain), list(...))
      lda_fit <- do.call(MASS::lda, lda_args)
      s <- Xtrain_proc %*% lda_fit$scaling
      discriminant_projector(
        v = lda_fit$scaling, s = s, sdev = lda_fit$svd,
        labels = labelstrain, preproc = preproc_obj, Sigma = lda_fit$covariance
      )
    }
  }
  
  # ---------- Observed Statistic ----------
  # Evaluate the original model on the original data with original labels
  # The measure_fun signature expects permuted labels, but for observed stat,
  # we compare model predictions on X to the true labels y.
  # We adapt the call slightly for the observed statistic calculation.
  obs_pred <- predict(x, X, method = predict_method, type = "class")
  valid_obs_idx <- !is.na(y)
  if (any(!valid_obs_idx)) {
      y_valid <- y[valid_obs_idx]
      obs_pred_valid <- obs_pred[valid_obs_idx]
      if (length(y_valid) == 0) {
          obs_stat <- NA_real_
      } else {
          obs_stat <- sum(obs_pred_valid == y_valid) / length(y_valid)
      }
  } else {
      obs_stat <- sum(obs_pred == y) / length(y)
  }
  
  if (!is.numeric(obs_stat) || length(obs_stat) != 1) {
      # This case should primarily happen if measure_fun returns NA
      # or if user provided a measure_fun that returned non-numeric/vector
      if (is.na(obs_stat)) {
          warning("Observed statistic could not be calculated (likely due to NAs). Permutation test cannot proceed meaningfully.")
      } else {
          stop("Internal error or custom measure_fun: Observed statistic is not a single numeric value.")
      }
  }
  
  # ---------- Permutation Loop Function ----------
  one_perm <- function(i, ...) {
    # Create arguments list for shuffle_fun, including extra_args
    shuffle_args <- c(list(labels = y), extra_args)
    y_perm <- do.call(shuffle_fun, shuffle_args)
    
    # Create arguments list for fit_fun, including extra_args
    # Pass original preprocessor object from 'x' explicitly
    fit_args <- c(list(Xtrain_orig = X, labelstrain = y_perm, preproc_obj = x$preproc), extra_args)
    mod <- try(do.call(fit_fun, fit_args), silent = TRUE)
    if (inherits(mod, "try-error")) {
        warning(sprintf("Permutation %d: fit_fun failed: %s. Returning NA.", i, mod))
        return(NA_real_)
    }
    
    # Create arguments list for measure_fun, including predict_method and extra_args
    measure_args <- c(list(model_perm = mod, X_orig = X, y_perm = y_perm, 
                           predict_method = predict_method), extra_args)
    stat_perm <- try(do.call(measure_fun, measure_args), silent = TRUE)
    
    if (inherits(stat_perm, "try-error")) {
        warning(sprintf("Permutation %d: measure_fun failed: %s. Returning NA.", i, stat_perm))
        return(NA_real_)
    }
    if (!is.numeric(stat_perm) || length(stat_perm) != 1) {
        warning(sprintf("Permutation %d: measure_fun did not return a single numeric value. Returning NA.", i))
        return(NA_real_)
    }
    stat_perm
  }
  
  # ---------- Run Permutations (Serial or Parallel) ----------
  message(sprintf("Running %d permutations for discriminant projector (%s)...", 
                  nperm, if(parallel) "parallel" else "serial"))
  apply_fun <- if (parallel) future.apply::future_lapply else lapply
  perm_args <- list(X = seq_len(nperm), FUN = one_perm)
  # Pass ... down to one_perm via the lapply function's ...
  if (parallel) perm_args$future.seed <- TRUE
  perm_args <- c(perm_args, extra_args) # Add ... to the apply_fun call
  
  perm_vals_list <- do.call(apply_fun, perm_args)
  perm_vals <- unlist(perm_vals_list)
  n_complete <- sum(!is.na(perm_vals))
  
  if (n_complete < nperm) {
      warning(sprintf("%d permutations failed and were excluded.", nperm - n_complete))
      perm_vals <- stats::na.omit(perm_vals)
  }
  if (n_complete == 0) {
      stop("All permutations failed. Cannot compute p-value.")
  }
  
  # ---------- P-value Calculation (Empirical with +1 smoothing) ----------
  if (alternative == "greater") {
    b <- sum(perm_vals >= obs_stat, na.rm = TRUE) # Added na.rm for safety
    pval <- (b + 1) / (n_complete + 1)
  } else if (alternative == "less") {
    b <- sum(perm_vals <= obs_stat, na.rm = TRUE)
    pval <- (b + 1) / (n_complete + 1)
  } else { # two.sided
    b_greater <- sum(perm_vals >= obs_stat, na.rm = TRUE)
    b_less <- sum(perm_vals <= obs_stat, na.rm = TRUE)
    pval <- 2 * min((b_greater + 1) / (n_complete + 1), (b_less + 1) / (n_complete + 1))
    pval <- min(pval, 1.0) # Ensure p-value doesn't exceed 1
  }
  
  # ---------- Output Structure ----------
  out <- structure(
    list(
      statistic = obs_stat,
      perm_values = perm_vals,
      p.value = pval,
      alternative = alternative,
      method = sprintf("Permutation test for discriminant_projector (measure: %s)", stat_name),
      nperm = n_complete, 
      call = match.call()
    ),
    class = "perm_test"
  )
  
  out
}

#' Print Method for perm_test Objects
#' 
#' Provides a concise summary of the permutation test results.
#' 
#' @param x An object of class `perm_test`.
#' @param ... Additional arguments passed to printing methods.
#' @return Invisibly returns the input object `x`.
#' @export
print.perm_test <- function(x, ...) {
  cat("\nPermutation Test Results\n\n")
  cat("Method: ", x$method, "\n")
  cat("Alternative: ", x$alternative, "\n")
  cat(sprintf("Observed Statistic = %.4g\n", x$statistic))
  cat(sprintf("Empirical P-value = %.4g (based on %d successful permutations)\n", 
              x$p.value, x$nperm))
  
  # Optionally show CI
  if (!is.null(x$perm_values) && length(x$perm_values) > 1) {
      ci <- stats::quantile(x$perm_values, c(0.025, 0.975), na.rm = TRUE)
      cat(sprintf("95%% CI for Permutation Stats: [%.4g, %.4g]\n", ci[1], ci[2]))
  }
  cat("\n")
  invisible(x)
}

#' @export
print.discriminant_projector <- function(x,...) {
  print.projector(x)
  # Print named counts
  cat("Label counts: \n")
  print(x$counts) 
}
</file>

<file path="R/geneig.R">
#' Generalized Eigenvalue Decomposition
#'
#' Computes the generalized eigenvalues and eigenvectors for the problem: A x = λ B x.
#' Various methods differ in assumptions about A and B.
#'
#' @param A The left-hand side square matrix.
#' @param B The right-hand side square matrix, same dimension as A.
#' @param ncomp Number of eigenpairs to return.
#' @param preproc A preprocessing function to apply to the matrices before solving the generalized eigenvalue problem.
#' @param method One of:
#'   - "robust": Uses a stable decomposition via a whitening transform (B must be symmetric PD).
#'   - "sdiag":  Uses a spectral decomposition of B (must be symmetric PD). Requires A to be symmetric for meaningful results.
#'   - "geigen": Uses the \pkg{geigen} package for a general solution (A and B can be non-symmetric).
#'   - "primme": Uses the \pkg{PRIMME} package for large/sparse symmetric problems (A and B must be symmetric).
#' @param which Only used for `method = "primme"`. Specifies which eigenvalues to compute ("LM", "SM", "LA", "SA", etc.). Default is "LM" (Largest Magnitude). See \code{\link[PRIMME]{eigs_sym}}.
#' @param ... Additional arguments to pass to the underlying solver.
#' @return A `projector` object with generalized eigenvectors and eigenvalues.
#' @seealso \code{\link{projector}} for the base class structure.
#' 
#' #' @references
#' Golub, G. H. & Van Loan, C. F. (2013) *Matrix Computations*,
#'   4th ed., § 8.7 – textbook derivation for the "robust" (Cholesky)
#'   and "sdiag" (spectral) transforms.
#'
#' Moler, C. & Stewart, G. (1973) "An Algorithm for Generalized Matrix
#'   Eigenvalue Problems". *SIAM J. Numer. Anal.*, 10 (2): 241‑256 –
#'   the QZ algorithm behind the \code{geigen} backend.
#'
#' Stathopoulos, A. & McCombs, J. R. (2010) "PRIMME: PReconditioned
#'   Iterative Multi‑Method Eigensolver". *ACM TOMS* 37 (2): 21:1‑21:30 –
#'   the algorithmic core of the \code{primme} backend.
#'
#' See also the \pkg{geigen} (CRAN) and \pkg{PRIMME} documentation.
#'
#' @importFrom PRIMME eigs_sym
#' @importFrom geigen geigen
#' @export
#' @examples
#' # Simulate two matrices
#' set.seed(123)
#' A <- matrix(rnorm(50 * 50), 50, 50)
#' B <- matrix(rnorm(50 * 50), 50, 50)
#' A <- A %*% t(A) # Make A symmetric
#' B <- B %*% t(B) + diag(50) * 0.1 # Make B symmetric positive definite
#'
#' # Solve generalized eigenvalue problem
#' result <- geneig(A = A, B = B, ncomp = 3)
#'
geneig <- function(A = NULL,
                   B = NULL,
                   ncomp = 2,
                   preproc = prep(pass()),
                   method = c("robust", "sdiag", "geigen", "primme"),
                   which = "LR", ...) {
  method <- match.arg(method)
  
  # Validate inputs
  # Restore original check
  stopifnot(is.numeric(A), is.numeric(B)) 
  chk::chk_equal(nrow(A), ncol(A))
  chk::chk_equal(nrow(B), ncol(B))
  chk::chk_equal(nrow(A), nrow(B))
  
  if (!is.numeric(ncomp) || ncomp <= 0 || !chk::vld_whole_number(ncomp)) {
    stop("'ncomp' must be a positive integer.")
  }
  
  # Truncate ncomp if it exceeds matrix dimensions
  if (ncomp > nrow(A)) {
    warning(sprintf("'ncomp' (%d) exceeds matrix dimensions (%d), truncating.", ncomp, nrow(A)))
    ncomp <- nrow(A)
  }
  
  # Dispatch to chosen method
  res_raw <- switch(
    method,
    robust = {
      if (!isSymmetric(B)) {
        stop("For method='robust', B must be symmetric.")
      }
      # Check for Positive Definiteness
      B_chol_try <- try(chol(B), silent = TRUE)
      if (inherits(B_chol_try, "try-error")) {
        stop("For method='robust', B must be positive definite (Cholesky failed).")
      }
      B_chol <- B_chol_try 
      
      # B_sqrt_inv <- solve(B_chol) # Avoid forming full inverse if possible
      # W <- t(B_sqrt_inv) %*% A %*% B_sqrt_inv # More stable calculation for W
      # Use solve(chol(B), X) for B^{-1/2} X and solve(t(chol(B)), X) for B^{-T/2} X
      tmp <- solve(B_chol, A) # tmp = R^-1 A where B=R^T R
      W <- solve(t(B_chol), t(tmp)) # W = R^-T (R^-1 A)^T = R^-T A^T R^-1
      # W should be symmetric if A is symmetric. Let's ensure it is treated as such.
      W <- (W + t(W)) / 2
      
      decomp <- eigen(W, symmetric = TRUE) 
      
      # Back-transform eigenvectors: vectors = B^{-1/2} * eigenvecs(W)
      # where B = R^T R, B^{-1/2} = solve(R)
      vectors_raw <- solve(B_chol, decomp$vectors[, 1:ncomp, drop = FALSE])
      values <- decomp$values[1:ncomp]
      
      # Explicitly B-orthonormalize
      norm_factor <- sqrt(diag(t(vectors_raw) %*% B %*% vectors_raw))
      # Avoid division by zero/NaN if norm_factor is very small
      norm_factor[norm_factor < .Machine$double.eps] <- 1 
      vectors <- sweep(vectors_raw, 2, norm_factor, "/")
      
      list(vectors = vectors, values = values)
    },
    sdiag = {
      if (!isSymmetric(B)) {
        stop("For method='sdiag', B must be symmetric.")
      }
      if (!isSymmetric(A)) {
        warning("For method='sdiag', A is not symmetric. Results may be inaccurate or complex.")
      }
      # Check for Positive Definiteness needed for B_sqrt_inv
      min_eigenvalue <- sqrt(.Machine$double.eps) # Use machine epsilon based threshold
      B_eig <- eigen(B, symmetric = TRUE)
      valsB <- B_eig$values
      
      if(any(valsB < min_eigenvalue)){
        warning(sprintf("B has %d eigenvalues close to zero or negative (min eigenvalue=%.2e). Clamping for inversion.", 
                        sum(valsB < min_eigenvalue), min(valsB)))
        valsB[valsB < min_eigenvalue] <- min_eigenvalue
      } 
      
      B_sqrt_inv_diag <- diag(1 / sqrt(valsB), nrow=length(valsB), ncol=length(valsB))
      B_sqrt_inv <- B_eig$vectors %*% B_sqrt_inv_diag %*% t(B_eig$vectors)
      
      A_transformed <- B_sqrt_inv %*% A %*% B_sqrt_inv
      # Ensure symmetry for eigen
      A_transformed <- (A_transformed + t(A_transformed)) / 2
      A_eig <- eigen(A_transformed, symmetric = TRUE)
      
      vectors_raw <- B_sqrt_inv %*% A_eig$vectors[, 1:ncomp, drop=FALSE]
      values  <- A_eig$values[1:ncomp]
      
      # Explicitly B-orthonormalize
      norm_factor <- sqrt(diag(t(vectors_raw) %*% B %*% vectors_raw))
      norm_factor[norm_factor < .Machine$double.eps] <- 1 
      vectors <- sweep(vectors_raw, 2, norm_factor, "/")
      
      list(vectors = vectors, values = values)
    },
    geigen = {
      if (!requireNamespace("geigen", quietly = TRUE)) {
        stop("Package 'geigen' not installed. Please install it for method='geigen'.")
      }
      res <- geigen::geigen(A, B, symmetric = FALSE) # Assume potentially non-symmetric
      
      # Sort by decreasing absolute value of eigenvalues
      ord <- order(abs(res$values), decreasing = TRUE)
      values_sorted  <- res$values[ord]
      vectors_sorted <- res$vectors[, ord, drop=FALSE]
      
      vectors <- vectors_sorted[, 1:ncomp, drop=FALSE]
      values  <- values_sorted[1:ncomp]
      list(vectors = vectors, values = values)
    },
    primme = {
      if (!requireNamespace("PRIMME", quietly = TRUE)) {
        stop("Package 'PRIMME' not installed. Please install it for method='primme'.")
      }
      if (!isSymmetric(A) || !isSymmetric(B)) {
        stop("For method='primme' using eigs_sym, both A and B must be symmetric.")
      }
      # Use the provided 'which' argument, pass others via ...
      res <- PRIMME::eigs_sym(A = A, B = B, NEig = ncomp, which = which, ...)
      vectors <- res$vectors
      values  <- res$values
      list(vectors = vectors, values = values)
    }
  )
  
  ev <- res_raw$values
  vec <- res_raw$vectors
  
  # Check for complex eigenvalues and handle
  if (is.complex(ev)) {
    ev_im <- Im(ev)
    if (any(abs(ev_im) > sqrt(.Machine$double.eps))) {
       warning("Complex eigenvalues found. Taking the real part.")
    }
    ev <- Re(ev)
    # If eigenvalues were complex, eigenvectors might be too. Take real part.
    if(is.complex(vec)){
      warning("Complex eigenvectors found. Taking the real part.")
      vec <- Re(vec)
    }
  }

  # Check for negative eigenvalues (after taking real part)
  if (any(ev < 0)) {
    warning("Some real eigenvalues are negative. 'sdev' computed using absolute values.")
  }
  
  sdev <- sqrt(abs(ev)) # Use abs(Re(ev))
  
  # Return a simple list with a class attribute
  out <- list(
    values  = ev,
    vectors = vec,
    sdev    = sdev,
    ncomp   = ncomp,
    method  = method
  )
  
  class(out) <- c("geneig", "list")
  out
}



#' Factor a matrix with regularization
#'
#' Attempts a Cholesky factorization with a diagonal `reg` until it succeeds.
#'
#' @param M A symmetric matrix to factor.
#' @param reg Initial regularization term.
#' @param max_tries Number of times to multiply reg by 10 if factorization fails.
#' @return A list with `ch` (the Cholesky factor) and `reg` (the final reg used).
#' @keywords internal
#' @importFrom Matrix Diagonal Cholesky
#' @noRd
factor_mat <- function(M, reg = 1e-3, max_tries = 5) {
  d <- nrow(M)
  M_reg <- M # Start with original M
  current_reg <- 0 # Keep track of added regularization
  
  for (i in seq_len(max_tries + 1)) { # Try initial M first, then add reg
    if (i > 1) { 
      # Add regularization for attempts 2 onwards
      if (i == 2) {
        current_reg <- reg
      } else {
        current_reg <- current_reg * 10
      }
      # Add Diagonal efficiently
      diag(M_reg) <- diag(M) + current_reg
    } else {
      # First attempt with M as is (or previous M_reg if loop continues)
      M_reg <- M
    }
    
    # Attempt Cholesky
    ch <- try(Matrix::Cholesky(M_reg, LDL = FALSE, super = TRUE), silent = TRUE) # Use Matrix::Cholesky
    
    if (!inherits(ch, "try-error")) {
      # Return the successful factor and the *added* regularization amount
      return(list(ch = ch, reg_added = if(i==1) 0 else current_reg)) 
    }
    
    # If first attempt failed, prepare M_reg for the next iteration with base reg
    if (i == 1) {
       M_reg <- M # Ensure we add reg to original M next time
    }
  }
  stop(sprintf("Unable to factor matrix even after adding regularization up to %.2e.", current_reg))
}

#' Solve using a precomputed Cholesky factor
#'
#' @param ch A Cholesky factor object from `Matrix::Cholesky()`.
#' @param RHS A right-hand-side matrix/vector compatible with `Matrix::solve`.
#' @keywords internal
#' @importFrom Matrix solve
#' @noRd
solve_chol <- function(ch, RHS) {
  Matrix::solve(ch, RHS) # Use Matrix::solve method
}

#' Orthonormalize columns via QR
#'
#' @param X A numeric matrix whose columns we want to orthonormalize.
#' @return A matrix of the same dimension with orthonormal columns. Handles potential rank deficiency by returning fewer columns.
#' @keywords internal
#' @importFrom methods as is
#' @noRd
orthonormalize <- function(X) {
  if (ncol(X) == 0) return(X) # Handle empty matrix
  
  # Use Matrix::qr for potential sparse input
  # Convert to dense first if it's not already suitable for base qr
  if (!methods::is(X, "matrix")) {
      X_dense <- try(methods::as(X, "matrix"), silent = TRUE)
      if (inherits(X_dense, "try-error")){
          stop("orthonormalize: Input matrix cannot be coerced to dense matrix for QR.")
      } 
      X <- X_dense
  }
  
  QR <- qr(X)
  rank <- QR$rank
  if (rank == 0) {
      warning("orthonormalize: Input matrix has rank 0.")
      return(matrix(0.0, nrow = nrow(X), ncol = 0)) # Return empty matrix
  }
  
  Q <- qr.Q(QR)
  
  # Return only the first 'rank' columns corresponding to the independent basis
  if (rank < ncol(X)) {
    warning(sprintf("orthonormalize: Input matrix rank (%d) is less than number of columns (%d). Returning orthonormal basis for the column space.", rank, ncol(X)))
  } 
  Q[, 1:rank, drop = FALSE]
}

#' Subspace Iteration for Generalized Eigenproblem
#'
#' Iteratively solves S1 x = λ S2 x using a subspace approach.
#' Assumes S1, S2 are symmetric. S2 (or S1 if which="smallest") must be PD.
#'
#' @param S1 A square symmetric matrix (e.g., n x n).
#' @param S2 A square symmetric positive definite matrix of the same dimension.
#' @param q Number of eigenpairs to approximate.
#' @param which "largest" or "smallest" eigenvalues to seek.
#' @param max_iter Maximum iteration count.
#' @param tol Convergence tolerance on relative change in eigenvalues.
#' @param V0 Optional initial guess matrix (n x q). If NULL, uses random.
#' @param reg_S Regularization added to S1 or S2 during factorization attempts. Default 1e-6.
#' @param reg_T Regularization for the small T matrix. Default 1e-9.
#' @param seed Optional seed for random V0 initialization. If NULL, uses current RNG state.
#' @return A list with `values` = the approximate eigenvalues, `vectors` = the approximate eigenvectors (n x q).
#' @keywords internal
#' @importFrom Matrix Diagonal solve t
#' @noRd
solve_gep_subspace <- function(S1, S2, q = 2,
                               which = c("largest", "smallest"),
                               max_iter = 100, tol = 1e-6,
                               V0 = NULL, reg_S = 1e-6, reg_T = 1e-9,
                               seed = NULL) { 
  
  which <- match.arg(which)
  d <- nrow(S1)
  
  # Factor either S2 or S1 once, depending on which eigenvalues we want
  if (which == "largest") {
    # Factor S2 => solve S2 V_hat = S1 V
    s_fact <- factor_mat(S2, reg = reg_S)
    ch <- s_fact$ch
    
    solve_step <- function(V) {
      RHS <- S1 %*% V
      solve_chol(ch, RHS)  # V_hat = S2^-1 (S1 V)
    }
  } else { # smallest
    # Factor S1 => solve S1 V_hat = S2 V
    s_fact <- factor_mat(S1, reg = reg_S)
    ch <- s_fact$ch
    
    solve_step <- function(V) {
      RHS <- S2 %*% V
      solve_chol(ch, RHS)  # V_hat = S1^-1 (S2 V)
    }
  }
  
  # Initialize subspace V
  if (is.null(V0)) {
    if (!is.null(seed)) set.seed(seed) # Set seed only if provided
    V <- matrix(rnorm(d * q), d, q)
  } else {
    if (ncol(V0) != q || nrow(V0) != d) {
       stop(sprintf("V0 dimensions (%d x %d) do not match expected (%d x %d).", 
                 nrow(V0), ncol(V0), d, q))
    }
    V <- V0
  }
  V <- orthonormalize(V)
  # Handle case where initial V is rank deficient
  if (ncol(V) < q) {
      warning(sprintf("Initial subspace V has rank %d, less than requested q=%d. Proceeding with reduced rank.", ncol(V), q))
      q <- ncol(V)
      if (q == 0) stop("Initial subspace V has rank 0.")
  }
  
  lambda_old <- rep(NA, q)
  
  for (iter in seq_len(max_iter)) {
    # 1) Expand subspace: V_hat = Op(V) where Op = S2^-1 S1 or S1^-1 S2
    V_hat <- solve_step(V)
    
    # 2) Orthonormalize V_hat using QR decomposition
    V_new <- orthonormalize(V_hat)
    
    # Check rank after orthonormalization
    q_new <- ncol(V_new)
    if (q_new < q) {
        warning(sprintf("Subspace rank reduced to %d during iteration %d. Stopping early.", q_new, iter))
        # Might need deflation or other strategy here, for now stop/return current
        q <- q_new
        if (q == 0) stop("Subspace iteration collapsed to rank 0.")
        # Trim lambda_old if needed
        lambda_old <- lambda_old[1:q]
        V <- V_new[, 1:q, drop=FALSE] # Update V to the reduced rank version
        break # Stop iteration as rank changed
    }
    
    # 3) Form Rayleigh quotient matrices using V_new (the orthonormal basis)
    # T_mat = V_new^T S2 V_new 
    # S_mat = V_new^T S1 V_new
    T_mat <- Matrix::t(V_new) %*% S2 %*% V_new
    S_mat <- Matrix::t(V_new) %*% S1 %*% V_new
    
    # Ensure symmetry (numerical precision might cause slight asymmetry)
    T_mat <- (T_mat + Matrix::t(T_mat)) / 2
    S_mat <- (S_mat + Matrix::t(S_mat)) / 2
    
    # 4) Solve the small (q x q) generalized eigenproblem: S_mat w = lambda T_mat w
    # Regularize T_mat before inversion if needed, although it should be PD if S2 is
    T_mat_reg <- T_mat + Matrix::Diagonal(q, reg_T)
    
    eig_res <- tryCatch({
        # Use base eigen for the small dense problem
        # Solve S_mat w = lambda T_mat w <=> T_mat^-1 S_mat w = lambda w
        # Ensure matrices are dense for base::eigen
        eigen(solve(as.matrix(T_mat_reg), as.matrix(S_mat)), symmetric = TRUE)
      },
      error = function(e) {
        warning(sprintf("Small eigenproblem failed at iter %d: %s. Trying geigen.", iter, e$message))
        # Fallback to geigen for the small problem if solve/eigen fails
        try(geigen::geigen(as.matrix(S_mat), as.matrix(T_mat_reg), symmetric=TRUE), silent=TRUE)
      }
    )
    
    if (inherits(eig_res, "try-error")) {
        stop(sprintf("Unable to solve small (%d x %d) eigenproblem at iteration %d even with fallback.", q, q, iter))
    }
    
    # Order eigenvalues (largest abs for convergence check stability, but use actual values)
    ord <- order(abs(eig_res$values), decreasing = TRUE)
    lambda <- eig_res$values[ord]
    W <- eig_res$vectors[, ord, drop = FALSE]
        
    # Adjust target eigenvalues based on 'which' argument
    if (which == "smallest") {
        # If we factored S1 (Op = S1^-1 S2), then eigenvalues of Op are 1/lambda_orig
        # We solved S_mat w = lambda_op T_mat w
        # where lambda_op corresponds to eigenvalues of Op = S1^-1 S2
        # We want eigenvalues of S1 x = lambda_orig S2 x
        # Need to sort 1/lambda_op to find the largest, which correspond to smallest lambda_orig
        ord_smallest_orig <- order(1/lambda, decreasing = TRUE) # Order by decreasing 1/lambda
        lambda <- lambda[ord_smallest_orig] # Get the lambda_op corresponding to smallest lambda_orig
        W <- W[, ord_smallest_orig, drop=FALSE]
    } else {
         # If we factored S2 (Op = S2^-1 S1), eigenvalues lambda_op are the ones we want (lambda_orig)
         # We already sorted by magnitude, which is standard for 'largest'
         ord_largest_orig <- order(lambda, decreasing = TRUE) # Order by value
         lambda <- lambda[ord_largest_orig]
         W <- W[, ord_largest_orig, drop=FALSE]
    }
    
    # 5) Update V using the eigenvectors W of the small problem
    # V_k+1 = V_new %*% W
    V <- V_new %*% W 
    # V should already be orthonormal if W is orthonormal and V_new is orthonormal
    # Re-orthonormalize just in case of numerical drift?
    V <- orthonormalize(V) 
    q_check <- ncol(V)
    if (q_check < q) {
        warning(sprintf("Subspace rank reduced to %d after update at iteration %d. Stopping early.", q_check, iter))
        q <- q_check
        if (q == 0) stop("Subspace iteration collapsed to rank 0 after update.")
        lambda_old <- lambda_old[1:q]
        lambda <- lambda[1:q]
        break
    }

    # 6) Check convergence
    if (iter > 1) { # Compare with previous iteration's eigenvalues
      # Use lambda_old from the *previous* iteration (corresponding to the same subspace V)
      valid_idx <- !is.na(lambda_old) & !is.na(lambda) & abs(lambda_old) > 1e-12
      if (all(!valid_idx)) {
         rel_change <- Inf # Cannot compare if no valid old values
      } else {
         rel_change <- max(abs(lambda[valid_idx] - lambda_old[valid_idx]) / pmax(abs(lambda_old[valid_idx]), 1e-12))
      }
      
      if (rel_change < tol) {
        # message(sprintf("Subspace iteration converged at iteration %d with rel_change=%.2e", iter, rel_change))
        break
      }
    }
    
    lambda_old <- lambda # Store current eigenvalues for next iteration's comparison
    
    if (iter == max_iter) {
       warning(sprintf("Subspace iteration did not converge within %d iterations (tol=%.1e, last rel_change=%.2e).", max_iter, tol, rel_change))
    }
  }
  
  # Return final approximate eigenpairs
  list(values = lambda_old, vectors = V[, 1:q, drop=FALSE])
}
</file>

<file path="R/bi_projector.R">
#' Construct a bi_projector instance
#'
#' A bi_projector offers a two-way mapping from samples (rows) to scores and from variables (columns) to components.
#' Thus, one can project from D-dimensional input space to d-dimensional subspace. And one can project (project_vars) from n-dimensional
#' variable space to the d-dimensional component space. The singular value decomposition is a canonical example of such a two-way mapping.
#'
#' @inheritParams projector
#' @param s The score matrix
#' @param sdev The standard deviations of the score matrix
#' @param preproc (optional) A pre-processing pipeline, default is prep(pass())
#' @param classes (optional) A character vector specifying the class attributes of the object, default is NULL
#' @return A bi_projector object
#' @examples
#' X <- matrix(rnorm(200), 10, 20)
#' svdfit <- svd(X)
#'
#' p <- bi_projector(svdfit$v, s = svdfit$u %*% diag(svdfit$d), sdev=svdfit$d)
#' @export
bi_projector <- function(v, s, sdev, preproc=prep(pass()), classes=NULL, ...) {
  chk::vld_matrix(v)
  chk::vld_matrix(s)
  chk::vld_numeric(sdev)
  chk::chk_equal(length(sdev), ncol(s))
  chk::chk_equal(ncol(v), length(sdev))
  
  out <- projector(v, preproc=preproc, s=s, sdev=sdev, classes=c(classes, "bi_projector"), ...)
}



#' @export
scores.bi_projector <- function(x,...) {
  x$s
}

#' @export
sdev.bi_projector <- function(x) {
  x$sdev
}


#' @export
project_vars.bi_projector <- function(x, new_data, ...) {
  if (is.vector(new_data)) {
    new_data <- matrix(new_data, ncol = 1)
  }
  sc <- scores(x)  # (n x d)
  chk::chk_equal(nrow(new_data), nrow(sc))
  
  variance <- sdev(x)^2
  # Optional: check for zero or near-zero sdev
  if (any(abs(variance) < 1e-12)) {
    warning("Some sdev values are near zero; results may be unstable.")
  }
  
  n <- nrow(sc)
  # (p_new x d) = (p_new x n) * (n x d) * (d x d) / (n - 1)
  (crossprod(new_data, sc) / (n - 1)) %*% diag(1 / variance, nrow = length(variance))
}

#' @keywords internal
#' @noRd
genreconstruct <- function(x, comp, rowind, colind) {
  ip <- inverse_projection(x)
  out <- scores(x)[rowind,comp,drop=FALSE] %*% ip[comp,,drop=FALSE][,colind,drop=FALSE]
  reverse_transform(x$preproc, out)
}

#' @export
reconstruct.bi_projector <- function(x, comp=1:ncomp(x), rowind=1:nrow(scores(x)), 
                                     colind=1:nrow(coef.projector(x)), ...) {
  chk::chk_numeric(comp)
  chk::chk_true(max(comp) <= ncomp(x))
  chk::chk_numeric(rowind)
  chk::chk_numeric(colind)
  chk::chk_range(comp, c(1,ncomp(x)))
  chk::chk_range(rowind, c(1,nrow(scores(x))))
  chk::chk_range(colind, c(1,nrow(coef.projector(x))))
  genreconstruct(x,comp, rowind, colind)
}

#' @export
residuals.bi_projector <- function(x, ncomp=ncomp(x), xorig,...) {
  recon <- reconstruct(x, comp=1:ncomp)
  xorig - recon
}


#' @export
reconstruct_new.bi_projector <- function(x, 
                                         new_data,
                                         comp = 1:ncomp(x),
                                         colind = 1:nrow(coef.projector(x)),
                                         ...)
{
  chk::chk_subset(comp, 1:ncomp(x))
  chk::chk_subset(colind, 1:nrow(coef.projector(x)))
  # 1) Basic checks
  chk::chk_numeric(comp)
  chk::chk_true(max(comp) <= ncomp(x))
  chk::chk_numeric(colind)
  chk::chk_range(comp, c(1,ncomp(x)))
  chk::chk_range(colind, c(1,nrow(coef.projector(x))))
  
  # 2) Possibly reprocess the new_data (e.g., center/scale)
  nd_proc <- apply_transform(x$preproc, new_data, colind = if (!missing(colind)) colind else NULL)
  
  # 3) Project new_data into row-scores for chosen comps
  #    's' in a bi_projector is basically row-scores for the original data. 
  #    For new data, we must do something akin to: row-scores = new_data * (some expression).
  #    Because a 'bi_projector' also has project_vars() for columns, etc.
  #    But we might define a new function like 'project_rows.bi_projector()'
  #    if we want to do a brand-new approach. 
  #    Alternatively, we re-implement partial LS logic here.
  
  #   Let's do a simple method: 
  #   row-scores for new_data = new_data %*% project_vars(...)^T? 
  #   Or we define a separate function. 
  #   For demonstration, let's do the direct approach (like SVD logic):
  
  # A typical formula if 'bi_projector' is from SVD: 
  #   row-scores(new_data) = new_data %*% x$v  *some inverse of sdev? 
  #   But let's re-use your 'project_vars.bi_projector()' approach if it suits.
  
  # e.g.:
  # factor_scores = (t(new_data) %*% sc) %*% diag(1/variance)
  # But that code in 'project_vars.bi_projector()' expects new_data's dimension to match nrow(sc)...
  
  # So there's some mismatch in your original code. 
  # We'll do a simpler method:
  
  # We'll mimic an SVD approach:
  # row_scores = nd_proc %*% x$v * (some diag(1/sdev^2)) ??? 
  # 
  # Actually, let's do a local LS approach if x$v is not orthonormal:
  # row_scores = nd_proc %*% x$v %*% solve( crossprod(x$v) )
  # Then pick columns 'comp'.
  
  v_mat <- coef.projector(x)  # shape (p x d)
  # just do an LS approach:
  inv_vt_v <- robust_inv_vTv(v_mat)  # we can define lambda=1e-6 or pass ...
  row_scores_full <- nd_proc %*% v_mat %*% inv_vt_v  # shape (m x d)
  
  # Now select comp
  row_scores_sub <- row_scores_full[, comp, drop=FALSE]  # (m x length(comp))
  
  # 4) Multiply row_scores_sub by the partial inverse of v (for columns in colind).
  #    Normally, original reconstruction is: row_scores_sub %*% t(v_mat[, comp]) 
  #    But we want only colind columns from that.
  
  # We can do:
  #   rec_sub = row_scores_sub %*% t(v_mat[colind, comp])  # but we must pick comp as well
  # or we do a standard approach:
  #   rec = row_scores_sub (m x length(comp)) %*% ip[comp, ][, colind]
  #   which is basically the same as your 'genreconstruct' logic. 
  #   Let's do a direct multiply:
  
  v_mat_sub <- v_mat[colind, comp, drop=FALSE]  # shape (|colind| x length(comp))
  rec_data_sub <- row_scores_sub %*% t(v_mat_sub)   # (m x |colind|)
  
  # 5) Reverse transform if needed
  out <- reverse_transform(x$preproc, rec_data_sub, colind = if (!missing(colind)) colind else NULL)
  
  out # Return the data in original space
}



#' @export
print.bi_projector <- function(x, ...) {
  cat("A bi_projector object with the following properties:\n\n")
  
  cat("Dimensions of the weights (v) matrix:\n")
  cat("  Rows: ", nrow(x$v), " Columns: ", ncol(x$v), "\n")
  
  cat("\nDimensions of the scores (s) matrix:\n")
  cat("  Rows: ", nrow(x$s), " Columns: ", ncol(x$s), "\n")
  
  cat("\nLength of the standard deviations (sdev) vector:\n")
  cat("  Length: ", length(x$sdev), "\n")
  
  cat("\nPreprocessing information:\n")
  print(x$preproc, ...)
  
  invisible(x)
}

#' @export
truncate.bi_projector <- function(x, ncomp) {
  old_ncomp <- ncomp(x)
  chk::chk_number(ncomp)
  if (ncomp < 1 || ncomp > old_ncomp) {
    stop("Requested ncomp must be between 1 and ", old_ncomp)
  }
  
  # Truncate projector components
  v_new <- components(x)[, seq_len(ncomp), drop = FALSE]
  x$v   <- v_new
  
  # Truncate scores and sdev specific to bi_projector
  if (!is.null(x$s)) {
      x$s <- x$s[, seq_len(ncomp), drop = FALSE]
  }
  if (!is.null(x$sdev)) {
      x$sdev <- x$sdev[seq_len(ncomp)]
  }
  
  # Clear any cached inverse projection etc.
  cache_env   <- attr(x, ".cache")
  if (!is.null(cache_env) && is.environment(cache_env)) {
    rm(list = ls(cache_env), envir = cache_env)
  }
  x
}
</file>

<file path="R/pca.R">
#' Principal Components Analysis (PCA)
#'
#' Compute the directions of maximal variance in a data matrix using the Singular Value Decomposition (SVD).
#'
#' @param X The data matrix.
#' @param ncomp The number of requested components to estimate (default is the minimum dimension of the data matrix).
#' @param preproc The pre-processing function to apply to the data matrix (default is centering).
#' @param method The SVD method to use, passed to \code{svd_wrapper} (default is "fast").
#' @param ... Extra arguments to send to \code{svd_wrapper}.
#' @return A \code{bi_projector} object containing the PCA results.
#' @export
#' @seealso \code{\link{svd_wrapper}} for details on SVD methods.
#' @examples
#' data(iris)
#' X <- as.matrix(iris[, 1:4])
#' res <- pca(X, ncomp = 4)
#' tres <- truncate(res, 3)
pca <- function(X, ncomp=min(dim(X)), preproc=center(), 
                method = c("fast", "base", "irlba", "propack", "rsvd", "svds"), ...) {
  chk::chkor_vld(chk::vld_matrix(X), chk::vld_s4_class(X, "Matrix"))
  
  method <- match.arg(method)
  svdres <- svd_wrapper(X, ncomp, preproc, method=method, ...)
  
  ## todo add rownames slot to `bi_projector`?
  if (!is.null(row.names(scores))) {
    row.names(scores) <- row.names(X)[seq_along(svdres$d)]
  }
  

  attr(svdres, "class") <- c("pca", attr(svdres, "class"))
  svdres
}


#' @keywords internal
#' @noRd
orth_distances.pca <- function(x, ncomp, xorig) {
  resid <- residuals(x, ncomp, xorig)
  scores <- scores(x)
  loadings <- coef(x)
  
  scoresn <- x$u
  
  Q <- matrix(0, nrow = nrow(scores), ncol = ncomp)
  
  for (i in seq_len(ncomp)) {
    res <- resid
    if (i < ncomp) {
      res <- res +
        tcrossprod(
          scores[, (i + 1):ncomp, drop = F],
          loadings[, (i + 1):ncomp, drop = F]
        )
    }
    
    Q[, i] <- rowSums(res^2)
    #T2[, i] <- rowSums(scoresn[, seq_len(i), drop = F]^2)
  }
  
  Q
}


#' @keywords internal
#' @noRd
score_distances.pca <- function(x, ncomp, xorig) {
  scores <- scores(x)
  loadings <- coef(x)
  
  scoresn <- x$u
  
  T2 <- matrix(0, nrow = nrow(scores), ncol = ncomp)
  for (i in seq_len(ncomp)) {
    T2[, i] <- rowSums(scoresn[, seq_len(i), drop = F]^2)
  }
  
  T2
  
}


#' @export
#' @importFrom chk chk_range
truncate.pca <- function(x, ncomp) {
  chk::chk_range(ncomp, c(1, ncomp(x)))
  x$v <- x$v[,1:ncomp, drop=FALSE]
  x$sdev <- x$sdev[1:ncomp]
  x$s <- x$s[,1:ncomp,drop=FALSE]
  x$u <- x$u[, 1:ncomp, drop=FALSE]
  x
}



#' @keywords internal
#' @noRd
perm_ci.pca <- function(x, X, nperm=100, k=4, distr="gamma", parallel=FALSE, ...) {
  .Deprecated("perm_test.pca", package="multivarious", 
              msg = "'perm_ci.pca' is deprecated. Please use 'perm_test.pca' with distribution=\"empirical\" instead.")
  
  # Minimal forwarding logic, likely won't match exactly but guides user
  perm_test.pca(x = x, X = X, nperm = nperm, comps = k, 
                distribution = "empirical", # Force empirical as distr is gone
                parallel = parallel, alternative = "greater", ...)
}


#' @importFrom stats quantile na.omit
#' @importFrom future.apply future_lapply
#' @export
#' @seealso \code{\link{perm_test}}, \code{\link{pca}}
perm_test.pca <- function(x,
                          X,
                          nperm = 1000,
                          measure_fun = NULL,
                          fit_fun = NULL, # Ignored
                          shuffle_fun = NULL,               
                          stepwise = TRUE,
                          parallel = FALSE,
                          alternative = c("greater", "less", "two.sided"),
                          alpha = 0.05, # Significance level for stopping rule
                          comps = 4,
                          use_svd_solver = c("fast", "RSpectra"), 
                          ...) 
{
  # Match args
  alternative <- match.arg(alternative)
  use_svd_solver <- match.arg(use_svd_solver)
  chk::chk_number(alpha)
  chk::chk_range(alpha, c(0, 1))
  
  # Capture extra args (though likely unused by defaults here)
  extra_args <- list(...)
  
  # --- Setup ---
  Q <- ncomp(x)
  orig_comps <- comps
  comps <- min(Q - 1, comps) # Can only test up to Q-1 components sequentially
  if (comps < 1) stop("Cannot perform permutation test, need at least 2 components in the model.")
  if (comps < orig_comps) {
      message(sprintf("Requested comps=%d truncated to %d (can test at most Q-1=%d components sequentially).", 
                      orig_comps, comps, Q - 1))
  }
  
  # --- Observed Statistics (Default: F_a) ---
  evals <- x$sdev^2
  if (is.null(measure_fun)) {
      observed_stats <- sapply(1:comps, function(a) {
          denom <- sum(evals[a:Q])
          if (denom <= .Machine$double.eps) return(0)
          evals[a] / denom
      })
      stat_name <- "F_a (Fraction of Remaining Variance)"
      internal_stat_fun <- function(modp, a, ...) { # Add ... to signature for consistency
          if (is.null(modp) || length(modp$sdev) == 0) return(NA_real_)
          eperm <- modp$sdev^2
          if (length(eperm) == 0) return(NA_real_)
          n_relevant_evals <- Q - (a - 1)
          if (length(eperm) < 1) return(NA_real_)
          if (length(eperm) < n_relevant_evals) {
              warning(sprintf("Permutation for component %d yielded only %d eigenvalues (expected at least %d). Using sum of available.", a, length(eperm), n_relevant_evals))
              n_relevant_evals <- length(eperm)
          }
          denom <- sum(eperm[1:n_relevant_evals])
          if (denom <= .Machine$double.eps) return(0)
          eperm[1] / denom
      }
  } else {
      stat_name <- deparse(substitute(measure_fun))
      # Calculate observed stats using user function (pass extra args)
      observed_stats <- sapply(1:comps, function(a) do.call(measure_fun, c(list(model_perm = x, comp_idx = a), extra_args)))
      # Internal function passes ... down
      internal_stat_fun <- function(modp, a, ...) {
          if (is.null(modp)) return(NA_real_)
          do.call(measure_fun, c(list(model_perm = modp, comp_idx = a), list(...)))
      }
  }
  
  # --- Default Shuffle Function (Column-wise) ---
  if (is.null(shuffle_fun)) {
      shuffle_fun <- function(dat, ...) apply(dat, 2, sample)
      shuffle_type <- "column-wise"
  } else {
      shuffle_type <- "custom"
  }
  
  # --- Preprocess Data ---
  Xp <- apply_transform(x$preproc, X)
  
  # --- Efficient PCA/SVD for Permutations & P3 Projection ---
  get_leading_svd_u <- function(M, k) {
      if (k <= 0) return(matrix(0.0, nrow = nrow(M), ncol = 0))
      if (nrow(M) < 1 || ncol(M) < 1 || k > min(nrow(M), ncol(M))) {
          warning(sprintf("Cannot compute %d singular vectors for %d x %d matrix.", k, nrow(M), ncol(M)))
          return(NULL)
      }
      solver <- use_svd_solver
      if (solver == "RSpectra" && !requireNamespace("RSpectra", quietly = TRUE)) {
          warning("RSpectra package not found, falling back to use_svd_solver='fast'.")
          solver <- "fast"
      }
      U_perm <- tryCatch({
          if (solver == "RSpectra") RSpectra::svds(M, k = k, nu = k, nv = 0)$u
          else svd(M, nu = k, nv = 0)$u
       }, error = function(e) {
           warning(sprintf("SVD(k=%d) calculation failed: %s. Returning NULL.", k, e$message))
           NULL
       })
      U_perm
  }
  run_pca_perm <- function(M_perm) {
      ncomp_needed <- Q
      pca_res <- try(svd_wrapper(M_perm, ncomp=ncomp_needed, preproc=pass(), method=use_svd_solver), silent=TRUE)
      if (inherits(pca_res, "try-error")) {
          warning(sprintf("PCA failed during permutation: %s", pca_res))
          return(NULL)
      }
      if (is.null(pca_res$sdev) || length(pca_res$sdev) == 0) {
          warning("PCA during permutation resulted in no valid singular values.")
          return(NULL)
      }
      return(pca_res)
  }
  
  # --- Pre-calculate Reconstructions if Stepwise ---
  recon_list <- list()
  if (stepwise && comps > 0) {
      message("Pre-calculating reconstructions for stepwise testing...")
      for (a in 1:comps) {
          cnums <- 1:a
          recon_list[[a]] <- reconstruct(x, comp = cnums)
      }
  }
  I_mat <- diag(nrow(Xp))
  
  # ---------- Permutation Loop Function ----------
  one_perm <- function(perm_idx, current_a, ...) {
      Ea <- if (!stepwise || current_a == 1) Xp else Xp - recon_list[[current_a - 1]]
      
      # Create args for shuffle_fun
      shuffle_args <- c(list(dat = Ea), list(...))
      Ea_perm <- do.call(shuffle_fun, shuffle_args)
      
      Ea_perm_proj <- if (stepwise && current_a > 1) {
          Ua_perm <- get_leading_svd_u(Ea_perm, current_a - 1)
          if (is.null(Ua_perm)) return(NA_real_)
          P_orth <- I_mat - tcrossprod(Ua_perm)
          proj_res <- P_orth %*% Ea_perm
          if (any(!is.finite(proj_res))) {
              warning(sprintf("Permutation %d, Comp %d: Non-finite values after P3 projection. Replacing with 0.", perm_idx, current_a))
              proj_res[!is.finite(proj_res)] <- 0
          }
          proj_res
      } else {
          Ea_perm
      }
      
      modp <- run_pca_perm(Ea_perm_proj)
      if (is.null(modp)) return(NA_real_)
      
      # Pass ... down to internal_stat_fun
      do.call(internal_stat_fun, c(list(modp = modp, a = current_a), list(...)))
  }

  # ---------- Run Permutations ----------
  Fq <- matrix(NA, nrow = nperm, ncol = comps)
  n_complete <- rep(0, comps)
  pvals <- rep(NA, comps)
  comps_tested <- 0
  
  apply_fun <- if (parallel) future.apply::future_lapply else lapply
  
  message(sprintf("Running %d permutations sequentially for up to %d PCA components (alpha=%.3f, %s)...", 
                  nperm, comps, alpha, if(parallel) "parallel" else "serial"))
  
  for (a in 1:comps) {
      message(sprintf("  Testing Component %d/%d...", a, comps))
      # Pass extra_args down via lapply's ...
      perm_args <- list(X = seq_len(nperm), FUN = one_perm, current_a = a)
      if (parallel) perm_args$future.seed <- TRUE
      perm_args <- c(perm_args, extra_args)
      
      perm_vals_list <- do.call(apply_fun, perm_args)
      perm_vals_a <- unlist(perm_vals_list)
      Fq[, a] <- perm_vals_a
      n_complete[a] <- sum(!is.na(perm_vals_a))
      
      if (n_complete[a] < nperm) {
          warning(sprintf("Component %d: %d/%d permutations failed (e.g., SVD error). Using %d successful permutations.",
                          a, nperm - n_complete[a], nperm, n_complete[a]))
      }
      if (n_complete[a] == 0) {
          warning(sprintf("Component %d: All permutations failed. Cannot compute p-value.", a))
          pvals[a] <- NA
          comps_tested <- a
          break # Stop if all perms failed
      }
      
      # Calculate empirical p-value
      obs_a <- observed_stats[a]
      perm_vals_a_clean <- stats::na.omit(perm_vals_a)
      if (is.na(obs_a)) {
          pval_a <- NA
          warning(sprintf("Component %d: Observed statistic is NA, cannot compute p-value.", a))
      } else {
          if (alternative == "greater") {
              b <- sum(perm_vals_a_clean >= obs_a)
              pval_a <- (b + 1) / (n_complete[a] + 1)
          } else if (alternative == "less") {
              b <- sum(perm_vals_a_clean <= obs_a)
              pval_a <- (b + 1) / (n_complete[a] + 1)
          } else { # two.sided
              b_greater <- sum(perm_vals_a_clean >= obs_a)
              b_less <- sum(perm_vals_a_clean <= obs_a)
              pval_two <- 2 * min((b_greater + 1) / (n_complete[a] + 1), (b_less + 1) / (n_complete[a] + 1))
              pval_a <- min(pval_two, 1.0)
          }
      }
      pvals[a] <- pval_a
      comps_tested <- a
      
      # Sequential stopping rule
      if (!is.na(pval_a) && pval_a > alpha) {
          message(sprintf("  Component %d p-value (%.4g) > alpha (%.3f). Stopping sequential testing.", a, pval_a, alpha))
          break
      }
  } # End loop over components
  
  # ---------- Calculate CIs and final results table ----------
  comp_df_list <- vector("list", comps_tested)
  for (i in 1:comps_tested) {
      lower_ci <- NA; upper_ci <- NA
      if (n_complete[i] > 1) {
          perm_vals_i_clean <- stats::na.omit(Fq[, i])
          cis <- stats::quantile(perm_vals_i_clean, probs = c(0.025, 0.975), na.rm = TRUE)
          lower_ci <- cis[1]; upper_ci <- cis[2]
      }
      comp_df_list[[i]] <- tibble::tibble(
          comp = i,
          observed = observed_stats[i],
          pval = pvals[i],
          lower_ci = lower_ci,
          upper_ci = upper_ci
      )
  }
  component_results <- dplyr::bind_rows(comp_df_list)
  
  # ---------- Output Structure ----------
  out <- list(
      call = match.call(),
      component_results = component_results,
      perm_values = Fq[, 1:comps_tested, drop = FALSE],
      alpha = alpha,
      alternative = alternative,
      method = sprintf("Permutation test for PCA (Vitale et al. 2017 P3) (statistic: %s, stepwise: %s, shuffle: %s)", 
                       stat_name, stepwise, shuffle_type),
      nperm = n_complete[1:comps_tested] # Report vector of successful permutations per component
  )
  class(out) <- c("perm_test_pca", "perm_test")
  out
}

# Re-introducing specific reconstruct.pca method as the generic bi_projector one is not suitable
#' Reconstruct Data from PCA Results
#'
#' Reconstructs the original (centered) data matrix from the PCA scores and loadings.
#'
#' @param x A `pca` object.
#' @param comp Integer vector specifying which components to use for reconstruction (default: all components in `x`).
#' @param ... Extra arguments (ignored).
#' @return A matrix representing the reconstructed data in the *original* scale (preprocessing reversed).
#' @export
reconstruct.pca <- function(x, comp = 1:ncomp(x), ...) {
  # Check component indices
  chk::chk_vector(comp)
  chk::chk_subset(comp, 1:ncomp(x))
  
  # Use standard PCA reconstruction: scores %*% t(loadings)
  reconstructed_proc <- scores(x)[, comp, drop=FALSE] %*% t(coef(x)[, comp, drop=FALSE])
  
  # Reverse the preprocessing to return data in original scale
  reverse_transform(x$preproc, reconstructed_proc)
}


# Ensure print.perm_test exists or define print.perm_test_pca
# Assuming print.perm_test from discriminant_projector handles the structure:
# list(statistic, perm_values, p.value, alternative, method, nperm, call)
# Our structure is different (component_results data frame).
# Let's define a specific print method.

#' Print Method for perm_test_pca Objects
#' 
#' Provides a concise summary of the PCA permutation test results.
#' 
#' @param x An object of class `perm_test_pca`.
#' @param ... Additional arguments passed to printing methods.
#' @return Invisibly returns the input object `x`.
#' @export
print.perm_test_pca <- function(x, ...) {
  cat("\nPCA Permutation Test Results\n\n")
  cat("Method: ", x$method, "\n")
  cat("Alternative: ", x$alternative, "\n")
  cat("\nComponent Results:\n")
  print(as.data.frame(x$component_results)) # Print the results table
  cat("\nNumber of successful permutations per component:", paste(x$nperm, collapse=", "), "\n")
  invisible(x)
}



#' Rotate PCA Loadings
#'
#' Apply a specified rotation to the component loadings of a PCA model. 
#' This function leverages the GPArotation package to apply orthogonal 
#' or oblique rotations.
#'
#' @param x A PCA model object, typically created using the `pca()` function.
#' @param ncomp The number of components to rotate. Must be <= ncomp(x).
#' @param type The type of rotation to apply. Supported rotation types:
#'
#' \describe{
#'   \item{"varimax"}{Orthogonal Varimax rotation}
#'   \item{"quartimax"}{Orthogonal Quartimax rotation}
#'   \item{"promax"}{Oblique Promax rotation}
#' }
#'
#' @param loadings_type For oblique rotations, which loadings to use:
#'
#' \describe{
#'   \item{"pattern"}{Use pattern loadings as \code{v}}
#'   \item{"structure"}{Use structure loadings (\code{pattern_loadings \%*\% Phi}) as \code{v}}
#' }
#'
#' Ignored for orthogonal rotations.
#'
#' @param score_method How to recompute scores after rotation:
#'
#' \describe{
#'   \item{"auto"}{For orthogonal rotations, use 
#'     \code{scores_new = scores_original \%*\% t(R)}} 
#'     For oblique rotations, recompute from the pseudoinverse.
#'
#'   \item{"recompute"}{Always recompute scores from \code{X_proc} and 
#'     the pseudoinverse of rotated loadings.}
#'
#'   \item{"original"}{For orth rotations, same as \code{auto}, 
#'     but may not work for oblique rotations.}
#' }
#'
#' @param ... Additional arguments passed to GPArotation functions.
#'
#' @return A modified PCA object with class \code{rotated_pca} and additional fields:
#' \describe{
#'   \item{v}{Rotated loadings}
#'   \item{s}{Rotated scores}
#'   \item{sdev}{Updated standard deviations of rotated components}
#'   \item{explained_variance}{Proportion of explained variance for each rotated component}
#'   \item{rotation}{A list with rotation details: \code{type}, \code{R} (orth) or \code{Phi} (oblique), and \code{loadings_type}}
#' }
#'
#' @importFrom GPArotation GPForth GPFoblq
#' @export
#'
#' @examples
#' # Perform PCA on the iris dataset
#' data(iris)
#' X <- as.matrix(iris[,1:4])
#' res <- pca(X, ncomp=4)
#'
#' # Apply varimax rotation to the first 3 components
#' rotated_res <- rotate(res, ncomp=3, type="varimax")
rotate.pca <- function(x, ncomp, type=c("varimax", "quartimax", "promax"),
                       loadings_type=c("pattern", "structure"),
                       score_method=c("auto", "recompute", "original"),
                       ...) {
  type <- match.arg(type)
  loadings_type <- match.arg(loadings_type)
  score_method <- match.arg(score_method)
  
  if (!requireNamespace("GPArotation", quietly = TRUE)) {
    stop("GPArotation package is required for rotations. Please install it.")
  }
  
  if (ncomp > ncomp(x)) {
    stop("ncomp cannot exceed the number of available components in 'x'.")
  }
  
  # Extract loadings and scores for the specified components
  loadings_to_rotate <- x$v[, 1:ncomp, drop=FALSE]
  scores_original <- x$s[, 1:ncomp, drop=FALSE]
  
  # We'll need X_proc to recompute scores in 'recompute' mode or for oblique rotations
  # If we don't have X_proc directly, we can reconstruct it:
  # X_proc ≈ scores_original %*% t(loadings_to_rotate)
  # This relies on the PCA model: X_proc = s * v', assuming preproc applied.
  
  # Just in case we need full pre-processed data:
  # For 'recompute' or oblique rotations, we must have a stable way to get X_proc.
  # We know: X_proc = scores_original %*% t(loadings_to_rotate)
  
  # Create loadings object for GPArotation
  L <- loadings_to_rotate
  class(L) <- "loadings"
  
  # Perform rotation
  if (type %in% c("varimax", "quartimax")) {
    # Orthogonal rotation
    rot_res <- GPArotation::GPForth(L, method=type, ...)
    rotated_loadings <- rot_res$loadings
    R <- rot_res$Th  # rotation matrix
    
    # Compute scores_new:
    # Depending on score_method:
    if (score_method == "auto" || score_method == "original") {
      # For orth rotations: scores_new = scores_original %*% t(R)
      scores_new <- scores_original %*% t(R)
    } else if (score_method == "recompute") {
      # recompute from X_proc:
      X_proc <- scores_original %*% t(loadings_to_rotate)
      inv_rotated <- corpcor::pseudoinverse(rotated_loadings)
      scores_new <- X_proc %*% inv_rotated
    }
    
    # Update sdev and explained variance
    variances <- apply(scores_new, 2, stats::var)
    sdev_new <- sqrt(variances)
    explained_variance <- variances / sum(variances)
    
    # Update object
    x$v[, 1:ncomp] <- as.matrix(rotated_loadings)
    x$s[, 1:ncomp] <- scores_new
    x$sdev[1:ncomp] <- sdev_new
    x$explained_variance <- explained_variance
    
    x$rotation <- list(type=type, loadings_type="N/A (orthogonal)", R=R, Phi=NULL)
    
  } else {
    # Oblique rotation
    rot_res <- GPArotation::GPFoblq(L, method=type, ...)
    pattern_loadings <- rot_res$loadings
    Phi <- rot_res$Phi
    
    # Choose loadings based on loadings_type
    if (loadings_type == "pattern") {
      chosen_loadings <- pattern_loadings
    } else {
      # structure loadings = pattern_loadings %*% Phi
      chosen_loadings <- pattern_loadings %*% Phi
    }
    
    # Compute scores_new:
    # For oblique rotations, if score_method == "original" doesn't make sense because original was orth-based.
    # We'll handle "original" by warning or just do what "auto" does for oblique (which is recompute).
    
    if (score_method == "original") {
      warning("For oblique rotations, 'original' score_method is not valid. Using 'auto'.")
      score_method <- "auto"
    }
    
    if (score_method == "auto") {
      # auto = oblique => recompute from pseudoinverse
      X_proc <- scores_original %*% t(loadings_to_rotate)
      inv_chosen <- corpcor::pseudoinverse(chosen_loadings)
      scores_new <- X_proc %*% inv_chosen
    } else if (score_method == "recompute") {
      # Same as above
      X_proc <- scores_original %*% t(loadings_to_rotate)
      inv_chosen <- corpcor::pseudoinverse(chosen_loadings)
      scores_new <- X_proc %*% inv_chosen
    }
    
    # Update sdev and explained variance
    variances <- apply(scores_new, 2, stats::var)
    sdev_new <- sqrt(variances)
    explained_variance <- variances / sum(variances)
    
    # Update object
    x$v[, 1:ncomp] <- chosen_loadings
    x$s[, 1:ncomp] <- scores_new
    x$sdev[1:ncomp] <- sdev_new
    x$explained_variance <- explained_variance
    
    x$rotation <- list(type=type, loadings_type=loadings_type, R=NULL, Phi=Phi)
  }
  
  # Add rotated_pca class
  if (!("rotated_pca" %in% class(x))) {
    class(x) <- c("rotated_pca", class(x))
  }
  
  x
}




#' Biplot for PCA Objects (Enhanced with ggrepel)
#'
#' Creates a 2D biplot for a \code{pca} object, using \pkg{ggplot2} and \pkg{ggrepel} 
#' to show both sample scores (observations) and variable loadings (arrows).
#'
#' @param x A \code{pca} object returned by \code{\link{pca}}.
#' @param y (ignored) Placeholder to match \code{biplot(x, y, ...)} signature.
#' @param dims A length-2 integer vector specifying which principal components to plot 
#'   on the x and y axes. Defaults to \code{c(1, 2)}.
#' @param scale_arrows A numeric factor to scale the variable loadings (arrows). Default is 2.
#' @param alpha_points Transparency level for the sample points. Default is 0.6.
#' @param point_size Size for the sample points. Default is 2.
#' @param point_labels Optional character vector of labels for the sample points. 
#'   If \code{NULL}, rownames of the scores matrix are used if available; otherwise numeric indices.
#' @param var_labels Optional character vector of variable names (columns in the original data). 
#'   If \code{NULL}, rownames of \code{x\$v} are used if available; otherwise "Var1", "Var2", etc.
#' @param arrow_color Color for the loading arrows. Default is "red".
#' @param text_color Color for the variable label text. Default is "red".
#' @param repel_points Logical; if TRUE, repel sample labels using \code{geom_text_repel}. Default is \code{TRUE}.
#' @param repel_vars Logical; if TRUE, repel variable labels using \code{geom_text_repel}. Default is \code{FALSE}.
#' @param ... Additional arguments passed on to \code{ggplot2} or \code{ggrepel} functions (if needed).
#'
#' @details
#' This function constructs a scatterplot of the PCA scores (observations) on two chosen components
#' and overlays arrows for the loadings (variables). The arrow length and direction indicate how each
#' variable contributes to those principal components. You can control arrow scaling with \code{scale_arrows}.
#'
#' If your \code{pca} object includes an \code{$explained_variance} field (e.g., proportion of variance per component),
#' those values will appear in the axis labels. Otherwise, the axes are labeled simply as "PC1", "PC2", etc.
#'
#' **Note**: If you do not have \pkg{ggrepel} installed, you can set \code{repel_points=FALSE} and 
#' \code{repel_vars=FALSE}, or install \pkg{ggrepel}.
#'
#' @return A \code{ggplot} object.
#'
#' @import ggplot2
#' @importFrom ggrepel geom_text_repel
#' @export
#'
#' @examples
#' \dontrun{
#' data(iris)
#' X <- as.matrix(iris[,1:4])
#' pca_res <- pca(X, ncomp=2)
#'
#' # Enhanced biplot with repelled text
#' biplot(pca_res, repel_points=TRUE, repel_vars=TRUE)
#' }
biplot.pca <- function(x,
                       y = NULL,  # ignored, matching biplot generic
                       dims = c(1, 2),
                       scale_arrows = 2,
                       alpha_points = 0.6,
                       point_size = 2,
                       point_labels = NULL,
                       var_labels = NULL,
                       arrow_color = "red",
                       text_color = "red",
                       repel_points = TRUE,
                       repel_vars = FALSE,
                       ...) 
{
  # Check dims
  n_comps <- ncomp(x)
  if (any(dims > n_comps)) {
    stop("Requested components exceed the number of available components in 'x'.")
  }
  if (length(dims) != 2) {
    stop("'dims' must be a vector of length 2, e.g. c(1, 2).")
  }
  
  # Extract scores and loadings
  sc <- x$s  # observations x components
  ld <- x$v  # variables x components
  if (is.null(sc) || is.null(ld)) {
    stop("The PCA object does not contain both scores (x$s) and loadings (x$v).")
  }
  
  # Subset to dims
  sc2 <- sc[, dims, drop = FALSE]
  ld2 <- ld[, dims, drop = FALSE]
  
  # If we have explained variance, use it
  if (!is.null(x$explained_variance)) {
    pc_var <- x$explained_variance
  } else {
    pc_var <- rep(NA_real_, ncol(sc))
  }
  
  # Convert scores to data.frame
  scores_df <- as.data.frame(sc2)
  colnames(scores_df) <- c("PCx", "PCy")
  
  # Assign labels to points
  if (is.null(point_labels)) {
    if (!is.null(rownames(sc))) {
      point_labels <- rownames(sc)
    } else {
      point_labels <- seq_len(nrow(sc))
    }
  }
  scores_df$labels <- point_labels
  
  # Convert loadings to data.frame
  loadings_df <- as.data.frame(ld2)
  colnames(loadings_df) <- c("PCx", "PCy")
  
  # Assign variable labels
  if (is.null(var_labels)) {
    if (!is.null(rownames(ld))) {
      var_labels <- rownames(ld)
    } else {
      var_labels <- paste0("Var", seq_len(nrow(ld)))
    }
  }
  loadings_df$var <- var_labels
  
  # Scale loadings for arrow length
  loadings_df$PCx <- loadings_df$PCx * scale_arrows
  loadings_df$PCy <- loadings_df$PCy * scale_arrows
  
  # Build axis labels (include % variance if available)
  pcx_lab <- if (!is.na(pc_var[dims[1]])) {
    paste0("PC", dims[1], " (", round(100 * pc_var[dims[1]], 1), "%)")
  } else {
    paste0("PC", dims[1])
  }
  pcy_lab <- if (!is.na(pc_var[dims[2]])) {
    paste0("PC", dims[2], " (", round(100 * pc_var[dims[2]], 1), "%)")
  } else {
    paste0("PC", dims[2])
  }
  
  # Start building the plot
  plt <- ggplot(scores_df, aes(x = .data$PCx, y = .data$PCy)) +
    geom_point(alpha = alpha_points, size = point_size, color = "blue") +
    theme_minimal(base_size = 12) +
    coord_equal() +
    xlab(pcx_lab) +
    ylab(pcy_lab)
  
  # Add text labels for points
  # Use ggrepel if repel_points=TRUE and ggrepel is installed
  # fallback to geom_text if not installed or repel_points=FALSE
  can_repel <- requireNamespace("ggrepel", quietly = TRUE)
  
  if (repel_points && can_repel) {
    plt <- plt + 
      ggrepel::geom_text_repel(aes(label = .data$labels), color = "black", size = 3, ...)
  } else {
    plt <- plt + 
      geom_text(aes(label = .data$labels), hjust = 1.1, vjust = 0.5, color = "black", size = 3, ...)
  }
  
  # Add loadings arrows
  plt <- plt +
    geom_segment(data = loadings_df,
                 aes(x = 0, y = 0, xend = .data$PCx, yend = .data$PCy),
                 arrow = arrow(length = unit(0.02, "npc")),
                 color = arrow_color,
                 size = 0.7)
  
  # Add variable names near arrow tips
  if (repel_vars && can_repel) {
    plt <- plt +
      ggrepel::geom_text_repel(data = loadings_df,
                               aes(x = .data$PCx, y = .data$PCy, label = .data$var),
                               color = text_color,
                               size = 3,
                               ...)
  } else {
    plt <- plt +
      geom_text(data = loadings_df,
                aes(x = .data$PCx, y = .data$PCy, label = .data$var),
                color = text_color, vjust = -0.5, size = 3,
                ...)
  }
  
  plt
}

#' Print Method for PCA Objects
#'
#' Provide a color-enhanced summary of the PCA object, including 
#' dimensions, variance explained, and a quick component breakdown.
#'
#' @param x A \code{pca} object.
#' @param ... Ignored (for compatibility).
#' @export
print.pca <- function(x, ...) {
  cat(
    crayon::bold(crayon::green("PCA object")),
    " -- derived from SVD\n\n"
  )
  
  # Basic dims
  nobs <- nrow(x$s)         # number of observations
  nvars <- nrow(x$v)        # number of variables
  ncomp_used <- ncol(x$s)   # how many comps we actually have
  
  cat(crayon::cyan("Data: "), nobs, " observations x ", nvars, " variables\n", sep = "")
  cat(crayon::cyan("Components retained: "), ncomp_used, "\n\n", sep = "")
  
  # Possibly show proportion of variance if available
  if (!is.null(x$sdev)) {
    eigenvals <- x$sdev^2
    prop_var <- eigenvals / sum(eigenvals)
    cum_var <- cumsum(prop_var)
    
    cat(crayon::bold("Variance explained (per component):\n"))
    cat(
      formatC(seq_len(ncomp_used), width = 2), " ",
      formatC(round(prop_var * 100, 2), width=6), "%  ",
      "(cumulative: ",
      formatC(round(cum_var * 100, 2), width=6), "%)\n",
      sep = ""
    )
    cat("\n")
  }
  
  # If we have an explained_variance field (like from rotate), use that
  if (!is.null(x$explained_variance)) {
    cat(crayon::bold("Explained variance from rotation:\n"))
    cat(paste(round(x$explained_variance * 100, 2), "%\n"), "\n\n")
  }
  
  # Possibly show info about rotation
  if (!is.null(x$rotation)) {
    cat(crayon::bold("Rotation details:\n"))
    cat("  Type:", x$rotation$type, "\n")
    if (!is.null(x$rotation$loadings_type)) {
      cat("  Loadings type:", x$rotation$loadings_type, "\n")
    }
    cat("\n")
  }
  
  invisible(x)
}


#' Screeplot for PCA
#'
#' Displays the variance explained by each principal component as a bar or line plot.
#' 
#' @param x A \code{pca} object.
#' @param type "barplot" or "lines".
#' @param main Plot title.
#' @param ... Additional args to pass to base R plotting.
#' @export
screeplot.pca <- function(x, type="barplot", main="Screeplot", ...) {
  pc_variance <- x$sdev^2
  percent_variance <- pc_variance / sum(pc_variance) * 100
  
  if (type == "barplot") {
    graphics::barplot(percent_variance, 
            xlab="Principal Component",
            ylab="Percentage of Variance Explained",
            names.arg=paste0("PC", 1:length(percent_variance)),
            main=main, ...)
  } else if (type == "lines") {
    plot(seq_len(length(percent_variance)), percent_variance, type="o",
         xlab="Component",
         ylab="Proportion of Variance",
         main=main, ...)
  }
  invisible(NULL)
}


#' PCA Outlier Diagnostics
#'
#' Calculates Hotelling T^2 (score distance) and Q-residual (orthogonal distance)
#' for each observation, given a chosen number of components.
#'
#' @param x A \code{pca} object.
#' @param X The original data matrix used for PCA.
#' @param ncomp Number of components to consider.
#' @param cutoff Logical or numeric specifying threshold for labeling outliers. If \code{TRUE},
#'   uses some typical statistical threshold (F-dist) for T^2, or sets an arbitrary Q limit.
#'   If numeric, treat it as a cutoff. Default is \code{FALSE} (no labeling).
#' @return A data frame with columns \code{T2} and \code{Q}, and optionally an outlier flag.
#' @export
pca_outliers <- function(x, X, ncomp, cutoff=FALSE) {
  # compute T2 (score distances)
  T2 <- score_distances.pca(x, ncomp, xorig=X)
  T2vals <- T2[, ncomp]
  
  # compute Q (residual distances)
  Q <- orth_distances.pca(x, ncomp, xorig=X)
  Qvals <- Q[, ncomp]
  
  # optional outlier logic...
  
  data.frame(T2 = T2vals, Q = Qvals)
}
</file>

<file path="R/multiblock.R">
#' Create a Multiblock Projector
#'
#' Constructs a multiblock projector using the given component matrix (`v`), a preprocessing function, and a list of block indices. 
#' This allows for the projection of multiblock data, where each block represents a different set of variables or features.
#'
#' @param v A matrix of components with dimensions `nrow(v)` by `ncol(v)` (columns = number of components).
#' @param preproc A pre-processing function for the data (default: `prep(pass())`).
#' @param block_indices A list of numeric vectors specifying the indices of each data block.
#' @param classes (optional) A character vector specifying additional class attributes of the object, default is NULL.
#' @param ... Extra arguments.
#' @return A `multiblock_projector` object.
#'
#' @seealso projector
#' @export
#' @examples
#' # Generate some example data
#' X1 <- matrix(rnorm(10 * 5), 10, 5)
#' X2 <- matrix(rnorm(10 * 5), 10, 5)
#' X <- cbind(X1, X2)
#'
#' # Compute PCA on the combined data
#' pc <- pca(X, ncomp = 8)
#'
#' # Create a multiblock projector using PCA components and block indices
#' mb_proj <- multiblock_projector(pc$v, block_indices = list(1:5, 6:10))
#'
#' # Project multiblock data using the multiblock projector
#' mb_scores <- project(mb_proj, X)
multiblock_projector <- function(v, preproc=prep(pass()), ..., block_indices, classes=NULL) {
  chk::chk_list(block_indices)
  sumind <- sum(sapply(block_indices, length))
  chk::chk_equal(sumind, nrow(v))
  
  projector(v, preproc, block_indices=block_indices, ..., classes=c(classes, "multiblock_projector"))
}


#' Create a Multiblock Bi-Projector
#'
#' Constructs a multiblock bi-projector using the given component matrix (`v`), score matrix (`s`), singular values (`sdev`),
#' a preprocessing function, and a list of block indices. This allows for two-way mapping with multiblock data.
#'
#' @param v A matrix of components (nrow = number of variables, ncol = number of components).
#' @param s A matrix of scores (nrow = samples, ncol = components).
#' @param sdev A numeric vector of singular values or standard deviations.
#' @param preproc A pre-processing object (default: `prep(pass())`).
#' @param block_indices A list of numeric vectors specifying data block variable indices.
#' @param classes Additional class attributes (default NULL).
#' @param ... Extra arguments.
#' @return A `multiblock_biprojector` object.
#'
#' @seealso bi_projector, multiblock_projector
#' @export
multiblock_biprojector <- function(v, s, sdev, preproc=prep(pass()), ..., block_indices, classes=NULL) {
  sumind <- sum(sapply(block_indices, length))
  chk::chk_equal(sumind, nrow(v))
  bi_projector(v, s=s, sdev=sdev, preproc=preproc, block_indices=block_indices, ..., classes=c(classes, "multiblock_biprojector", "multiblock_projector"))
}


#' Extract the Block Indices from a Multiblock Projector
#'
#' @param x A `multiblock_projector` object.
#' @param i Ignored.
#' @param ... Ignored.
#' @return The list of block indices.
#' @export
block_indices.multiblock_projector <- function(x,i,...) {
  x$block_indices
}

#' @export
block_lengths.multiblock_projector <- function(x) {
  sapply(block_indices(x), length)
}

#' @export
nblocks.multiblock_projector <- function(x) {
  length(block_indices(x))
}

#' Project Data onto a Specific Block
#'
#' Projects the new data onto the subspace defined by a specific block of variables.
#'
#' @param x A `multiblock_projector` object.
#' @param new_data The new data to be projected.
#' @param block The block index (1-based) to project onto.
#' @param least_squares Logical. If `TRUE` (default), use least squares projection.
#' @param ... Additional arguments passed to `partial_project`.
#' @return The projected scores for the specified block.
#' @export
project_block.multiblock_projector <- function(x, new_data, block,least_squares=TRUE, ...) {
  # Check block validity
  nb <- nblocks(x)
  if (block < 1 || block > nb) {
    stop("Block index out of range.")
  }
  
  ind <- block_indices(x)[[block]]
  partial_project(x, new_data, colind=ind, least_squares,...)
}

#' Coefficients for a Multiblock Projector
#'
#' Extracts the components (loadings) for a given block or the entire projector.
#'
#' @param object A `multiblock_projector` object.
#' @param block Optional block index. If missing, returns loadings for all variables.
#' @param ... Additional arguments.
#' @return A matrix of loadings.
#' @export
coef.multiblock_projector <- function(object, block,...) {
  if (missing(block)) {
    # Instead of NextMethod(object), just use NextMethod() to call coef.projector
    NextMethod()
  } else {
    nb <- nblocks(object)
    if (block < 1 || block > nb) {
      stop("Block index out of range.")
    }
    ind <- object$block_indices[[block]]
    object$v[ind,,drop=FALSE]
  }
}

#' Pretty Print Method for `multiblock_biprojector` Objects
#'
#' Display a summary of a `multiblock_biprojector` object.
#'
#' @param x A `multiblock_biprojector` object.
#' @param ... Additional arguments passed to `print()`.
#' @return Invisible `multiblock_biprojector` object.
#' @export
print.multiblock_biprojector <- function(x, ...) {
  cat("Multiblock Bi-Projector object:\n")
  cat("  Projection matrix dimensions: ", nrow(x$v), "x", ncol(x$v), "\n")
  
  # Print block indices in a nicer format
  cat("  Block indices:\n")
  lapply(seq_along(x$block_indices), function(i) {
    cat("    Block ", i, ": ", paste(x$block_indices[[i]], collapse = ","), "\n", sep="")
  })
  
  invisible(x)
}





#' @importFrom stats var
#' @importFrom RSpectra svds
#' @importFrom future.apply future_lapply
#' @export
perm_test.multiblock_biprojector <- function(
    x,
    Xlist          = NULL,          # optional list of blocks
    nperm          = 500,
    comps          = 4,             # max # components to test
    alpha          = 0.05,
    shuffle_fun    = NULL,          # custom shuffler
    parallel       = FALSE,
    alternative    = c("greater","less","two.sided"),
    use_rspectra   = TRUE,
    ...
){
  alternative <- match.arg(alternative)

  ## ------------------------------------------------------------------
  ## 1.  pull block indices & component count
  ## ------------------------------------------------------------------
  blk_ind   <- block_indices(x)
  B         <- length(blk_ind)
  Kmax      <- ncol(x$v)                    # components available
  comps     <- min(comps, Kmax)
  if (comps < 1L) stop("Need at least one component to test.")

  n         <- nrow(scores(x))

  ## ------------------------------------------------------------------
  ## 2.  helper  –  get block‑wise score matrix  (n × B)  for comp k
  ## ------------------------------------------------------------------
  get_Tk <- function(comp_k, data_list = NULL, S_perm = NULL){
      if (is.null(data_list)){          # use stored scores — fast
          # If S_perm is provided (already shuffled full score matrix), use it
          # Otherwise, use the original scores (only for calculating observed stat)
          sc_matrix <- if (!is.null(S_perm)) S_perm else scores(x)
          sc <- sc_matrix[ , comp_k, drop = FALSE]   # n × 1
          # Return a matrix with B identical columns of these scores
          do.call(cbind, replicate(B, sc, simplify = FALSE))
      } else {                          # re‑project if user gave data
          # Project each block in data_list onto component k using original model x
          lapply(seq_len(B), function(b){
              xb <- data_list[[b]]
              # project block b onto *single* component k
              nd_proc <- apply_transform(x$preproc, xb, blk_ind[[b]])
              v_sub   <- x$v[ blk_ind[[b]] , comp_k , drop = FALSE]
              as.vector(nd_proc %*% v_sub)
          }) |> do.call(cbind, args = _) # Result is n x B matrix
      }
  }

  ## ------------------------------------------------------------------
  ## 3.  statistic per component  –  leading eigenvalue of  TᵀT
  ## ------------------------------------------------------------------
  comp_stat <- function(Tk){
      S <- crossprod(Tk)                       # B × B
      if (B == 1) return(sum(Tk^2))            # trivial 1‑block case
      if (use_rspectra && requireNamespace("RSpectra", quietly = TRUE))
           RSpectra::svds(S, k = 1, nu = 0, nv = 1)$d[1]
      else eigen(S, symmetric = TRUE, only.values = TRUE)$values[1]
  }

  ## observed statistics ------------------------------------------------
  T_list_obs <- lapply(seq_len(comps), function(k){
      comp_stat( get_Tk(k, Xlist) )
  })
  obs_vec <- unlist(T_list_obs)

  ## ------------------------------------------------------------------
  ## 4.  default shuffle – independently permute rows of each block
  ## ------------------------------------------------------------------
  if (is.null(shuffle_fun)){
      shuffle_fun <- function(blks){           # blks is a list
         lapply(blks, function(mat) mat[sample(nrow(mat)), , drop = FALSE])
      }
  }

  ## pre‑extract data blocks (scores or original) -----------------------
  if (is.null(Xlist)){
      # We will shuffle the full original score matrix once per permutation
      S_orig <- scores(x) # n x Kmax
      # Ensure default shuffle works on this matrix N x Kmax
      if (is.null(shuffle_fun)) {
        shuffle_fun <- function(S, ...) S[sample(nrow(S)), , drop = FALSE]
      }
      data_to_shuffle <- S_orig 
      use_shuffled_scores_matrix = TRUE
  } else {
      # user gave full data; keep as list
      if (length(Xlist) != B)
         stop("Length of Xlist must equal number of blocks in the model.")
      # Default shuffle works on list of matrices
      if (is.null(shuffle_fun)){
           shuffle_fun <- function(blks, ...) {           # blks is a list
              lapply(blks, function(mat) mat[sample(nrow(mat)), , drop = FALSE])
           }
      }
      data_to_shuffle <- Xlist 
      use_shuffled_scores_matrix = FALSE
  }

  ## ------------------------------------------------------------------
  ## 5.  permutation loop (sequential stopping like PCA)
  ## ------------------------------------------------------------------
  Fperm   <- matrix(NA_real_, nrow = nperm, ncol = comps)
  n_ok    <- integer(comps)

  apply_fun <- if (parallel) future.apply::future_lapply else lapply
  if (parallel) message("perm_test.multiblock_biprojector : permutations in parallel …")

  # Pre-calculate observed stats outside loop
  obs_stats <- sapply(seq_len(comps), function(k) {
    Tk_obs <- get_Tk(k, Xlist) # Use original Xlist or scores
    comp_stat(Tk_obs)
  })

  for (k in seq_len(comps)){ # Loop over components for sequential testing
      perm_fun <- function(i){ # Function for one permutation
          # Shuffle either the original scores matrix OR the list of X blocks
          data_perm <- shuffle_fun(data_to_shuffle)
          
          # Calculate statistic for component k based on shuffled data
          if (use_shuffled_scores_matrix) {
              # Pass the shuffled full score matrix (data_perm) to get_Tk
              Tk_perm <- get_Tk(comp_k = k, S_perm = data_perm)
          } else {
              # Pass the shuffled list of X blocks (data_perm) to get_Tk for re-projection
              Tk_perm <- get_Tk(comp_k = k, data_list = data_perm)
          }
          comp_stat(Tk_perm)
      }
      
      # Run permutations for component k
      plist   <- apply_fun(seq_len(nperm), perm_fun)
      perm_vals_k <- unlist(plist)
      Fperm[, k] <- perm_vals_k # Store results for component k
      n_ok[k]    <- sum(is.finite(perm_vals_k))

      ## empirical p‑value for component k
      obs_k <- obs_stats[k]
      g      <- sum(perm_vals_k >= obs_k, na.rm = TRUE)
      l      <- sum(perm_vals_k <= obs_k, na.rm = TRUE)
      pval   <- switch(alternative,
                       greater   = (g + 1)/(n_ok[k] + 1),
                       less      = (l + 1)/(n_ok[k] + 1),
                       two.sided = min(1, 2*min((g + 1)/(n_ok[k]+1),
                                                (l + 1)/(n_ok[k]+1))))
      if (!is.na(pval) && pval > alpha){          # Vitale‑like stop rule
          comps_tested <- k
          Fperm <- Fperm[, seq_len(comps_tested), drop = FALSE]
          n_ok  <- n_ok[seq_len(comps_tested)]
          obs_stats <- obs_stats[seq_len(comps_tested)]
          break # Stop testing further components
      }
      comps_tested <- k # Record that this component was tested
  }# end loop over components
  
  # Rename obs_vec to obs_stats to avoid clash
  obs_vec <- obs_stats 

  ## ------------------------------------------------------------------
  ## 6.  tidy return object  (same shape as perm_test.pca())
  ## ------------------------------------------------------------------
  comp_df <- data.frame(comp      = seq_along(obs_vec),
                        observed  = obs_vec,
                        pval      = sapply(seq_along(obs_vec), function(j){
                                         g <- sum(Fperm[,j] >= obs_vec[j], na.rm = TRUE)
                                         (g + 1)/(n_ok[j] + 1)}),
                        lower_ci  = apply(Fperm, 2, quantile, 0.025, na.rm = TRUE),
                        upper_ci  = apply(Fperm, 2, quantile, 0.975, na.rm = TRUE))

  structure(
     list(call              = match.call(),
          component_results = comp_df,
          perm_values       = Fperm,
          alpha             = alpha,
          alternative       = alternative,
          method            = sprintf(
              "Permutation test for multiblock consensus (stat = leading‑eigenvalue, %d blocks)", B),
          nperm             = n_ok),
     class = c("perm_test_multiblock","perm_test")
  )
}



#' @export
perm_test.multiblock_projector <- function(x,
                                           Xlist,
                                           nperm       = 500,
                                           comps       = 4,
                                           shuffle_fun = NULL,
                                           measure_fun = NULL,
                                           parallel    = FALSE,
                                           alternative = c("greater", "less",
                                                           "two.sided"),
                                           alpha       = 0.05,
                                           ...) {

  ## ---------- safety checks ----------
  if (is.null(Xlist)) 
    stop("Xlist is required for perm_test.multiblock_projector")
  if (!is.list(Xlist))
    stop("Xlist must be a list of block matrices.")

  B <- nblocks(x)
  if (length(Xlist) != B)
    stop("Length(Xlist) (", length(Xlist),
         ") differs from nblocks(x) (", B, ").")

  blk_idx <- block_indices(x)
  p_each  <- sapply(blk_idx, length)
  for (b in seq_len(B))
    if (ncol(Xlist[[b]]) != p_each[b])
      stop("Block ", b, ": #cols in Xlist (", ncol(Xlist[[b]]), 
           ") does not match block_indices (", p_each[b], ").")

  N <- nrow(Xlist[[1]])
  if (any(sapply(Xlist, nrow) != N))
    stop("All blocks must have the same number of rows.")

  Kmax <- min(comps, ncol(x$v))
  alternative <- match.arg(alternative)

  ## ---------- helpers ----------
  # block‑specific preprocessing (reuse x$preproc) ----
  prep_all <- function(Xl) {
    # Concatenate, apply transform to full matrix, then split back
    p_all <- sum(p_each)
    Xall <- matrix(0.0, nrow=N, ncol=p_all) # Pre-allocate
    cidx <- 1
    for(b in 1:B) {
      Xall[, cidx:(cidx+p_each[b]-1)] <- Xl[[b]]
      cidx <- cidx + p_each[b]
    }
    Xproc_all <- apply_transform(x$preproc, Xall)
    
    # Split back into list
    Xp_list <- vector("list", B)
    cidx <- 1
    for(b in 1:B) {
      Xp_list[[b]] <- Xproc_all[, cidx:(cidx+p_each[b]-1), drop=FALSE]
      cidx <- cidx + p_each[b]
    }
    Xp_list
  }

  # compute block scores for first K comps using original model 'x' ----
  get_block_scores <- function(Xp_list, K) {
    v      <- coef(x)        # Use original model's loadings (p_tot × d)
    vlist  <- lapply(seq_len(B), function(b)
                     v[ blk_idx[[b]], 1:K, drop = FALSE ])
    mapply(`%*%`, Xp_list, vlist, SIMPLIFY = FALSE)  # list of (N × K) matrices
  }

  # default statistic: mean |corr| across block pairs ----
  default_measure <- function(Scores_list, K) {
    # Scores_list is a list (length B) of (N x K) score matrices
    out <- numeric(K)
    if (B < 2) return(rep(NA_real_, K)) # Correlation requires at least 2 blocks
    for (k in seq_len(K)) {
      corrs <- combn(B, 2, FUN = function(idx) {
        # Ensure scores are vectors for cor()
        score1 <- Scores_list[[idx[1]]][, k]
        score2 <- Scores_list[[idx[2]]][, k]
        if (stats::var(score1) < .Machine$double.eps || stats::var(score2) < .Machine$double.eps) {
            # Return NA or 0 if variance is zero to avoid cor error/NA
            return(NA_real_) 
        } else {
            stats::cor(score1, score2)
        }
      })
      # Handle potential NAs from zero variance columns
      out[k] <- mean(abs(corrs), na.rm = TRUE)
      if (is.nan(out[k])) out[k] <- 0 # If all pairs had zero variance, result is NaN -> 0
    }
    out
  }

  ## ---------- defaults for shuffle / fit / measure ----------
  if (is.null(shuffle_fun))
    # Use the standardized helper name
    shuffle_fun <- .default_multiblock_shuffle
    # Original shuffle_fun <- function(Blocks, ...) lapply(Blocks, function(M) M[sample(N), , drop = FALSE])

  if (is.null(measure_fun))
    measure_fun <- default_measure

  ## ---------- observed statistic ----------
  Xp_obs <- prep_all(Xlist) # Preprocess original data
  Scores_obs <- get_block_scores(Xp_obs, Kmax) # Project using original model
  T_obs <- measure_fun(Scores_obs, Kmax, ...) # Measure consensus on observed scores

  ## ---------- permutation loop ----------
  worker <- function(iter, ...) {
    Xperm <- shuffle_fun(Xlist, ...) # Shuffle original data
    Xp    <- prep_all(Xperm)         # Preprocess shuffled data
    
    # Project shuffled preprocessed data using original model 'x'
    Scores_perm <- get_block_scores(Xp, Kmax) 
    
    # Measure consensus on permuted scores
    stat_perm <- measure_fun(Scores_perm, Kmax, ...)
    if (length(stat_perm) != Kmax) {
        warning(sprintf("Permutation %d: measure_fun returned %d values, expected %d. Filling with NA.", iter, length(stat_perm), Kmax))
        stat_perm_full <- rep(NA_real_, Kmax)
        valid_len <- min(length(stat_perm), Kmax)
        stat_perm_full[1:valid_len] <- stat_perm[1:valid_len]
        return(stat_perm_full)
    }
    stat_perm
  }

  applyFUN <- if (parallel) future.apply::future_lapply else lapply
  perm_mat <- do.call(rbind,
                      applyFUN(seq_len(nperm), worker, ...))
  ok       <- stats::complete.cases(perm_mat)
  if (!all(ok)) {
    warning(sum(!ok), " permutations failed; removed.")
    perm_mat <- perm_mat[ok, , drop = FALSE]
  }
  n_ok <- nrow(perm_mat)
  if (n_ok == 0) stop("All permutations failed.")

  ## ---------- p‑values & sequential stop ----------
  keep   <- seq_len(Kmax)
  pvals  <- rep(NA_real_, Kmax)
  stopat <- Kmax
  for (k in keep) {
    if (alternative == "greater")
      pvals[k] <- (sum(perm_mat[, k] >= T_obs[k]) + 1) / (n_ok + 1)
    else if (alternative == "less")
      pvals[k] <- (sum(perm_mat[, k] <= T_obs[k]) + 1) / (n_ok + 1)
    else {          # two‑sided
      p_high <- (sum(perm_mat[, k] >= T_obs[k]) + 1) / (n_ok + 1)
      p_low  <- (sum(perm_mat[, k] <= T_obs[k]) + 1) / (n_ok + 1)
      pvals[k] <- 2 * min(p_high, p_low)
    }
    if (pvals[k] > alpha) { stopat <- k; break }
  }

  ci <- t(apply(perm_mat[, keep, drop = FALSE], 2,
                stats::quantile, probs = c(.025, .975)))

  comp_res <- data.frame(comp      = keep,
                         observed  = T_obs[keep],
                         p.value   = pvals[keep],
                         lower_ci  = ci[, 1],
                         upper_ci  = ci[, 2])

  structure(list(call              = match.call(),
                 component_results = comp_res,
                 perm_values       = perm_mat[, keep, drop = FALSE],
                 method            = "Permutation test for multiblock_projector (score-based consensus)",
                 nperm             = n_ok),
            class = c("perm_test_multiblock", "perm_test")
  )
}
</file>

<file path="R/utils.R">
#' @noRd
split_matrix <- function(X, fac) {
  idx <- split(1:nrow(X), fac)
  lapply(idx, function(i) X[i,])
}


#' Compute column-wise mean in X for each factor level of Y
#'
#' This function computes group means for each factor level of Y in the provided data matrix X.
#'
#' @param Y a vector of labels to compute means over disjoint sets
#' @param X a data matrix from which to compute means
#' @return a matrix with row names corresponding to factor levels of Y and column-wise means for each factor level
#' @export
#' @examples
#' # Example data
#' X <- matrix(rnorm(50), 10, 5)
#' Y <- factor(rep(1:2, each = 5))
#'
#' # Compute group means
#' gm <- group_means(Y, X)
group_means <- function (Y, X) {
  chk::chk_equal(nrow(X), length(Y))
  
  if (all(table(Y) == 1)) {
    warnings("`Y` does not contain more than one replicate of any level")
    row.names(X) <- names(table(Y))
    X
  }
  else {
    if (any(is.na(X))) {
      xspl <- split_matrix(X, Y)
      ret <- do.call(rbind, lapply(xspl, function(x) matrixStats::colMeans2(x, na.rm = TRUE)))
      row.names(ret) <- names(xspl)
      ret
    }
    else {
      Rs <- rowsum(X, Y, na.rm = TRUE)
      yt <- table(Y)
      ret <- sweep(Rs, 1, yt, "/")
      row.names(ret) <- names(yt)
      ret
    }
  }
}



#' @noRd
.avg_pair_princ_ang <- function(bases, ...) {
  warning("Subspace similarity method 'avg_pair' is not implemented.")
  return(NA_real_)
}

#' @noRd
.grassmann_dispersion <- function(bases, ...) {
  warning("Subspace similarity method 'grassmann' is not implemented.")
  return(NA_real_)
}

#' @noRd
.worst_case_angle <- function(bases, ...) {
  warning("Subspace similarity method 'worst_case' is not implemented.")
  return(NA_real_)
}

#' Compute subspace similarity
#' 
#' @param fits a list of bi_projector objects
#' @param method the method to use for computing subspace similarity
#' @param ... additional arguments to pass to the method
#' @return a numeric value representing the subspace similarity
#' @export
subspace_similarity <- function(fits,
                                method = c("avg_pair",     # mean of all pair–wise princ. angles
                                           "grassmann",    # Grassmann‑mean dispersion
                                           "worst_case"),  # max{ min angle of each pair }
                                ...) {
  method <- match.arg(method)
  bases  <- lapply(fits, function(f) qr.Q(qr(scores(f))))   # orthonormalise

  switch(method,
    avg_pair   = .avg_pair_princ_ang(bases, ...),
    grassmann  = .grassmann_dispersion(bases, ...),
    worst_case = .worst_case_angle(bases, ...)
  )
}



#' Principal angles (two sub‑spaces)
#'
#' @param fit1,fit2  bi_projector objects (or any object with $v loadings)
#' @param k          number of dimensions to compare (default: min(ncomp))
#' @return numeric vector of principal angles (radians, length = k)
#' @export
principal_angles <- function(fit1, fit2, k = NULL) {
  stopifnot(inherits(fit1, "bi_projector"),
            inherits(fit2, "bi_projector"))

  V1 <- fit1$v
  V2 <- fit2$v
  k  <- if (is.null(k)) min(ncol(V1), ncol(V2)) else k
  V1 <- qr.Q(qr(V1[, 1:k, drop = FALSE]))      # orthonormal bases
  V2 <- qr.Q(qr(V2[, 1:k, drop = FALSE]))

  s  <- svd(crossprod(V1, V2), nu = 0, nv = 0)$d  # singular values
  s  <- pmin(pmax(s, -1), 1)                      # numerical safety

  acos(s)                                         # angles in radians
}

#' Compute a regression model for each column in a matrix and return residual matrix
#' 
#' @param form the formula defining the model to fit for residuals
#' @param X the response matrix
#' @param design the \code{data.frame} containing the design variables specified in \code{form} argument.
#' @param intercept add an intercept term (default is FALSE)
#' 
#' @return a \code{matrix} of residuals
#' @examples 
#' 
#' X <- matrix(rnorm(20*10), 20, 10)
#' des <- data.frame(a=rep(letters[1:4], 5), b=factor(rep(1:5, each=4)))
#' xresid <- residualize(~ a+b, X, design=des)
#' 
#' ## design is saturated, residuals should be zero
#' xresid2 <- residualize(~ a*b, X, design=des)
#' sum(xresid2) == 0
#' @export
#' @importFrom stats model.matrix lsfit resid
residualize <- function(form, X, design, intercept=FALSE) {
  #options(contrasts = c("contr.sum", "contr.poly"))
  modmat <- model.matrix(form, data=design)
  stats::resid(lsfit(modmat, X, intercept=intercept))
}

#' Calculate Principal Angles Between Subspaces
#'
#' Computes the principal angles between two subspaces defined by the
#' columns of two orthonormal matrices Q1 and Q2.
#'
#' @param Q1 An n x p matrix whose columns form an orthonormal basis for the first subspace.
#' @param Q2 An n x q matrix whose columns form an orthonormal basis for the second subspace.
#' @return A numeric vector containing the principal angles in radians, sorted in ascending order.
#'         The number of angles is `min(p, q)`.
#' @export
#' @examples
#' # Example: Angle between xy-plane and a plane rotated 45 degrees around x-axis
#' Q1 <- cbind(c(1,0,0), c(0,1,0)) # xy-plane basis
#' theta <- pi/4
#' R <- matrix(c(1, 0, 0, 0, cos(theta), sin(theta), 0, -sin(theta), cos(theta)), 3, 3)
#' Q2 <- R %*% Q1 # Rotated basis
#' angles_rad <- prinang(Q1, Q2)
#' angles_deg <- angles_rad * 180 / pi
#' print(angles_deg) # Should be approximately 0 and 45 degrees
#'
#' # Example with PCA loadings (after ensuring orthonormality if needed)
#' # Assuming pca1$v and pca2$v are loading matrices (variables x components)
#' # Orthonormalize them first if they are not already (e.g., from standard SVD)
#' # Q1 <- qr.Q(qr(pca1$v[, 1:3]))
#' # Q2 <- qr.Q(qr(pca2$v[, 1:3]))
#' # prinang(Q1, Q2)
prinang <- function(Q1, Q2) {
  # Basic dimension checks
  if (nrow(Q1) != nrow(Q2)) {
    stop("Q1 and Q2 must have the same number of rows.")
  }
  p <- ncol(Q1)
  q <- ncol(Q2)
  if (p == 0 || q == 0) {
    warning("One or both matrices have zero columns. Returning empty vector.")
    return(numeric(0))
  }
  
  # Optional: Add checks for orthonormality (can be computationally expensive)
  # tol <- 1e-8
  # if (max(abs(crossprod(Q1) - diag(p))) > tol || max(abs(crossprod(Q2) - diag(q))) > tol) {
  #   warning("Input matrices may not be orthonormal. Results might be inaccurate.")
  # }
  
  # Compute the SVD of the cross-product
  svd_res <- svd(crossprod(Q1, Q2))
  
  # Singular values are cosines of the principal angles
  # Clamp values to [-1, 1] to avoid domain errors in acos due to potential numerical inaccuracies
  cos_thetas <- pmax(-1, pmin(1, svd_res$d))
  
  # Angles are acos of singular values
  angles <- acos(cos_thetas)
  
  # Return angles sorted in ascending order
  sort(angles)
}
</file>

<file path="R/pre_process.R">
#' construct a new pre-processing pipeline
#' 
#' Creates a bare prepper object (a pipeline holder).
#' 
#' TODO: Consider using a single environment in the finalized pre_processor
#'       to store all step parameters, instead of individual environments
#'       per step, potentially improving serialization and GC performance
#'       for very large pipelines.
#' 
#' @keywords internal
#' @noRd
prepper <- function() {
  steps <- list()
  ret <- list(steps=steps)
  class(ret) <- c("prepper", "list")
  ret
}

#' Add a pre-processing node to a pipeline
#' 
#' @param x A `prepper` pipeline
#' @param step The pre-processing step to add
#' @param ... Additional arguments
#' @export
add_node.prepper <- function(x, step,...) {
  x$steps[[length(x$steps)+1]] <- step
  invisible(x) # Return invisibly for better pipe behavior
}



#' @export
prep.prepper <- function(x,...) {
  # Use local to capture a stable copy of steps
  local({
    steps <- x$steps
    orig_ncol <- NULL # Placeholder for original number of columns
    
    # init transform: applies all forward steps and learns parameters
    tinit <- function(X) {
      xin <- X
      # Store original dimension when initializing
      assign("orig_ncol", ncol(X), envir = parent.env(environment()))
      for (st in steps) {
        xin <- st$forward(xin)
      }
      xin
    }
    
    # transform: apply learned steps in forward direction using partial 'colind'
    tform <- function(X, colind=NULL) {
      xin <- X
      for (st in steps) {
        xin <- st$apply(xin, colind)
      }
      xin
    }
    
    # reverse_transform: apply learned steps in reverse order
    rtform <- function(X, colind=NULL) {
      xin <- X
      # Use rev() for clarity and robustness
      for (st in rev(steps)) {
        xin <- st$reverse(xin, colind)
      }
      xin
    }
    
    ret <- list(
      preproc = x, # Store the original prepper structure if needed
      init = tinit,
      transform = tform,
      reverse_transform = rtform,
      # Store orig_ncol after init is called
      get_orig_ncol = function() orig_ncol 
    )
    
    class(ret) <- "pre_processor"
    ret
  })
}


#' @export
fresh.prepper <- function(x,...) {
  p <- prepper()
  for (step in x$steps) {
    # Recreate each node by calling 'prep_node' again
    p <- prep_node(p, step$name, step$create)
  }
  p
}

#' @export
init_transform.pre_processor <- function(x, X,...) {
  chk::chk_matrix(X)
  res <- x$init(X)
  # After init, orig_ncol should be set, store it directly in the object? 
  # Modifying the object after creation might be tricky with environments.
  # For now, rely on the closure environment within prep.prepper and get_orig_ncol.
  res
}

#' @export
apply_transform.pre_processor <- function(x, X, colind=NULL,...) {
  chk::chk_matrix(X)
  
  # GENERAL CHECK: Ensure preprocessor is initialized before any application
  # REMOVED: This check prevents using preprocessors with manually supplied parameters
  # orig_ncol <- x$get_orig_ncol()
  # if (is.null(orig_ncol)) {
  #     stop("Preprocessor must be initialized with 'init_transform' before applying transformations.")
  # }
  
  # COLIND SPECIFIC CHECKS: Only if colind is provided
  if (!is.null(colind)) {
    # Check colind against original dimension (orig_ncol is known to be non-NULL here)
    # Need to get orig_ncol without stopping if NULL
    orig_ncol <- x$get_orig_ncol() # May be NULL, that's okay now
    chk::chk_vector(colind)
    if (!is.null(orig_ncol)) { # Only check subset if orig_ncol is known
        chk::chk_subset(colind, 1:orig_ncol)
    }
    # Ensure the provided X matches the dimension implied by colind
    chk::chk_equal(ncol(X), length(colind))
  }
  
  x$transform(X, colind)
}

#' @export
reverse_transform.pre_processor <- function(x, X, colind=NULL,...) {
  chk::chk_matrix(X)
  
  # GENERAL CHECK: Ensure preprocessor is initialized before any reversal
  # REMOVED: This check prevents using preprocessors with manually supplied parameters
  # orig_ncol <- x$get_orig_ncol()
  # if (is.null(orig_ncol)) {
  #     stop("Preprocessor must be initialized with 'init_transform' before reversing transformations.")
  # }
  
  # COLIND SPECIFIC CHECKS: Only if colind is provided
  if (!is.null(colind)) {
    # Check colind against original dimension (orig_ncol is known to be non-NULL here)
    # Need to get orig_ncol without stopping if NULL
    orig_ncol <- x$get_orig_ncol() # May be NULL, that's okay now
    chk::chk_vector(colind)
     if (!is.null(orig_ncol)) { # Only check subset if orig_ncol is known
        chk::chk_subset(colind, 1:orig_ncol)
    }
    # Ensure the provided X matches the dimension implied by colind
    chk::chk_equal(ncol(X), length(colind))
  }
  
  x$reverse_transform(X, colind)
}

#' @export
fresh.pre_processor <- function(x, preproc=prepper(),...) {
  # Attempt to recreate the original pipeline from stored steps in x$preproc
  # similar to fresh.prepper
  chk::chk_s3_class(x$preproc, "prepper")
  fresh(x$preproc)
}

#' prepare a new node and add to pipeline
#' 
#' @param pipeline the pre-processing pipeline
#' @param name the name of the step to add
#' @param create the creation function
#' 
#' @keywords internal
#' @noRd
prep_node <- function(pipeline, name, create,  ...) {
  node <- create()
  ret <- list(name=name,
              create=create,
              forward=node$forward,
              reverse=node$reverse,
              apply=node$apply,
              ...)
  class(ret) <- c(name, "pre_processor")
  add_node(pipeline, ret)
}

new_pre_processor <- function(x) {
  chk::chk_not_null(x[["forward"]])
  chk::chk_not_null(x[["apply"]])
  chk::chk_not_null(x[["reverse"]])
  chk::chk_function(x[["forward"]])
  chk::chk_function(x[["apply"]])
  chk::chk_function(x[["reverse"]])
  
  funlist <- x
  structure(funlist,
            class="pre_processing_step")
}


#' a no-op pre-processing step
#' 
#' `pass` simply passes its data through the chain
#' 
#' @param preproc the pre-processing pipeline
#' @return a `prepper` list 
#' @export
pass <- function(preproc=prepper()) {
  
  create <- function() {
    list(
      forward = function(X, colind=NULL) {
        X
      },
      
      reverse = function(X, colind=NULL) {
        X
      },
      
      apply = function(X, colind=NULL) {
        X
      }
    )
  }
  
  prep_node(preproc, "pass", create)
}


#' center a data matrix
#' 
#' remove mean of all columns in matrix
#' 
#' @param cmeans optional vector of precomputed column means
#' 
#' @inheritParams pass
#' @export
#' @importFrom Matrix colMeans
#' @return a `prepper` list 
center <- function(preproc = prepper(), cmeans=NULL) {
  create <- function() {
    env <- rlang::new_environment()
    env[["cmeans"]] <- cmeans
    
    list(
      forward = function(X) {
        if (is.null(env$cmeans)) {
          cm <- colMeans(X)
          env$cmeans <- cm
        } else {
          cm <- env$cmeans
          chk::chk_equal(ncol(X), length(cm))
        }
        sweep(X, 2, cm, "-")
      },
      
      apply = function(X, colind = NULL) {
        cm <- env$cmeans
        chk::chk_not_null(cm, "Means not initialized. Run init_transform first or supply 'cmeans'.")
        if (is.null(colind)) {
          sweep(X, 2, cm, "-")
        } else {
          chk::chk_equal(ncol(X), length(colind))
          sweep(X, 2, cm[colind], "-")
        }
      },
      
      reverse = function(X, colind = NULL) {
        chk::chk_not_null(env$cmeans)
        if (is.null(colind)) {
          # Use only the means corresponding to the columns present in X
          nc <- ncol(X)
          if (nc > length(env$cmeans)) {
             stop(sprintf("Internal error in center$reverse: ncol(X) [%d] > length(stored means) [%d]", 
                           nc, length(env$cmeans)))
          } 
          means_to_use <- env$cmeans[1:nc]
          sweep(X, 2, means_to_use, "+")
        } else {
          chk::chk_equal(ncol(X), length(colind))
          sweep(X, 2, env$cmeans[colind], "+")
        }
      }
    )
  }
  
  prep_node(preproc, "center", create)
}


#' scale a data matrix
#' 
#' normalize each column by a scale factor.
#' 
#' @inheritParams pass
#' 
#' @param type the kind of scaling, `unit` norm, `z`-scoring, or precomputed `weights`
#' @param weights optional precomputed weights
#' @return a `prepper` list 
#' @export
colscale <- function(preproc = prepper(),
                     type = c("unit", "z", "weights"),
                     weights = NULL) {
  type <- match.arg(type)
  
  if (type != "weights" && !is.null(weights)) {
    warning("colscale: weights ignored because type != 'weights'")
  }
  if (type == "weights") {
    chk::chk_not_null(weights)
  }
  
  create <- function() {
    env <- rlang::new_environment()
    env$weights <- weights # Store precomputed weights if provided
    
    list(
      forward = function(X) {
        if (is.null(env$weights)) { # Compute weights only if not precomputed
          sds <- matrixStats::colSds(as.matrix(X)) # Ensure matrix for colSds
          
          # Handle zero standard deviations robustly
          zero_sd <- sds < .Machine$double.eps
          if (any(zero_sd)) {
              warning(sprintf("Columns %s have zero standard deviation. Setting scale factor to 1.", 
                              paste(which(zero_sd), collapse=", ")))
          }
          sds[zero_sd] <- 1 # Set zero SDs to 1 to avoid Inf weights
          
          if (type == "unit") {
            # Unit norm scaling: weight is 1 / (sd * sqrt(N-1))
            # Ensure N > 1 for sqrt
            N <- nrow(X)
            if (N <= 1) stop("Cannot compute unit norm scaling with N <= 1.")
            scaling_factor <- sds * sqrt(N - 1)
            # Check for zeros again after scaling (unlikely but possible)
            scaling_factor[scaling_factor < .Machine$double.eps] <- 1
            wts <- 1 / scaling_factor
          } else { # type == "z"
            # z-scaling weight is 1 / sd
            wts <- 1 / sds
          }
          env$weights <- wts
        } else {
            wts <- env$weights
            chk::chk_equal(length(wts), ncol(X))
        }
        sweep(X, 2, wts, "*")
      },
      
      apply = function(X, colind = NULL) {
        chk::chk_not_null(env$weights, "Weights not initialized. Run init_transform first.")
        wts <- env$weights
        if (is.null(colind)) {
          sweep(X, 2, wts, "*")
        } else {
          # Ensure X matches colind length, and colind is valid (checked by caller)
          chk::chk_equal(ncol(X), length(colind))
          sweep(X, 2, wts[colind], "*")
        }
      },
      
      reverse = function(X, colind = NULL) {
        chk::chk_not_null(env$weights, "Weights not initialized. Run init_transform first.")
        wts <- env$weights
        # Handle potential division by zero if weights were somehow zero
        wts_safe <- wts
        wts_safe[abs(wts) < .Machine$double.eps] <- 1 # Avoid division by zero
        
        if (is.null(colind)) {
          # Use only the weights corresponding to the columns present in X
          nc <- ncol(X)
           if (nc > length(wts_safe)) {
             stop(sprintf("Internal error in colscale$reverse: ncol(X) [%d] > length(stored weights) [%d]", 
                           nc, length(wts_safe)))
           } 
          weights_to_use <- wts_safe[1:nc]
          sweep(X, 2, weights_to_use, "/")
        } else {
          chk::chk_equal(ncol(X), length(colind))
          sweep(X, 2, wts_safe[colind], "/")
        }
      }
    )
  }
  
  prep_node(preproc, "colscale", create)
}


#' center and scale each vector of a matrix
#' 
#' @param cmeans an optional vector of column means
#' @param sds an optional vector of sds
#' @inheritParams pass
#' @return a `prepper` list 
#' @export
standardize <- function(preproc = prepper(), cmeans=NULL, sds=NULL) {
  create <- function() {
    env <- rlang::new_environment()
    list(
      forward = function(X) {
        if (is.null(sds)) {
          sds2 <- matrixStats::colSds(X)
        } else {
          chk::chk_equal(length(sds), ncol(X))
          sds2 <- sds
        }
        
        if (is.null(cmeans)) {
          cmeans2 <- colMeans(X)
        } else {
          chk::chk_equal(length(cmeans), ncol(X))
          cmeans2 <- cmeans
        }
        
        if (all(sds2 == 0)) {
          sds2[] <- mean(sds2[sds2>0], na.rm=TRUE)
          sds2[is.na(sds2)] <- 1 # fallback if all zero
        }
        
        env$sds <- sds2
        env$cmeans <- cmeans2
        
        # TODO: [INEFF] This uses two sweep() calls. For large matrices, 
        #       consider combining into a single pass or using optimized 
        #       functions if performance becomes critical.
        x1 <- sweep(X, 2, cmeans2, "-")
        sweep(x1, 2, sds2, "/")
      },
      
      apply = function(X, colind = NULL) {
        sds2 <- env$sds
        cmeans2 <- env$cmeans
        chk::chk_not_null(sds2, "SDs not initialized. Run init_transform first or supply 'sds'.")
        chk::chk_not_null(cmeans2, "Means not initialized. Run init_transform first or supply 'cmeans'.")
        if (is.null(colind)) {
          # TODO: [INEFF] See forward() - two sweep() calls.
          x1 <- sweep(X, 2, cmeans2, "-")
          sweep(x1, 2, sds2, "/")
        } else {
          chk::chk_equal(ncol(X), length(colind))
          # TODO: [INEFF] See forward() - two sweep() calls.
          x1 <- sweep(X, 2, cmeans2[colind], "-")
          sweep(x1, 2, sds2[colind], "/")
        }
      },
      
      reverse = function(X, colind = NULL) {
        sds2 <- env$sds
        cmeans2 <- env$cmeans
        chk::chk_not_null(sds2, "SDs not initialized.")
        chk::chk_not_null(cmeans2, "Means not initialized.")
        
        if (is.null(colind)) {
          # Use only parameters corresponding to columns in X
          nc <- ncol(X)
          if (nc > length(sds2) || nc > length(cmeans2)) {
               stop(sprintf("Internal error in standardize$reverse: ncol(X) [%d] inconsistent with stored params [%d, %d]", 
                           nc, length(sds2), length(cmeans2)))
          }
          sds_to_use <- sds2[1:nc]
          means_to_use <- cmeans2[1:nc]
          
          # TODO: [INEFF] Two sweep() calls.
          x0 <- sweep(X, 2, sds_to_use, "*")
          sweep(x0, 2, means_to_use, "+")
        } else {
          chk::chk_equal(ncol(X), length(colind))
          # TODO: [INEFF] Two sweep() calls.
          x0 <- sweep(X, 2, sds2[colind], "*")
          sweep(x0, 2, cmeans2[colind], "+")
        }
      }
    )
  }
  prep_node(preproc, "standardize", create)
}


#' bind together blockwise pre-processors
#' 
#' concatenate a sequence of pre-processors, each applied to a block of data.
#' 
#' @param preprocs a list of initialized `pre_processor` objects
#' @param block_indices a list of integer vectors specifying the global column indices for each block
#' @return a new `pre_processor` object that applies the correct transformations blockwise
#' @examples 
#' 
#' p1 <- center() |> prep()
#' p2 <- center() |> prep()
#' 
#' x1 <- rbind(1:10, 2:11)
#' x2 <- rbind(1:10, 2:11)
#' 
#' p1a <- init_transform(p1,x1)
#' p2a <- init_transform(p2,x2)
#' 
#' clist <- concat_pre_processors(list(p1,p2), list(1:10, 11:20))
#' t1 <- apply_transform(clist, cbind(x1,x2))
#' 
#' t2 <- apply_transform(clist, cbind(x1,x2[,1:5]), colind=1:15)
#' @export
concat_pre_processors <- function(preprocs, block_indices) {
  chk::chk_equal(length(preprocs), length(block_indices))
  
  unraveled_ids <- unlist(block_indices)
  # Check for overlaps and completeness more thoroughly?
  if (any(duplicated(unraveled_ids))) stop("Duplicate indices found in block_indices.")
  # Assuming indices cover a contiguous range for simplicity now, but could add checks.
  max_idx <- max(unraveled_ids)
  
  # Precompute mapping for efficiency and correct ordering
  map_list <- lapply(seq_along(block_indices), function(i) {
      list(orig_indices = block_indices[[i]], 
           proc = preprocs[[i]])
  })
  names(map_list) <- as.character(seq_along(map_list))

  # Internal helper for applying functions blockwise respecting colind order
  # TODO: [INEFF] This helper recalculates mappings on each call.
  #       If called repeatedly with the same colind structure (unlikely?),
  #       pre-calculating and caching the exact mappings might be faster.
  apply_blockwise <- function(func, X, colind) {
    chk::chk_matrix(X)
    chk::chk_vector(colind)
    # Remove unsupported named arguments x_arg, y_arg
    chk::chk_equal(ncol(X), length(colind))
    
    results_list <- vector("list", length(colind))
    processed_cols <- logical(length(colind)) # Track which columns are processed
    
    # Find which original blocks are relevant for the given colind
    for (block_num in seq_along(map_list)) {
        block_info <- map_list[[block_num]]
        # Find which of the requested `colind` fall into this block's original indices
        matched_in_colind_idx <- which(colind %in% block_info$orig_indices)
        
        if (length(matched_in_colind_idx) > 0) {
            # Get the subset of X corresponding to these columns
            X_subset <- X[, matched_in_colind_idx, drop = FALSE]
            # Get the original GLOBAL indices within this block that correspond to X_subset
            orig_indices_in_block <- colind[matched_in_colind_idx]
            
            # Map global indices to the LOCAL indices expected by the block's preprocessor
            local_indices_for_proc <- match(orig_indices_in_block, block_info$orig_indices)
            if (any(is.na(local_indices_for_proc))) {
                stop("Internal error: Mismatch between requested global indices and block's original indices.")
            }
            
            # Apply the function using the LOCAL indices relative to the block
            res_subset <- func(block_info$proc, X_subset, colind = local_indices_for_proc)
            
            # Place results back in the correct positions in the output list
            results_list[matched_in_colind_idx] <- lapply(seq_len(ncol(res_subset)), function(k) res_subset[,k])
            processed_cols[matched_in_colind_idx] <- TRUE
        }
    }
    
    if (!all(processed_cols)) {
        stop("Some requested columns in 'colind' did not map to any provided block.")
    }
    
    # Combine results respecting original colind order
    do.call(cbind, results_list)
  }
  
  ret <- list(
    # Note: init_transform is not defined for concat_pre_processor
    # User should initialize individual preprocs before concatenating.
    transform = function(X, colind = NULL) {
      chk::chk_matrix(X)
      if (is.null(colind)) {
        # Apply to full blocks in their natural order
        chk::chk_equal(ncol(X), length(unraveled_ids))
        res_list <- lapply(seq_along(map_list), function(i) {
          block_info <- map_list[[i]]
          apply_transform(block_info$proc, X[, block_info$orig_indices, drop = FALSE])
        })
        do.call(cbind, res_list)
      } else {
        # Apply blockwise respecting colind order
        apply_blockwise(apply_transform, X, colind)
      }
    },
    reverse_transform = function(X, colind = NULL) {
      chk::chk_matrix(X)
      if (is.null(colind)) {
        # Reverse full blocks in their natural order
        chk::chk_equal(ncol(X), length(unraveled_ids))
        res_list <- lapply(seq_along(map_list), function(i) {
          block_info <- map_list[[i]]
          reverse_transform(block_info$proc, X[, block_info$orig_indices, drop = FALSE])
        })
        do.call(cbind, res_list)
      } else {
        # Reverse blockwise respecting colind order
        apply_blockwise(reverse_transform, X, colind)
      }
    }
    # Store map_list? Might be useful for introspection.
    # map_list = map_list 
  )
  
  class(ret) <- c("concat_pre_processor", "pre_processor")
  ret
}



#' @export
apply_transform.concat_pre_processor <- function(x, X, colind=NULL,...) {
  # Directly call the transform function stored within the concat object
  x$transform(X, colind, ...)
}

#' @export
reverse_transform.concat_pre_processor <- function(x, X, colind=NULL,...) {
  # Directly call the reverse_transform function stored within the concat object
  x$reverse_transform(X, colind, ...)
}

#' Print a prepper pipeline
#' 
#' Uses `crayon` to produce a colorful and readable representation of the pipeline steps.
#'
#' @param x A `prepper` object.
#' @param ... Additional arguments (ignored).
#' @export
print.prepper <- function(x,...) {
  nn <- sapply(x$steps, function(st) st$name)
  if (length(nn) == 0) {
    cat(crayon::cyan("A preprocessor with no steps.\n"))
    return(invisible(x))
  }
  
  cat(crayon::bold(crayon::green("Preprocessor pipeline:\n")))
  for (i in seq_along(nn)) {
    cat(crayon::magenta(" Step ", i, ": "), crayon::cyan(nn[i]), "\n", sep="")
  }
  invisible(x)
}


#' Print a pre_processor object
#' 
#' Display information about a `pre_processor` using crayon-based formatting.
#' 
#' @param x A `pre_processor` object.
#' @param ... Additional arguments (ignored).
#' @export
print.pre_processor <- function(x, ...) {
  # A pre_processor comes from prep(prepper)
  # It has x$preproc to show original steps.
  # Let's show the chain of steps and indicate it's finalized.
  
  cat(crayon::bold(crayon::green("A finalized pre-processing pipeline:\n")))
  if (!is.null(x$preproc) && inherits(x$preproc, "prepper")) {
    nn <- sapply(x$preproc$steps, function(st) st$name)
    if (length(nn) == 0) {
      cat(crayon::cyan("  No steps.\n"))
    } else {
      for (i in seq_along(nn)) {
        cat(crayon::magenta(" Step ", i, ": "), crayon::cyan(nn[i]), "\n", sep="")
      }
    }
  } else {
    cat(crayon::cyan("  No associated prepper information.\n"))
  }
  invisible(x)
}


#' Print a concat_pre_processor object
#' 
#' @param x A `concat_pre_processor` object.
#' @param ... Additional arguments (ignored).
#' @export
print.concat_pre_processor <- function(x, ...) {
  cat(crayon::bold(crayon::green("A concatenated (blockwise) pre-processing pipeline:\n")))
  cat(crayon::cyan("  This object applies different pre-processors to distinct column blocks.\n"))
  invisible(x)
}
</file>

<file path="R/regress.R">
#' Multi-output linear regression
#'
#' Fit a multivariate regression model for a matrix of basis functions, `X`, and a response matrix `Y`.
#' The goal is to find a projection matrix that can be used for mapping and reconstruction.
#'
#' @param X the set of independent (basis) variables
#' @param Y the response matrix
#' @param preproc the pre-processor (currently unused)
#' @param method the regression method: `lm`, `enet`, `mridge`, or `pls`
#' @param intercept whether to include an intercept term
#' @param lambda ridge shrinkage parameter (for methods `mridge` and `enet`)
#' @param alpha the elastic net mixing parameter if method is `enet`
#' @param ncomp number of PLS components if method is `pls`
#' @param ... extra arguments sent to the underlying fitting function
#' @return a bi-projector of type `regress`. The `sdev` component of this object
#'   stores the standard deviations of the columns of the design matrix (`X` potentially
#'   including an intercept) used in the fit, not the standard deviations of latent
#'   components as might be typical in other `bi_projector` contexts (e.g., SVD).
#' @export
#' @importFrom glmnet glmnet
#' @importFrom Matrix t
#' @importFrom pls plsr
#' @importFrom stats coef lm.fit sd
#' @examples
#' # Generate synthetic data
#' set.seed(123) # for reproducibility
#' Y <- matrix(rnorm(10 * 100), 10, 100)
#' X <- matrix(rnorm(10 * 9), 10, 9)
#' 
#' # Fit regression models and reconstruct the fitted response matrix
#' r_lm <- regress(X, Y, intercept = FALSE, method = "lm")
#' recon_lm <- reconstruct(r_lm) # Reconstructs fitted Y
#' 
#' r_mridge <- regress(X, Y, intercept = TRUE, method = "mridge", lambda = 0.001)
#' recon_mridge <- reconstruct(r_mridge)
#' 
#' r_enet <- regress(X, Y, intercept = TRUE, method = "enet", lambda = 0.001, alpha = 0.5)
#' recon_enet <- reconstruct(r_enet)
#' 
#' r_pls <- regress(X, Y, intercept = TRUE, method = "pls", ncomp = 5)
#' recon_pls <- reconstruct(r_pls)
regress <- function(X, Y, preproc=pass(), method=c("lm", "enet", "mridge", "pls"), 
                    intercept=FALSE, lambda=.001, alpha=0, 
                    # Default ncomp for PLS: arbitrary, consider tuning
                    ncomp=ceiling(ncol(X)/2), ...) {
  method <- match.arg(method)
  
  # --- Preprocessing Handling ---
  # Ensure preproc is initialized and apply it to X
  if (!inherits(preproc, "pre_processor")) {
    # If it's a prepper object or similar, finalize it
    proc <- prep(preproc)
  } else {
    # Already a finalized pre_processor
    proc <- preproc
  }
  
  # Initialize and apply the transform to X
  # Note: We process X *before* adding the intercept column
  X_processed <- init_transform(proc, X)
  
  # --- Intercept Handling ---
  # Manually add intercept column to the *processed* X if requested,
  # and tell underlying fitters NOT to add one.
  if (intercept) {
    X_fit <- cbind(Intercept = 1, X_processed)  
    intercept_flag <- FALSE            
  } else {
    X_fit <- X_processed
    intercept_flag <- FALSE # Fitters should not add intercept if not in X_fit
  }
  
  # Store the processed design matrix (including intercept if added)
  # that coefficients will correspond to
  # TODO: [INEFF] Storing the full design matrix can be memory intensive.
  #       Consider alternatives if X_fit is very large.
  scores <- X_fit 
  
  # Compute betas depending on the method
  betas <- {
    # Compute betas depending on the method (using X_fit)
    b <- if (method == "lm") {
      # Use lm.fit for potentially better performance/stability than lsfit
      lfit <- stats::lm.fit(X_fit, Y)
      # Coefficients: rows are predictors (matching X_fit cols), cols are responses
      t(stats::coef(lfit)) # Transpose to get shape (p_out x p_in)
      
    } else if (method == "mridge") {
      if (!requireNamespace("glmnet", quietly = TRUE)) {
          stop("Package 'glmnet' needed for method='mridge'. Please install it.", call. = FALSE)
      }
      gfit <- glmnet::glmnet(X_fit, Y, alpha = 0, family = "mgaussian", 
                             lambda = lambda, intercept = intercept_flag, ...) 
      cf <- stats::coef(gfit, s = lambda)
      bmat <- do.call(cbind, lapply(cf, as.matrix))
      # Transpose to get p_out x (p_in+1 if intercept fitted by glmnet)
      t(bmat)

    } else if (method == "enet") {
      if (!requireNamespace("glmnet", quietly = TRUE)) {
          stop("Package 'glmnet' needed for method='enet'. Please install it.", call. = FALSE)
      }
      gfit <- glmnet::glmnet(X_fit, Y, alpha = alpha, family = "mgaussian", 
                             lambda = lambda, intercept = intercept_flag, ...)
      cf <- stats::coef(gfit, s = lambda)
      bmat <- do.call(cbind, lapply(cf, as.matrix))
      # Transpose to get p_out x (p_in+1 if intercept fitted by glmnet)
      t(bmat)
      
    } else { # method == "pls"
      if (!requireNamespace("pls", quietly = TRUE)) {
          stop("Package 'pls' needed for method='pls'. Please install it.", call. = FALSE)
      }
      fit <- pls::plsr(Y ~ scores, ncomp = ncomp, data = data.frame(Y=Y, scores=scores), ...) # Need data frame for formula
      cf <- stats::coef(fit, ncomp = ncomp, intercept = FALSE) # Intercept already in 'scores'
      # Result is typically p_in x p_out, need p_out x p_in
      t(cf)
    }
    
    # FIX: Remove intercept column from glmnet betas if intercept was FALSE for regress() input
    if (method %in% c("mridge", "enet") && !intercept) {
      b[, -1, drop=FALSE] # Remove the first column (intercept)
    } else {
      b
    }
  }
  
  # Create a bi_projector
  # v = betas (coefficients, p_out x p_in)
  # s = scores (design matrix X_fit, N x p_in)
  # Y_approx = s %*% t(v)
  
  # Calculate sdev for the scores matrix (X_fit)
  sds <- matrixStats::colSds(as.matrix(scores))
  # Handle columns with zero standard deviation (like intercept)
  zero_sd_idx <- sds < .Machine$double.eps
  if (any(zero_sd_idx)) {
      # Optional: Warn the user
      # warning("Columns ", paste(which(zero_sd_idx), collapse=", "), " in the design matrix have zero standard deviation. Setting sdev to 1.")
      sds[zero_sd_idx] <- 1
  }
  
  p <- bi_projector(v = betas, 
                    s = scores,
                    sdev = sds, # Pass the calculated standard deviations
                    preproc=proc, # Store the *initialized* preprocessor
                    coefficients = betas, # Store betas explicitly
                    method = method,
                    classes = "regress")
  p
}


#' @export
inverse_projection.regress <- function(x,...) {
  # inverse projection should map scores back to Y (or approx Y)
  # Y_approx = scores %*% t(betas)
  # If v = betas, then inverse_projection = t(v) = t(betas)
  t(x$coefficients) # This matches if v = betas
}

#' @export
project_vars.regress <- function(x, new_data,...) {
  if (is.vector(new_data)) {
    new_data <- matrix(new_data)
  }
  # Check dimension: new_data rows = nrow(scores(x))
  chk::chk_equal(nrow(new_data), nrow(scores(x)))
  
  # project_vars for regress: t(new_data) %*% scores(x)
  # Projects new variables onto the original predictor space.
  # If new_data is NxM and scores is NxC, result is MxC
  t(new_data) %*% (scores(x))
}

#' Partial Inverse Projection for a `regress` Object
#'
#' This function computes a sub-block inversion of the regression coefficients,
#' allowing you to focus on only certain columns (e.g. partial factors).
#' If your coefficient matrix is not orthonormal or is not square, we use a
#' pseudoinverse approach (via `corpcor::pseudoinverse`) to find a minimal-norm
#' solution. 
#'
#' @param x A `regress` object (created by \code{\link{regress}}).
#' @param colind A numeric vector specifying which columns of the \emph{factor space}
#'        (i.e., the second dimension of \code{x$coefficients}) you want to invert.
#'        Typically these refer to a subset of canonical / PCA / PLS components.
#' @param ... Further arguments passed to or used by methods (not used here).
#'
#' @return A matrix of shape \code{(length(colind) x nrow(x$coefficients))}. When
#'         multiplied by partial factor scores \code{(n x length(colind))}, it yields
#'         an \code{(n x nrow(x$coefficients))} reconstruction in the original domain.
#'
#' @export
partial_inverse_projection.regress <- function(x, colind, ...) {
  # We assume x$coefficients is shape (p_out x p_in),
  # where p_out is # of outputs, and p_in is # of (X dimension).
  # For partial factors, we interpret 'colind' as columns
  # in that factor dimension. So we subset horizontally.
  
  # Subset columns of x$coefficients
  # e.g., if x$coefficients is (p_out x d_total),
  # then betas_sub is (p_out x length(colind)).
  betas_sub <- x$coefficients[, colind, drop=FALSE]
  
  # Inverse it (in a minimal-norm sense) with corpcor::pseudoinverse
  # This yields (length(colind) x p_out).
  if (!requireNamespace("corpcor", quietly = TRUE)) {
    stop("Package 'corpcor' must be installed for partial_inverse_projection in `regress`.")
  }
  inv_mat_sub <- corpcor::pseudoinverse(betas_sub)
  
  inv_mat_sub
}

#' Pretty Print Method for `regress` Objects
#'
#' Display a human-readable summary of a `regress` object using crayon formatting, 
#' including information about the method and dimensions.
#'
#' @param x A `regress` object (a bi_projector with regression info).
#' @param ... Additional arguments passed to `print()`.
#' @export
print.regress <- function(x, ...) {
  cat(crayon::bold(crayon::green("Regression bi_projector object:\n")))
  
  # Display method
  if (!is.null(x$method)) {
    cat(crayon::yellow("  Method: "), crayon::cyan(x$method), "\n", sep="")
  } else {
    cat(crayon::yellow("  Method: "), crayon::cyan("unknown"), "\n", sep="")
  }
  
  # Input/Output dims from v
  cat(crayon::yellow("  Input dimension: "), nrow(x$v), "\n", sep="")
  cat(crayon::yellow("  Output dimension: "), ncol(x$v), "\n", sep="")
  
  # Check if intercept was used: If intercept present, betas includes an extra row
  # but we have no direct flag. The code doesn't store intercept explicitly,
  # so we won't guess. Let's just print coefficients dim:
  cat(crayon::yellow("  Coefficients dimension: "),
      paste(dim(x$coefficients), collapse=" x "), "\n")
  
  invisible(x)
}
</file>

<file path="DESCRIPTION">
Package: multivarious
Title: Extensible Data Structures for Multivariate Analysis
Version: 0.2.0
Authors@R: 
    person(given = "Bradley",
           family = "Buchsbaum",
           role = c("aut", "cre"),
           email = "brad.buchsbaum@gmail.com",
           comment = c(ORCID = "0000-0002-1108-4866"))
Description: Provides a set of basic and extensible data structures and functions for multivariate analysis, including dimensionality reduction techniques, projection methods, and preprocessing functions. The aim of this package is to offer a flexible and user-friendly framework for multivariate analysis that can be easily extended for custom requirements and specific data analysis tasks.
License: MIT + file LICENSE
Encoding: UTF-8
LazyData: true
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.3.2.9000
Imports: 
    rlang,
    chk,
    glmnet,
    corpcor,
    Matrix,
    rsvd,
    svd,
    pls,
    irlba,
    RSpectra,
    proxy,
    matrixStats,
    ggplot2,
    ggrepel,
    future.apply,
    tibble,
    tidyr,
    dplyr,
    crayon,
    MASS,
    methods,
    cli,
    withr,
    assertthat,
    future,
    geigen,
    PRIMME,
    GPArotation
Suggests: 
    covr,
    randomForest,
    testthat,
    magrittr,
    knitr,
    rmarkdown
URL: https://bbuchsbaum.github.io/multivarious/
VignetteBuilder: knitr
</file>

<file path="R/projector.R">
#' Construct a `projector` instance
#'
#' A `projector` maps a matrix from an N-dimensional space to d-dimensional space, where `d` may be less than `N`.
#' The projection matrix, `v`, is not necessarily orthogonal. This function constructs a `projector` instance which can be
#' used for various dimensionality reduction techniques like PCA, LDA, etc.
#'
#' @param v A matrix of coefficients with dimensions `nrow(v)` by `ncol(v)` (columns = components)
#' @param preproc A prepped pre-processing object (S3 class `pre_processor`). Default is the no-op `pass()` preprocessor.
#' @param classes Additional class information used for creating subtypes of `projector`. Default is NULL.
#' @param ... Extra arguments to be stored in the `projector` object.
#'
#' @return An instance of type `projector`.
#'
#' @export
projector <- function(v, preproc = prep(pass()), ..., classes = NULL) {
  # B3: Ensure v is matrix
  if (!is.matrix(v)) {
    v <- as.matrix(v)
  }
  
  # chkor_vld(chk::chk_matrix(v), chk::chk_s4_class(v, "Matrix")) # Removed: Redundant after as.matrix and caused error
  chk::chk_s3_class(preproc, "pre_processor")
  
  out <- structure(
    list(
      v       = v,
      preproc = preproc,
      ...), 
    class = c(classes, "projector")
  )
  
  # Add cache environment attribute
  attr(out, ".cache") <- new.env(parent = emptyenv())
  out
}

#' @export
components.projector <- function(x, ...) {
  x$v
}



#' @export
coef.projector <- function(object, ...) {
  components(object)
}


#' @export
#' @importFrom assertthat assert_that
ncomp.projector <- function(x) {
  assertthat::assert_that(inherits(x, "projector"))
  ncol(components(x))
}

#' Stricter check for true orthogonality
#'
#' We test if v^T * v = I (when rows >= cols) or v * v^T = I (when cols > rows).
#'
#' @param tol tolerance for checking orthogonality
#' @param x the projector object
#' @export
#' @importFrom assertthat assert_that
is_orthogonal.projector <- function(x, tol=1e-6) {
  assertthat::assert_that(inherits(x, "projector"))
  v <- components(x)
  
  # If p >= d, check crossprod(v). Otherwise, check tcrossprod(v).
  if (nrow(v) >= ncol(v)) {
    mat <- crossprod(v)  # d x d
  } else {
    mat <- tcrossprod(v) # p x p
  }
  
  # We want it ~ identity. Check diagonal ~ 1 and off-diagonal ~ 0
  diag_elems <- diag(mat)
  off_diag   <- mat - diag(diag_elems)
  all(abs(diag_elems - 1) < tol) && all(abs(off_diag) < tol)
}

#' Possibly use ridge-regularized inversion of crossprod(v)
#' @keywords internal
robust_inv_vTv <- function(v, lambda = 1e-6) {
  vt_v   <- crossprod(v)                # d x d
  vt_v_r <- vt_v + diag(lambda, nrow(vt_v))
  solve(vt_v_r)
}


#' @importFrom assertthat assert_that
inverse_projection.projector <- function(x, ...) {
  assertthat::assert_that(inherits(x, "projector"))
  
  # Robust caching check
  cache_env <- attr(x, ".cache")
  use_caching <- !is.null(cache_env) && is.environment(cache_env)
  key <- "inv_proj"

  if (use_caching && !is.null(cache_env[[key]])) {
    return(cache_env[[key]])
  }

  # Compute if not cached or cache not available
  v <- coef(x)
  if (!requireNamespace("corpcor", quiet=TRUE)) {
    stop("package corpcor required for inverse_projection.")
  }
  inv_p <- corpcor::pseudoinverse(v)

  # Store in cache if available
  if (use_caching) {
    cache_env[[key]] <- inv_p
  }

  inv_p
}


#' @export
partial_inverse_projection.projector <- function(x, colind, ...) {
  assertthat::assert_that(inherits(x, "projector"))
  chk::chk_vector(colind)
  chk::chk_whole_numeric(colind)
  chk::chk_range(max(colind, na.rm=TRUE), c(1, ncol(coef(x))))
  chk::chk_range(min(colind, na.rm=TRUE), c(1, ncol(coef(x))))

  # Robust caching check
  cache_env <- attr(x, ".cache")
  use_caching <- !is.null(cache_env) && is.environment(cache_env)
  key <- paste0(".pinv_v_subset_", paste(sort(unique(colind)), collapse = "_"))

  if (use_caching && !is.null(cache_env[[key]])) {
    return(cache_env[[key]])
  }

  # Compute if not cached or cache not available
  v <- coef(x)[, colind, drop=FALSE]
  if (!requireNamespace("corpcor", quiet=TRUE)) {
      stop("package corpcor required for partial_inverse_projection.")
  }
  pinv_sub <- corpcor::pseudoinverse(v)

  # Store in cache if available
  if (use_caching) {
    cache_env[[key]] <- pinv_sub
  }

  pinv_sub
}


#' @export
truncate.projector <- function(x, ncomp) {
  old_ncomp <- ncomp(x)
  chk::chk_number(ncomp)
  if (ncomp < 1 || ncomp > old_ncomp) {
    stop("Requested ncomp must be between 1 and ", old_ncomp)
  }
  
  v_new       <- components(x)[, seq_len(ncomp), drop = FALSE]
  x$v         <- v_new
  cache_env   <- attr(x, ".cache")
  if (!is.null(cache_env)) {
    rm(list = ls(cache_env), envir = cache_env)  # Clear the cache
  }
  x
}


#' @export
reprocess.projector <- function(x, new_data, colind = NULL, ...) {
  p <- nrow(components(x))
  if (is.null(colind)) {
    # Full dimension
    chk::chk_equal(ncol(new_data), p)
    apply_transform(x$preproc, new_data)
  } else {
    chk::chk_equal(length(colind), ncol(new_data))
    apply_transform(x$preproc, new_data, colind)
  }
}

#' @export
shape.projector <- function(x, ...) {
  dim(components(x))
}


#' @export
project.projector <- function(x, new_data, ...) {
  if (is.vector(new_data)) {
    chk::chk_equal(length(new_data), shape(x)[1])
    new_data <- matrix(new_data, nrow = 1)
  }
  chk::vld_matrix(new_data)
  chk::chk_equal(ncol(new_data), nrow(components(x)))
  
  # Reprocess
  nd_proc <- reprocess(x, new_data)
  nd_proc %*% components(x)
}


#' @export
partial_project.projector <- function(x,
                                      new_data,
                                      colind,
                                      least_squares = TRUE,
                                      lambda = 1e-6,
                                      ...)
{
  # shape checks
  if (is.vector(new_data) && length(colind) > 1) {
    new_data <- matrix(new_data, nrow = 1)
  } else if (is.vector(new_data) && length(colind) == 1) {
    new_data <- matrix(new_data, ncol = 1)
  }
  chk::chk_equal(ncol(new_data), length(colind))
  
  # reprocess partial
  nd_proc <- reprocess(x, new_data, colind)
  
  v_sub <- components(x)[colind, , drop = FALSE]  # subset
  if (least_squares) {
    inv_vt_v   <- robust_inv_vTv(v_sub, lambda = lambda)  # (d x d)
    factor_scr <- nd_proc %*% v_sub %*% inv_vt_v
  } else {
    factor_scr <- nd_proc %*% v_sub
  }
  factor_scr
}


#' @export
reconstruct_new.projector <- function(x,
                                      new_data,
                                      colind         = NULL,
                                      least_squares  = TRUE,
                                      lambda         = 1e-6,
                                      ...)
{
  v_full <- components(x)
  
  if (is.null(colind)) {
    # Reconstruct all columns => new_data is (n x p)
    nd_proc <- reprocess(x, new_data, colind = NULL)
    if (least_squares) {
      inv_vt_v    <- robust_inv_vTv(v_full, lambda = lambda)
      factor_scr  <- nd_proc %*% v_full %*% inv_vt_v
    } else {
      factor_scr  <- nd_proc %*% v_full
    }
    rec_data <- factor_scr %*% t(v_full)
    
    # If you wish, do a reverse transform on rec_data:
    # Apply reverse transform to return to original space
    reverse_transform(x$preproc, rec_data)
  } else {
    # PARTIAL reconstruction => new_data is (n x length(colind))
    factor_scr <- partial_project.projector(
      x, new_data, colind,
      least_squares = least_squares,
      lambda        = lambda
    )
    v_sub    <- v_full[colind, , drop = FALSE]
    rec_data <- factor_scr %*% t(v_sub)
    
    # Similarly, you could do a partial reverse_transform:
    # Apply reverse transform (potentially partial if needed) to return to original space
    # Assuming reverse_transform handles partial cases correctly based on input dim
    reverse_transform(x$preproc, rec_data)
  }
}


#' @export
print.projector <- function(x, ...) {
  cat(crayon::bold(crayon::green("Projector object:\n")))
  cat(crayon::yellow("  Input dimension: "),  shape(x)[1], "\n", sep = "")
  cat(crayon::yellow("  Output dimension: "), shape(x)[2], "\n", sep = "")
  
  if (!is.null(x$preproc)) {
    cat(crayon::cyan("  With pre-processing:\n"))
    if (inherits(x$preproc, "pre_processor")) {
      print(x$preproc)
    } else {
      cat(crayon::cyan("    (pre-processing pipeline not fully available)\n"))
    }
  } else {
    cat(crayon::cyan("  No pre-processing pipeline.\n"))
  }
  invisible(x)
}



#' @export
partial_projector.projector <- function(x, colind, ...) {
  # We'll store a reference to the original (full) projector in x$porig
  # so partial calls can delegate back.
  new_obj <- projector(x$v[colind, ],
                       preproc = x$preproc,
                       colind  = colind,
                       porig   = x,
                       classes = "partial_projector")
  new_obj
}


#' @export
reprocess.partial_projector <- function(x, new_data, colind = NULL, ...) {
  chk::chk_not_null(x$colind)
  
  if (is.null(colind)) {
    # Full dimension = length(x$colind)
    chk::chk_equal(ncol(new_data), nrow(components(x)))
    apply_transform(x$preproc, new_data)
  } else {
    chk::chk_equal(length(colind), ncol(new_data))
    base_colind <- x$colind
    chk::chk_not_null(base_colind)
    # Now map the local colind to the original projector's colind
    # e.g. base_colind[colind]
    apply_transform(x$preproc, new_data, base_colind[colind])
  }
}


#' @export
project.partial_projector <- function(x, new_data, ...) {
  chk::chk_not_null(x$porig)
  chk::chk_not_null(x$colind)
  partial_project(x$porig, new_data, x$colind, ...)
}


#' @export
truncate.partial_projector <- function(x, ncomp) {
  chk::chk_not_null(x$porig)
  old_ncomp <- ncomp(x)
  if (ncomp < 1 || ncomp > old_ncomp) {
    stop("Requested ncomp must be between 1 and ", old_ncomp)
  }
  
  porig_trunc <- truncate(x$porig, ncomp)  # preserve extra fields
  # partial_projector from the truncated original
  partial_projector(porig_trunc, x$colind)
}

#' @export
partial_project.partial_projector <- function(x, new_data, colind, ...) {
  # Ensure we have a reference to the original projector and the stored colind
  if (is.null(x$porig) || is.null(x$colind)) {
    stop("`x` must have `porig` (original projector) and `colind` fields.")
  }
  
  # Local check: colind must be within [1, length(x$colind)]
  n_local_cols <- length(x$colind)
  if (any(colind < 1) || any(colind > n_local_cols)) {
    stop(
      "Requested columns (", paste(colind, collapse=", "), 
      ") exceed local partial projector's range [1..", n_local_cols, "]."
    )
  }
  
  # Map the local colind to the original projector's colind
  global_colind <- x$colind[colind]
  
  # Delegate to the original projector's partial_project
  partial_project(x$porig, new_data, global_colind, ...)
}
</file>

<file path="R/all_generic.R">
#' New sample projection
#'
#' Project one or more samples onto a subspace. This function takes a model fit and new observations, and projects them onto the
#' subspace defined by the model. This allows for the transformation of new data into the same lower-dimensional space as the original data.
#'
#' @param x The model fit, typically an object of class bi_projector or any other class that implements a project method
#' @param new_data A matrix or vector of new observations with the same number of columns as the original data. Rows represent observations and columns represent variables
#' @param ... Extra arguments to be passed to the specific project method for the object's class
#' @return A matrix or vector of the projected observations, where rows represent observations and columns represent the lower-dimensional space
#' @export
#' @family project
#' @seealso \code{\link{bi_projector}} for an example of a class that implements a project method
#' @examples
#' # Example with the bi_projector class
#' X <- matrix(rnorm(10*20), 10, 20)
#' svdfit <- svd(X)
#' p <- bi_projector(svdfit$v, s = svdfit$u %% diag(svdfit$d), sdev=svdfit$d)
#'
#' # Project new_data onto the same subspace as the original data
#' new_data <- matrix(rnorm(5*20), 5, 20)
#' projected_data <- project(p, new_data)
project <- function(x, new_data, ...) UseMethod("project")



#' Partially project a new sample onto subspace
#'
#' Project a selected subset of column indices (`colind`) of `new_data` onto
#' the subspace defined by the model \code{x}. Optionally do a
#' ridge-regularized least-squares solve if columns are non-orthonormal.
#'
#' @param x The fitted model, e.g. `bi_projector`, that has a partial_project method.
#' @param new_data A numeric matrix (n x length(colind)) or vector, representing
#'   the observations to be projected.
#' @param colind A numeric vector of column indices in the original data space
#'   that correspond to \code{new_data}'s columns.
#' @param least_squares Logical; if TRUE (default), do a ridge-regularized solve.
#' @param lambda Numeric; ridge penalty (default 1e-6). Ignored if `least_squares=FALSE`.
#' @param ... Additional arguments passed to class-specific partial_project methods.
#'
#' @return A numeric matrix (n x d) of factor scores in the model's subspace, for
#'   those columns only.
#' @export
partial_project <- function(x, new_data, colind,
                            least_squares = TRUE,
                            lambda = 1e-6,
                            ...) {
  UseMethod("partial_project")
}




#' Construct a partial projector
#'
#' Create a new projector instance restricted to a subset of input columns. This function allows for the generation of
#' a new projection object that focuses only on the specified columns, enabling the projection of data using a limited
#' set of variables.
#'
#' @param x The original `projector` instance, typically an object of class `bi_projector` or any other class that implements a `partial_projector` method
#' @param colind A numeric vector of column indices to select in the projection matrix. These indices correspond to the variables used for the partial projector
#' @param ... Additional arguments passed to the underlying `partial_projector` method
#' @return A new `projector` instance, with the same class as the original object, that is restricted to the specified subset of input columns
#' @export
#' @seealso \code{\link{bi_projector}} for an example of a class that implements a `partial_projector` method
#' @examples
#' # Example with the bi_projector class
#' X <- matrix(rnorm(10*20), 10, 20)
#' svdfit <- svd(X)
#' p <- bi_projector(svdfit$v, s = svdfit$u %*% diag(svdfit$d), sdev=svdfit$d)
#'
#' # Create a partial projector using only the first 10 variables
#' colind <- 1:10
#' partial_p <- partial_projector(p, colind)
partial_projector <- function(x, colind, ...) UseMethod("partial_projector")
  


#' Project a single "block" of data onto the subspace
#'
#' When observations are concatenated into "blocks", it may be useful to project one block from the set.
#' This function facilitates the projection of a specific block of data onto a subspace. It is a
#' convenience method for multi-block fits and is equivalent to a "partial projection" where the
#' column indices are associated with a given block.
#'
#' @param x The model fit, typically an object of a class that implements a `project_block` method
#' @param new_data A matrix or vector of new observation(s) with the same number of columns as the original data
#' @param block An integer representing the block ID to select in the block projection matrix. This ID corresponds to the specific block of data to be projected
#' @param least_squares Logical. If `TRUE` use least squares projection.
#' @param ... Additional arguments passed to the underlying `project_block` method
#' @return A matrix or vector of the projected data for the specified block
#' @export
#' @family project
#' @seealso \code{\link{project}} for the generic projection function
project_block <- function(x, new_data, block, least_squares, ...) UseMethod("project_block")




#' Project one or more variables onto a subspace
#'
#' This function projects one or more variables onto a subspace. It is often called supplementary variable
#' projection and can be computed for a biorthogonal decomposition, such as Singular Value Decomposition (SVD).
#'
#' @param x The model fit, typically an object of a class that implements a `project_vars` method
#' @param new_data A matrix or vector of new observation(s) with the same number of rows as the original data
#' @param ... Additional arguments passed to the underlying `project_vars` method
#' @return A matrix or vector of the projected variables in the subspace
#' @export
#' @family project
#' @seealso \code{\link{project}} for the generic projection function for samples
project_vars <- function(x, new_data, ...) UseMethod("project_vars")


#' Transpose a model
#'
#' This function transposes a model by switching coefficients and scores. It is useful when you want to
#' reverse the roles of samples and variables in a model, especially in the context of dimensionality
#' reduction methods.
#'
#' @param x The model fit, typically an object of a class that implements a `transpose` method
#' @param ... Additional arguments passed to the underlying `transpose` method
#' @return A transposed model with coefficients and scores switched
#' @export
#' @family transpose
#' @seealso \code{\link{bi_projector}} for an example of a two-way mapping model that can be transposed
transpose <- function(x,...) UseMethod("transpose")



#' Reconstruct the data
#'
#' Reconstruct a data set from its (possibly) low-rank representation. This can be useful when analyzing
#' the impact of dimensionality reduction or when visualizing approximations of the original data.
#'
#' @param x The model fit, typically an object of a class that implements a `reconstruct` method
#' @param comp A vector of component indices to use in the reconstruction
#' @param rowind The row indices to reconstruct (optional). If not provided, all rows are used.
#' @param colind The column indices to reconstruct (optional). If not provided, all columns are used.
#' @param ... Additional arguments passed to the underlying `reconstruct` method
#' @return A reconstructed data set based on the selected components, rows, and columns
#' @export
#' @family reconstruct
#' @seealso \code{\link{bi_projector}} for an example of a two-way mapping model that can be reconstructed
reconstruct <- function(x, comp, rowind, colind, ...) UseMethod("reconstruct")



#' Reconstruct new data in a model's subspace
#'
#' This function takes a model (e.g., `projector` or `bi_projector`) and a new dataset,
#' and computes the rank-d approximation of the new data in the same subspace that
#' was defined by the model. In other words, we \strong{project} the new data into
#' the fitted subspace and then \strong{map it back} to the original dimensionality.
#'
#' Similar to \code{\link{reconstruct}} but operates on an external \code{new_data}
#' rather than the original fitted data. Often used to see how well the model's
#' subspace explains unseen data.
#'
#' @param x The fitted model object (e.g., \code{bi_projector}) that defines
#'   a subspace or factorization.
#' @param new_data A numeric matrix (or data frame) of shape
#'   \code{(n x p_full)} or possibly fewer columns if you allow partial reconstruction.
#' @param ... Additional arguments passed to the specific \code{reconstruct_new} method
#'   for the class of \code{x}.
#'
#' @return A numeric matrix (same number of rows as \code{new_data}, and typically
#'   the same number of columns if you're reconstructing fully) representing the
#'   rank-d approximation in the model's subspace.
#'
#' @export
#' @family reconstruct
#' @seealso \code{\link{reconstruct}} for reconstructing the original data in the model.
reconstruct_new <- function(x, new_data, ...) {
  UseMethod("reconstruct_new")
}



#' Transfer data from one domain/block to another via a latent space
#'
#' Convert between data representations in a multiblock or cross-decomposition
#' model by projecting the input `new_data` from the `from` domain/block
#' onto a latent space and then reconstructing it in the `to` domain/block.
#'
#' @param x The model fit, typically an object that implements a `transfer` method
#'   and ideally a `block_names` method.
#' @param new_data The data to transfer, typically matching the dimension of the `from` domain.
#' @param from Character string or index identifying the source domain/block.
#'   Must be present in `block_names(x)` if that method exists.
#' @param to Character string or index identifying the target domain/block.
#'   Must be present in `block_names(x)` if that method exists.
#' @param opts A list of optional arguments controlling the transfer process:
#'   \describe{
#'     \item{`cols`}{Optional numeric vector specifying column indices of the *target* 
#'       domain to reconstruct. If NULL (default), reconstructs all columns.}
#'     \item{`comps`}{Optional numeric vector specifying which latent components to use 
#'       for the projection/reconstruction. If NULL (default), uses all components.}
#'     \item{`ls_rr`}{Logical; if TRUE, use a ridge-regularized LS approach for the 
#'       initial projection from the `from` domain. Default FALSE.}
#'     \item{`lambda`}{Numeric ridge penalty (if `ls_rr=TRUE`). Default 1e-6.}
#'   }
#' @param ... Additional arguments passed to specific methods (discouraged, prefer `opts`).
#'
#' @return A matrix or data frame representing the transferred data in the `to` domain/block 
#'   (or a subset of columns/components if specified in `opts`).
#' @export
transfer <- function(x, new_data, from, to, opts = list(), ...) {
  UseMethod("transfer")
}

#' Default method for transfer
#' @param x Object
#' @param ... Ignored
#' @importFrom cli cli_abort
#' @export
#' @noRd
transfer.default <- function(x, ...) {
    cli::cli_abort(c(
      "!" = "{.fn transfer} is not implemented for {.cls {class(x)[1]}}.",
      "i" = "Provide a method or ensure the object inherits from a supported class."
      # Consider adding: "i" = "Supported classes include: cross_projector, ..."
    ))
}


## TODO 
## partial_residuals?
## partial_reconstruct?

#' Obtain residuals of a component model fit
#'
#' Calculate the residuals of a model after removing the effect of the first `ncomp` components.
#' This function is useful to assess the quality of the fit or to identify patterns that are not
#' captured by the model.
#'
#' @param x The model fit object.
#' @param ncomp The number of components to factor out before calculating residuals.
#' @param xorig The original data matrix (X) used to fit the model.
#' @param ... Additional arguments passed to the method.
#' @return A matrix of residuals, with the same dimensions as the original data matrix.
#' @export
#' @family residuals
residuals <- function(x, ncomp, xorig, ...) UseMethod("residuals")


#' Retrieve the component scores
#'
#' Extract the factor score matrix from a fitted model. The factor scores represent the projections of the
#' data onto the components, which can be used for further analysis or visualization.
#'
#' @param x The model fit object.
#' @param ... Additional arguments passed to the method.
#' @return A matrix of factor scores, with rows corresponding to samples and columns to components.
#' @export
#' @family scores
#' @seealso \code{\link{project}} for projecting new data onto the components.
scores <- function(x,...) UseMethod("scores")



#' Compute standardized component scores
#'
#' Calculate standardized factor scores from a fitted model. Standardized scores are useful for comparing
#' the contributions of different components on the same scale, which can help in interpreting the results.
#'
#' @param x The model fit object.
#' @param ... Additional arguments passed to the method.
#' @return A matrix of standardized factor scores, with rows corresponding to samples and columns to components.
#' @export
#' @seealso \code{\link{scores}} for retrieving the original component scores.
std_scores <- function(x, ...) UseMethod("std_scores")



#' get the components
#' 
#' Extract the component matrix of a fit.
#' 
#' @param x the model fit
#' @param ... extra args
#' @return the component matrix
#' @export
components <- function(x,...) UseMethod("components")



#' Shape of the Projector
#'
#' Get the input/output shape of the projector.
#'
#' This function retrieves the dimensions of the sample loadings matrix `v` in the form of a vector with two elements.
#' The first element is the number of rows in the `v` matrix, and the second element is the number of columns.
#'
#' @param x The model fit.
#' @param ... Extra arguments.
#' @return A vector containing the dimensions of the sample loadings matrix `v` (number of rows and columns).
#' @export
shape <- function(x,...) UseMethod("shape")


#' Inverse of the Component Matrix
#'
#' Return the inverse projection matrix, which can be used to map back to data space.
#' If the component matrix is orthogonal, then the inverse projection is the transpose of the component matrix.
#'
#' @param x The model fit.
#' @param ... Extra arguments.
#' @return The inverse projection matrix.
#' @export
#' @seealso \code{\link{project}} for projecting data onto the subspace.
inverse_projection <- function(x, ...) UseMethod("inverse_projection")


#' Partial Inverse Projection of a Columnwise Subset of Component Matrix
#'
#' Compute the inverse projection of a columnwise subset of the component matrix (e.g., a sub-block).
#' Even when the full component matrix is orthogonal, there is no guarantee that the partial component matrix is orthogonal.
#'
#' @param x A fitted model object, such as a `projector`, that has been fit to a dataset.
#' @param colind A numeric vector specifying the column indices of the component matrix to consider for the partial inverse projection.
#' @param ... Additional arguments to be passed to the specific model implementation of `partial_inverse_projection`.
#' @return A matrix representing the partial inverse projection.
#' @export
partial_inverse_projection <- function(x, colind, ...) UseMethod("partial_inverse_projection")


#' Compose Two Projectors
#'
#' Combine two projector models into a single projector by sequentially applying the first projector and then the second projector.
#'
#' @param x A fitted model object (e.g., `projector`) that has been fit to a dataset and will be applied first in the composition.
#' @param y A second fitted model object (e.g., `projector`) that has been fit to a dataset and will be applied after the first projector.
#' @param ... Additional arguments to be passed to the specific model implementation of `compose_projector`.
#' @return A new `projector` object representing the composed projector, which can be used to project data onto the combined subspace.
#' @export
compose_projector <- function(x,y,...) UseMethod("compose_projector")


#' Get a fresh pre-processing node cleared of any cached data
#' 
#' @param x the processing pipeline
#' @param ... extra args
#' @return a fresh pre-processing pipeline
#' @export
fresh <- function(x,...) UseMethod("fresh")



#' add a pre-processing stage
#' 
#' @param x the processing pipeline
#' @param step the pre-processing step to add
#' @param ... extra args
#' @export
#' @return a new pre-processing pipeline with the added step
add_node <- function(x, step, ...) UseMethod("add_node")


#' prepare a dataset by applying a pre-processing pipeline
#' 
#' @param x the pipeline
#' @param ... extra args
#' @return the pre-processed data
#' @export
prep <- function(x, ...) UseMethod("prep")


#' apply pre-processing parameters to a new data matrix
#' 
#' Given a new dataset, process it in the same way the original data was processed (e.g. centering, scaling, etc.)
#' 
#' @param x the model fit object
#' @param new_data the new data to process
#' @param colind the column indices of the new data
#' @param ... extra args
#' @return the reprocessed data
#' @export
reprocess <- function(x, new_data, colind, ...) UseMethod("reprocess")


#' refit a model
#' 
#' refit a model given new data or new parameter(s)
#'
#'
#' @param x the original model fit object
#' @param new_data the new data to process
#' @param ... extra args
#' @return a refit model object
#' @export
refit <- function(x, new_data, ...) UseMethod("refit")


#' Get the number of components
#'
#' This function returns the total number of components in the fitted model.
#'
#' @param x A fitted model object.
#' @return The number of components in the fitted model.
#' @export
#' @examples
#' # Example using the svd_wrapper function
#' data(iris)
#' X <- iris[, 1:4]
#' fit <- svd_wrapper(X, ncomp = 3, preproc = center(), method = "base")
#' ncomp(fit) # Should return 3
ncomp <- function(x) UseMethod("ncomp")


#' standard deviations 
#' 
#' The standard deviations of the projected data matrix
#' 
#' @param x the model fit
#' @return the standard deviations
#' @export
sdev <- function(x) UseMethod("sdev")

#' is it orthogonal
#' 
#' test whether components are orthogonal
#' 
#' @param x the object
#' @param tol tolerance for checking orthogonality
#' @return a logical value indicating whether the transformation is orthogonal
is_orthogonal <- function(x, tol=1e-6) UseMethod("is_orthogonal")


#' truncate a component fit
#' 
#' take the first n components of a decomposition
#' 
#' @param x the object to truncate
#' @param ncomp number of components to retain
#' @return a truncated object (e.g. PCA with 'ncomp' components)
#' @export
truncate <- function(x, ncomp) UseMethod("truncate")


#' get block_lengths
#' 
#' extract the lengths of each block in a multiblock object
#' 
#' @param x the object
#' @export
#' @return the block lengths
block_lengths <- function(x) UseMethod("block_lengths")


#' get block_indices 
#' 
#' extract the list of indices associated with each block in a `multiblock` object
#' 
#' @param x the object
#' @param ... extra args
#' @export
#' @return a list of block indices
block_indices <- function(x, ...) UseMethod("block_indices")


#' get the number of blocks
#' 
#' The number of data blocks in a multiblock element
#' 
#' @param x the object
#' @return the number of blocks
#' @export
nblocks <- function(x) UseMethod("nblocks")


#' initialize a transform
#' 
#' @param x the pre_processor
#' @param X the data matrix
#' @keywords internal
#' @return an initialized pre-processor
#' @export
init_transform <- function(x, X, ...) UseMethod("init_transform")



#' apply a pre-processing transform
#' 
#' @inheritParams init_transform
#' @param colind column indices
#' @param ... extra args
#' @export
#' @return the transformed data
apply_transform <- function(x, X, colind, ...) UseMethod("apply_transform")


#' reverse a pre-processing transform
#' 
#' @inheritParams init_transform
#' @param colind column indices
#' @param ... extra args
#' @return the reverse-transformed data
#' @export
reverse_transform <- function(x, X, colind, ...) UseMethod("reverse_transform")



#' Bootstrap Resampling for Multivariate Models
#'
#' Perform bootstrap resampling on a multivariate model to estimate the variability of components and scores.
#'
#' @param x A fitted model object, such as a `projector`, that has been fit to a training dataset.
#' @param nboot An integer specifying the number of bootstrap resamples to perform.
#' @param ... Additional arguments to be passed to the specific model implementation of `bootstrap`.
#' @return A list containing the bootstrap resampled components and scores for the model.
#' @export
bootstrap <- function(x, nboot, ...) UseMethod("bootstrap")


#' Construct a Classifier
#'
#' Create a classifier from a given model object (e.g., `projector`). This classifier can generate predictions for new data points.
#'
#' @param x A model object, such as a `projector`, that has been fit to a training dataset.
#' @param colind Optional vector of column indices used for prediction. If not provided, all columns will be used.
#' @param ... Additional arguments to be passed to the specific model implementation of `classifier`.
#' @return A classifier function that can be used to make predictions on new data points.
#' @export
classifier <- function(x, colind, ...) UseMethod("classifier")


#' construct a random forest wrapper classifier 
#' 
#' Given a model object (e.g. `projector` construct a random forest classifier that can generate predictions for new data points.
#' 
#' @param x the model object
#' @param colind the (optional) column indices used for prediction
#' @param ... extra arguments to `randomForest` function
#' @return a random forest classifier
#' @export
rf_classifier <- function(x, colind, ...) UseMethod("rf_classifier")


#' Permutation Confidence Intervals
#'
#' Estimate confidence intervals for model parameters using permutation testing.
#'
#' @param x A model fit object.
#' @param X The original data matrix used to fit the model.
#' @param nperm The number of permutations to perform for the confidence interval estimation.
#' @param ... Additional arguments to be passed to the specific model implementation of `perm_ci`.
#' @return A list containing the estimated lower and upper bounds of the confidence intervals for model parameters.
#' @export
perm_ci <- function(x, X, nperm, ...) UseMethod("perm_ci")



#' Rotate a Component Solution
#'
#' Perform a rotation of the component loadings to improve interpretability.
#'
#' @param x The model fit, typically a result from a dimensionality reduction method like PCA.
#' @param ncomp The number of components to rotate.
#' @param type The type of rotation to apply (e.g., "varimax", "quartimax", "promax").
#' @param ... extra args
#' @return A modified model fit with the rotated components.
#' @export
rotate <- function(x, ncomp, type, ...) UseMethod("rotate")



#' Apply rotation
#' 
#' Apply a specified rotation to the fitted model
#' 
#' @param x A model object, possibly created using the `pca()` function.
#' @param rotation_matrix \code{matrix} reprsenting the rotation.
#' @param ... extra args
#' @return A modified object with updated components and scores after applying the specified rotation.
#' @export
apply_rotation <- function(x, rotation_matrix, ...) { UseMethod("apply_rotation") }


#' Evaluate feature importance
#' 
#' Calculate the importance of features in a model
#' 
#' @param x the model fit
#' @param ... extra args
#' @return the feature importance scores
#' @export
feature_importance <- function(x, ...) UseMethod("feature_importance")



#' Generic Permutation-Based Test
#'
#' This generic function implements a permutation-based test to assess the significance
#' of components or statistics in a fitted model. The actual procedure depends on
#' the method defined for the specific model class. Typical usage:
#'
#' \enumerate{
#'   \item Shuffle or permute the data in a way that breaks the structure of interest
#'         (e.g., shuffle labels for supervised methods, shuffle columns/rows for unsupervised).
#'   \item Re-fit or re-project the model on the permuted data. Depending on the class,
#'         this can be done via a \code{fit_fun} or a class-specific approach.
#'   \item Measure the statistic of interest (e.g., variance explained, classification accuracy, canonical correlation).
#'   \item Compare the distribution of permuted statistics to the observed statistic to compute an empirical p-value.
#' }
#'
#' S3 methods define the specific defaults and required signatures for the functions
#' involved in shuffling, fitting, and measuring.
#'
#' @name perm_test
#' @aliases perm_test perm_test.pca perm_test.cross_projector perm_test.discriminant_projector perm_test.multiblock_biprojector
#'
#' @usage perm_test(x, ...)
#'
#' @param x A fitted model object (e.g. \code{pca}, \code{cross_projector}, \code{discriminant_projector}, \code{multiblock_biprojector}).
#' @param X (Used by \code{pca}, \code{cross_projector}, \code{discriminant_projector}) The original primary data matrix used to fit \code{x}. Ignored by the \code{multiblock_biprojector} method.
#' @param Y (Used by \code{cross_projector}) The secondary data block (n x pY). Ignored by other methods.
#' @param Xlist (Used by \code{multiblock_biprojector} \[optional, default \code{NULL}\] and \code{multiblock_projector} \[required\]) List of data blocks.
#' @param nperm Integer number of permutations (Default: 1000 for PCA, 500 for multiblock methods, 100 otherwise).
#' @param measure_fun (Optional; Used by \code{pca}, \code{cross_projector}, \code{discriminant_projector}, \code{multiblock_projector}) A function for computing the statistic(s) of interest. Ignored by \code{multiblock_biprojector}. Signature/default varies by method (see Details).
#' @param shuffle_fun (Optional; Used by all methods) A function for permuting the data appropriately. Signature/default varies by method (see Details).
#' @param fit_fun (Optional; Used by \code{cross_projector}, \code{discriminant_projector}) A function for re-fitting a new model. Ignored by PCA and multiblock methods. Signature/default varies by method (see Details).
#' @param stepwise (Used by \code{pca}) Logical indicating if sequential testing (P3 projection) should be performed. Default \code{TRUE}. (The multiblock methods also perform sequential testing based on \code{alpha} and \code{comps}, but this argument is ignored). Ignored by other methods.
#' @param parallel (Used by all methods) Logical; if `TRUE`, attempt parallel execution via `future.apply::future_lapply`.
#' @param alternative (Used by all methods) Character string for the alternative hypothesis: "greater" (default), "less", or "two.sided".
#' @param alpha (Used by \code{pca}, \code{multiblock_biprojector}, \code{multiblock_projector}) Significance level for sequential stopping rule (default 0.05). Passed directly as a named argument to these methods.
#' @param comps (Used by \code{pca}, \code{multiblock_biprojector}, \code{multiblock_projector}) Maximum number of components to test sequentially (default 4). Passed directly as a named argument to these methods.
#' @param use_svd_solver (Used by \code{pca}) Optional string specifying the SVD solver (default "fast").
#' @param use_rspectra (Used by \code{multiblock_biprojector}) Logical indicating whether to use RSpectra for eigenvalue calculation (default \code{TRUE}). Passed directly as a named argument.
#' @param predict_method (Used by \code{discriminant_projector}) Prediction method (`"lda"` or `"euclid"`) used by the default measure function (default "lda").
#' @param ... Additional arguments passed down to `shuffle_fun` or `measure_fun` (if applicable).\n#'   Note: For \code{multiblock} methods, \code{Xlist}, \code{comps}, \code{alpha}, and \code{use_rspectra} (for biprojector) are handled as direct named arguments, not via \code{...}.
#'
#' @details
#' This function provides a framework for permutation testing in various multivariate models.
#' The specific implementation details, default functions, and relevant arguments vary by method.
#'
#' \strong{PCA Method (`perm_test.pca`):} 
#' Relevant arguments: \code{X}, \code{nperm}, \code{measure_fun}, \code{shuffle_fun}, \code{stepwise}, \code{parallel}, \code{alternative}, \code{alpha}, \code{comps}, \code{use_svd_solver}, \code{...}. Assesses significance of variance explained by each PC (Vitale et al., 2017). Default statistic: F_a. Default shuffle: column-wise. Default uses P3 projection and sequential stopping with \code{alpha}.
#'
#' \strong{Cross Projector Method (`perm_test.cross_projector`):} 
#' Relevant arguments: \code{X}, \code{Y}, \code{nperm}, \code{measure_fun}, \code{shuffle_fun}, \code{fit_fun}, \code{parallel}, \code{alternative}, \code{...}. Tests the X-Y relationship. Default statistic: `x2y.mse`. Default shuffle: rows of Y. Default fit: `stats::cancor`.
#'
#' \strong{Discriminant Projector Method (`perm_test.discriminant_projector`):} 
#' Relevant arguments: \code{X}, \code{nperm}, \code{measure_fun}, \code{shuffle_fun}, \code{fit_fun}, \code{predict_method}, \code{parallel}, \code{alternative}, \code{...}. Tests class separation. Default statistic: prediction accuracy. Default shuffle: labels. Default fit: `MASS::lda`.
#'
#' \strong{Multiblock Bi-Projector Method (`perm_test.multiblock_biprojector`):} 
#' Relevant arguments: \code{Xlist} (optional), \code{nperm}, \code{shuffle_fun}, \code{parallel}, \code{alternative}, \code{alpha}, \code{comps}, \code{use_rspectra}, \code{...}. Tests consensus using fixed internal statistic (eigenvalue) on scores for each component. The statistic is the leading eigenvalue of the covariance matrix of block scores for a given component (T^T, where T columns are scores of block \emph{b} on component \emph{k}). By default, it shuffles rows within each block independently (either from \code{Xlist} if provided via \code{...}, or using the internally stored scores). It performs sequential testing for components specified by \code{comps} using the stopping rule defined by \code{alpha} (both passed via \code{...}).
#'
#' \strong{Multiblock Projector Method (`perm_test.multiblock_projector`):} 
#' Relevant arguments: \code{Xlist} (required), \code{nperm}, \code{measure_fun}, \code{shuffle_fun}, \code{parallel}, \code{alternative}, \code{alpha}, \code{comps}, \code{...}. Tests consensus using \code{measure_fun} (default: mean abs corr) on scores projected from \code{Xlist} using the original model \code{x}. Does not refit.
#'
#' @return
#' The structure of the return value depends on the method:
#' \describe{
#'   \item{\strong{`cross_projector`} and \strong{`discriminant_projector`}:}{
#'     Returns an object of class \code{perm_test}, a list containing: \code{statistic}, \code{perm_values}, \code{p.value}, \code{alternative}, \code{method}, \code{nperm}, \code{call}.}
#'   \item{\strong{`pca`}, \strong{`multiblock_biprojector`}, and \strong{`multiblock_projector`}:}{
#'     Returns an object inheriting from \code{perm_test} (classes \code{perm_test_pca}, \code{perm_test_multiblock}, or \code{perm_test} respectively for multiblock_projector), \n
#'     a list containing: \code{component_results} (data frame with observed stat, pval, CIs per component), \code{perm_values} (matrix of permuted stats), \code{alpha} (if applicable), \code{alternative}, \code{method}, \code{nperm} (vector of successful permutations per component), \code{call}.}
#' }
#'
#' @references
#' Buja, A., & Eyuboglu, N. (1992). Remarks on parallel analysis. *Multivariate Behavioral Research*, 27(4), 509-540. (Relevant for PCA permutation concepts)
#'
#' Vitale, R., Westerhuis, J. A., Næs, T., Smilde, A. K., de Noord, O. E., & Ferrer, A. (2017).
#' Selecting the number of factors in principal component analysis by permutation testing—
#' Numerical and practical aspects. *Journal of Chemometrics*, 31(10), e2937.
#' \doi{10.1002/cem.2937} (Specific to `perm_test.pca`)
#'
#' @seealso \code{\link{pca}}, \code{\link{cross_projector}}, \code{\link{discriminant_projector}},
#'   \code{\link{multiblock_biprojector}},
#'   \code{\link{measure_interblock_transfer_error}}
#' @family perm_test
#'
#' @examples
#' # PCA Example
#' data(iris)
#' X_iris <- as.matrix(iris[,1:4])
#' mod_pca <- pca(X_iris, ncomp=4, preproc=center()) # Ensure centering
#'
#' # Test first 3 components sequentially (faster with more nperm)
#' # Ensure a future plan is set for parallel=TRUE, e.g., future::plan("multisession")
#' res_pca <- perm_test(mod_pca, X_iris, nperm=50, comps=3, parallel=FALSE)
#' print(res_pca)
#'
#' # PCA Example with row shuffling (tests different null hypothesis)
#' row_shuffle <- function(dat, ...) dat[sample(nrow(dat)), ]
#' res_pca_row <- perm_test(mod_pca, X_iris, nperm=50, comps=3,
#'                          shuffle_fun=row_shuffle, parallel=FALSE)
#' print(res_pca_row)
#'
#' \dontrun{
#' # Cross Projector Example (using cancor)
#' X <- as.matrix(iris[,1:2])
#' Y <- as.matrix(iris[,3:4])
#' ccr <- cancor(X, Y)
#' mod_cp <- cross_projector(ccr$xcoef, ccr$ycoef)
#'
#' # Perm test (is x2y.mse lower than chance?)
#' res_cp <- perm_test(mod_cp, X, Y=Y, nperm=50, alternative="less")
#' print(res_cp)
#'
#' # Discriminant Projector Example (using LDA)
#' library(MASS)
#' lda_fit <- lda(X_iris, grouping=iris$Species)
#' mod_dp <- discriminant_projector(
#'   v = lda_fit$scaling,
#'   s = X_iris %*% lda_fit$scaling,
#'   sdev = lda_fit$svd,
#'   labels = iris$Species,
#'   preproc = prep(center()), # Assuming center() was intended for LDA
#'   Sigma = lda_fit$covariance # Needed for LDA prediction method
#' )
#'
#' # Perm test (is accuracy higher than chance?)
#' res_dp <- perm_test(mod_dp, X_iris, nperm=50, alternative="greater")
#' print(res_dp)
#'
#' # Multiblock Bi-Projector Example
#' # (Requires a multiblock model 'mod_mb' from e.g. MFA or ComDim)
#' # Assuming 'mod_mb' exists and has 2 blocks:
#' # res_mb <- perm_test(mod_mb, nperm=50, comps=3) 
#' # print(res_mb)
#' # Example using provided Xlist (list of matrices X1, X2):
#' # X1 <- matrix(rnorm(50*10), 50, 10)
#' # X2 <- matrix(rnorm(50*15), 50, 15)
#' # Assume mod_mb was fit on cbind(X1, X2) with block_indices=list(1:10, 11:25)
#' # res_mb_xlist <- perm_test(mod_mb, Xlist=list(X1, X2), nperm=50, comps=3)
#' # print(res_mb_xlist)
#' }
#' @export
perm_test <- function(x, ...) {
  UseMethod("perm_test")
}



#' Screeplot for PCA
#'
#' Displays the variance explained by each principal component as a bar or line plot.
#' 
#' @param x A \code{pca} object.
#' @param ... extra args
#' @export
screeplot <- function(x, ...) UseMethod("screeplot")


#' Cross-validation Framework
#'
#' Generic function for performing cross-validation on various objects or data.
#' Specific methods should be implemented for different data types or model types.
#'
#' @param x The object to perform cross-validation on (e.g., data matrix, formula, model object).
#' @param folds A list defining the cross-validation folds, typically containing `train` and `test` indices for each fold.
#' @param ... Additional arguments passed to specific methods.
#'
#' @return The structure of the return value depends on the specific S3 method.
#'   Typically, it will be an object containing the results of the cross-validation,
#'   such as performance metrics per fold or aggregated metrics.
#'
#' @details
#' The specific implementation details, default functions, and relevant arguments vary by method.
#'
#' \strong{Bi-Projector Method (`cv.bi_projector`):}
#' Relevant arguments: \code{x}, \code{folds}, \code{max_comp}, \code{fit_fun},
#'   \code{measure}, \code{measure_fun}, \code{return_models}, \code{...}.
#'
#' This method performs cross-validation specifically for \code{bi_projector} models
#' (or models intended to be used like them, typically from unsupervised methods
#' like PCA or SVD). For each fold, it fits a single model using the training data
#' with the maximum number of components specified (\code{max_comp}). It then iterates
#' from 1 to \code{max_comp} components:
#' \enumerate{
#'   \item It truncates the full model to \code{k} components using \code{truncate()}. 
#'        (Requires a \code{truncate} method for the fitted model class).
#'   \item It reconstructs the held-out test data using the k-component truncated model
#'        via \code{reconstruct_new()}.
#'   \item It calculates reconstruction performance metrics (e.g., MSE, R2) by comparing
#'        the original test data to the reconstruction using the \code{measure} argument
#'        or a custom \code{measure_fun}.
#' }
#' The \code{fit_fun} must accept an argument \code{ncomp}. Additional arguments in \code{...}
#' are passed to \code{fit_fun} and \code{measure_fun}.
#'
#' The return value is a \code{cv_fit} object (a list with class `cv_fit`), where the
#' \code{$results} element is a tibble. Each row corresponds to a fold, containing
#' the fold index (\code{fold}) and a nested tibble (\code{component_metrics}).
#' The \code{component_metrics} tibble has rows for each component evaluated (1 to
#' \code{max_comp}) and columns for the component index (\code{comp}) plus all
#' calculated metrics (e.g., \code{mse}, \code{r2}, \code{mae}) or error messages
#' (\code{comp_error}). If \code{return_models=TRUE}, the full model fitted on the training
#' data for each fold is included in a list column \code{model_full}.
#'
#' @export
#' @family cv
#' @seealso \code{\link{cv_generic}}, \code{\link{summary.cv_fit}}, \code{\link{plot.cv_fit}}
cv <- function(x, folds, ...) {
  UseMethod("cv")
}


#' Identify Original Variables Used by a Projector
#' 
#' Determines which columns from the *original* input space contribute 
#' (have non-zero influence) to *any* of the output components of the projector.
#' 
#' @param x A projector object (e.g., `projector`, `composed_projector`).
#' @param ... Additional arguments passed to specific methods.
#' 
#' @return A sorted numeric vector of unique indices corresponding to the original input variables.
#' @export
variables_used <- function(x, ...) {
    UseMethod("variables_used")
}

#' Identify Original Variables for a Specific Component
#' 
#' Determines which columns from the *original* input space contribute 
#' (have non-zero influence) to a *specific* output component of the projector.
#' 
#' @param x A projector object (e.g., `projector`, `composed_projector`).
#' @param k The index of the output component to query.
#' @param ... Additional arguments passed to specific methods.
#' 
#' @return A sorted numeric vector of unique indices corresponding to the original input variables.
#' @export
vars_for_component <- function(x, k, ...) {
    UseMethod("vars_for_component")
}
</file>

<file path="R/classifier.R">
#' Multiblock Bi-Projector Classifier
#'
#' Constructs a k-Nearest Neighbors (k-NN) classifier based on a fitted
#' `multiblock_biprojector` model object. The classifier uses the projected scores
#' as the feature space for k-NN.
#'
#' Users can specify whether to use the globally projected scores stored within the model
#' (`global_scores = TRUE`) or to generate reference scores by projecting provided `new_data`
#' (`global_scores = FALSE`). Partial projections based on `colind` or `block` can be used
#' when `global_scores = FALSE` or when `new_data` is provided alongside `colind`/`block`.
#' Prediction behavior is further controlled by arguments passed to `predict.classifier`.
#'
#' @param x A fitted `multiblock_biprojector` object.
#' @param colind An optional numeric vector specifying column indices from the original data space.
#'   If provided when `global_scores=FALSE`, these indices are used to perform a partial projection for the reference scores.
#'   If provided when `global_scores=TRUE`, this value is stored but does not affect the
#'   reference scores (which remain global); however, it may influence the default projection
#'   behavior during prediction unless overridden there. See `predict.classifier`.
#' @param labels A factor or vector of class labels for the training data.
#' @param new_data An optional data matrix used to generate reference scores when `global_scores=FALSE`,
#'   or when `global_scores=TRUE` but `colind` or `block` is also provided (overriding `global_scores`).
#'   Must be provided if `global_scores=FALSE`.
#' @param block An optional integer specifying a predefined block index.
#'   Used for partial projection if `global_scores=FALSE` or if `new_data` is also provided.
#'   Cannot be used simultaneously with `colind`.
#' @param knn The integer number of nearest neighbors (k) for the k-NN algorithm (default: 1).
#' @param global_scores Logical. **DEPRECATED** This argument is deprecated and its behavior has changed.
#'   Reference scores are now determined automatically:
#'   - If `new_data` is NULL: Uses the globally projected scores stored in `x` (`scores(x)`).
#'   - If `new_data` is provided: Always projects `new_data` to generate reference scores
#'     (using `partial_project`/`project_block` if `colind`/`block` are given, `project` otherwise).
#' @param ... Additional arguments (currently ignored).
#' @return An object of class `multiblock_classifier`, which also inherits from `classifier`.
#' @export 
#' @family classifier
classifier.multiblock_biprojector <- function(x, colind=NULL, labels, new_data=NULL, 
                                              block=NULL, global_scores=TRUE, knn=1,...) {
  if (!is.null(colind)) {
    chk::chk_true(length(colind) <= shape(x)[1])
    chk::chk_true(all(colind>0))
    if (!is.null(block)) {
      rlang::abort("can either supply `colind` or `block` but not both")
    }
  }
  
  # Check knn
  if (!is.numeric(knn) || knn < 1 || knn != as.integer(knn)) {
    stop("knn must be a positive integer")
  }
  
  scores_ref <- NULL # reference scores
  
  # Fix 3: Revised logic for reference scores
  if (is.null(new_data)) {
    # Case 1: No new_data provided -> Use global scores from projector
    scores_ref <- scores(x)
    # Ensure labels match the number of global scores
    if (length(labels) != nrow(scores_ref)) {
      stop("Length of labels does not match the number of rows in scores(x). Provide new_data or ensure labels match existing scores.")
    }
  } else {
    # Case 2: new_data IS provided -> Always project new_data
    # Ensure labels match the provided new_data
    chk::chk_equal(length(labels), nrow(new_data))
    
    # Project new_data to get reference scores.
    # Use partial projection if colind/block are given, otherwise global.
    if (!is.null(colind)) {
      scores_ref <- partial_project(x, new_data, colind = colind)
    } else if (!is.null(block)) {
      chk::chk_whole_number(block)
      scores_ref <- project_block(x, new_data, block = block)
    } else {
      scores_ref <- project(x, new_data)
    }
  }
  
  # Deprecate global_scores argument (no longer used in logic)
  if (!missing(global_scores) && !is.null(global_scores)) {
     warning("'global_scores' argument is deprecated and ignored. Reference scores are now determined automatically based on whether 'new_data' is provided.")
  }

  new_classifier(x, labels=labels, scores=scores_ref, colind=colind, block=block, knn=knn, # global_scores=global_scores,
                 classes="multiblock_classifier")
}


#' Create a k-NN classifier for a discriminant projector
#'
#' @param x the discriminant projector object
#' @param colind an optional vector specifying the column indices of the components
#' @param knn the number of nearest neighbors (default=1)
#' @param ... extra arguments
#' @return a classifier object
#' @export
#' @examples
#' # Assume dp is a fitted discriminant_projector object
#' # classifier(dp, knn = 5) # Basic example
classifier.discriminant_projector <- function(x, colind=NULL, knn=1,...) {
  if (!is.null(colind)) {
    chk::chk_true(length(colind) <= shape(x)[1])
    chk::chk_true(all(colind>0))
  }
  if (!is.numeric(knn) || knn < 1 || knn != as.integer(knn)) {
    stop("knn must be a positive integer")
  }
  
  new_classifier(x, x$labels, scores(x), colind=colind, knn=knn)
}


#' Create a new k-NN classifier
#'
#' @param x the model fit
#' @param labels class labels
#' @param scores scores used for classification
#' @param colind optional component indices
#' @param knn number of nearest neighbors
#' @param classes additional S3 classes
#' @param ... extra args
#' @keywords internal
#' @noRd
new_classifier <- function(x, labels, scores, colind=NULL, knn=1, classes=NULL, ...) {
  if (!is.null(colind)) {
    chk::chk_true(length(colind) <= shape(x)[1])
    chk::chk_true(all(colind>0))
  }
  
  if (!is.numeric(knn) || knn < 1 || knn != as.integer(knn)) {
    stop("knn must be a positive integer")
  }
  
  if (knn > nrow(scores)) { # Check against reference scores
    stop("knn cannot exceed the number of training cases in scores")
  }
  
  # C1: Coerce labels to factor
  if (!is.factor(labels)) {
      labels <- factor(labels)
  }
  
  chk::chk_equal(length(labels), nrow(scores))
  
  structure(
    list(
      projector=x,
      labels=labels,
      scores=scores,
      colind=colind,
      knn=knn,
      ...), # Store extra args passed to constructor
    class=c(classes, "classifier")
  )
}


#' Create a random forest classifier
#' 
#' Uses `randomForest` to train a random forest on the provided scores and labels.
#'
#' @param x a projector object
#' @param colind optional col indices
#' @param labels class labels
#' @param scores reference scores
#' @param ... passed to `randomForest`
#' @export
#' @return a `rf_classifier` object with rfres (rf model), labels, scores
#' @family classifier
#' @seealso \code{\link[randomForest]{randomForest}}
#' @examples
#' # Assume proj is a fitted projector object
#' # Assume lbls are labels and sc are scores
#' # if (requireNamespace("randomForest", quietly = TRUE)) {
#' #   rf_classifier(proj, labels = lbls, scores = sc)
#' # }
rf_classifier.projector <- function(x, colind=NULL, labels, scores, ...) {
  if (!requireNamespace("randomForest", quietly = TRUE)) {
    stop("Please install package 'randomForest' for 'rf_classifier'")
  }
  
  if (!is.null(colind)) {
    chk::chk_true(length(colind) <= shape(x)[1])
    chk::chk_true(all(colind>0))
  }
  
  # C1: Coerce labels to factor (if not already factor)
  if (!is.factor(labels)) {
    labels <- factor(labels)
  }
  
  chk::chk_equal(length(labels), nrow(scores))
  
  # C4: Convert scores matrix to data.frame for randomForest
  scores_df <- as.data.frame(scores, stringsAsFactors = FALSE)
  if (!is.null(colnames(scores))) {
    colnames(scores_df) <- make.names(colnames(scores)) # Ensure valid names
  } else {
    colnames(scores_df) <- paste0("Comp", 1:ncol(scores_df))
  }
  
  rfres <- randomForest::randomForest(scores_df, labels, ...) # Pass ... here
  
  # Store rf variable importance if needed
  imp <- NULL
  if ("importance" %in% names(rfres)) {
    imp <- rfres$importance
  }
  
  structure(
    list(
      projector=x,
      rfres=rfres,
      labels=labels,
      scores=scores,
      importance=imp,
      colind=colind
      # Do not pass ... here unless intended for storage
      ),
    class=c("rf_classifier", "classifier")
  )
}


#' create classifier from a projector
#' 
#' @param x projector
#' @param colind ...
#' @param labels ...
#' @param new_data ...
#' @param knn ...
#' @param global_scores ...
#' @param ... extra args
#' @method classifier projector
#' @family classifier
#' @examples
#' # Assume proj is a fitted projector object
#' # Assume lbls are labels and dat is new data
#' # classifier(proj, labels = lbls, new_data = dat, knn = 3)
classifier.projector <- function(x, colind=NULL, labels, new_data=NULL, knn=1, global_scores=TRUE, ...) {
  # Deprecate global_scores argument
  if (!missing(global_scores) && !is.null(global_scores)) {
     warning("'global_scores' argument is deprecated and ignored. Reference scores are now determined automatically based on whether 'new_data' is provided.")
  }
  
  if (!is.null(colind)) {
    chk::chk_true(length(colind) <= shape(x)[1])
    chk::chk_true(all(colind>0))
  }
  
  if (!is.numeric(knn) || knn < 1 || knn != as.integer(knn)) {
    stop("knn must be a positive integer")
  }
  
  # Fix 3: Revised logic for reference scores
  scores_ref <- NULL
  if (is.null(new_data)) {
    # Case 1: No new_data -> Use global scores from projector if they exist
    if (!is.null(scores(x))) {
        scores_ref <- scores(x)
        # Ensure labels match global scores
        chk::chk_equal(length(labels), nrow(scores_ref))
    } else {
        stop("If new_data is NULL, the projector 'x' must contain scores (e.g., be a bi_projector). Alternatively, provide new_data.")
    }
  } else {
    # Case 2: new_data provided -> Always project new_data
    chk::chk_equal(length(labels), nrow(new_data))
    if (!is.null(colind)) {
      # Use partial projection if colind is given
      scores_ref <- partial_project(x, new_data, colind = colind)
    } else {
      # Use global projection otherwise
      scores_ref <- project(x, new_data)
    }
  }
  
  new_classifier(x, labels, scores_ref, colind, knn, ...)
}


#' Calculate Rank Score for Predictions
#'
#' Computes the rank score (normalized rank of the true class probability) for each observation.
#' Lower rank scores indicate better predictions (true class has higher probability).
#'
#' @param prob Numeric matrix of predicted probabilities (observations x classes).
#'   Column names must correspond to class labels.
#' @param observed Factor or vector of observed class labels. Must be present in `colnames(prob)`.
#' @return A `data.frame` with columns `prank` (the normalized rank score) and `observed` (the input labels).
#' @export
#' @family classifier evaluation
#' @examples
#' probs <- matrix(c(0.1, 0.9, 0.8, 0.2), 2, 2, byrow=TRUE,
#'                dimnames = list(NULL, c("A", "B")))
#' obs <- factor(c("B", "A"))
#' rank_score(probs, obs)
rank_score <- function(prob, observed) {
  pnames <- colnames(prob)
  # B3: Check if pnames is NULL (can happen if prob matrix lacks colnames)
  if (is.null(pnames)) {
      stop("Input probability matrix 'prob' must have column names corresponding to class labels.")
  }
  chk::chk_true(all(observed %in% pnames))

  # B2: Use matrixStats::rowRanks for efficiency
  # Ensure matrixStats is suggested/imported
  if (!requireNamespace("matrixStats", quietly = TRUE)) {
    stop("Please install package 'matrixStats' for rank_score")
  }
  # Note: matrixStats::rowRanks ranks *lowest* value as 1 by default.
  # We want *highest* probability to have rank 1, so use negative probabilities.
  # ties.method = "random" matches the original apply logic.
  prank_mat <- matrixStats::rowRanks(-prob, ties.method = "random")

  # Normalize ranks to be within (0, 1)
  # Original code: rp / (length(rp) + 1) -> ncol(prob) + 1
  ncols <- ncol(prob)
  prank_mat_norm <- prank_mat / (ncols + 1)

  mids <- match(observed, pnames)
  # Efficiently extract the rank for the observed class for each row
  # Use matrix indexing: rows are 1:n, columns are the matched indices
  pp <- prank_mat_norm[cbind(seq_along(observed), mids)]

  data.frame(prank = pp, observed = observed)
}


#' top-k accuracy indicator
#' 
#' Determines if the true class label is among the top `k` predicted probabilities for each observation.
#'
#' @param prob Numeric matrix of predicted probabilities (observations x classes).
#'   Column names must correspond to class labels.
#' @param observed Factor or vector of observed class labels. Must be present in `colnames(prob)`.
#' @param k Integer; the number of top probabilities to consider.
#' @return A `data.frame` with columns `topk` (logical indicator: `TRUE` if observed class is in top-k) and `observed`.
#' @export
#' @family classifier evaluation
#' @examples
#' probs <- matrix(c(0.1, 0.9, 0.8, 0.2, 0.3, 0.7), 3, 2, byrow=TRUE,
#'                 dimnames = list(NULL, c("A", "B")))
#' obs <- factor(c("B", "A", "B"))
#' topk(probs, obs, k=1)
#' topk(probs, obs, k=2)
topk <- function(prob, observed, k) {
  pnames <- colnames(prob)
  # B3: Check for NULL colnames
  if (is.null(pnames)) {
      stop("Input probability matrix 'prob' must have column names corresponding to class labels.")
  }
  chk::chk_true(all(observed %in% pnames))
  chk::chk_whole_number(k)
  chk::chk_gt(k, 0)
  chk::chk_lte(k, ncol(prob))

  # B2: Vectorized approach
  # Get indices of top k probabilities for each row
  # Use Rfast::rowOrder or base order within apply (more compatible)
  # apply is likely fine here unless this is extremely performance critical
  # Use base 'order' with decreasing=TRUE
  topk_indices <- t(apply(prob, 1, order, decreasing = TRUE))[, 1:k, drop = FALSE]

  observed_indices <- match(observed, pnames)

  # Check if observed index is in the top k for each row
  # Vectorized check using row/column indexing with any()
  # We create a matrix comparing each observed_index to the rows of topk_indices
  topk_result <- apply(topk_indices == observed_indices, 1, any)

  data.frame(topk = topk_result, observed = observed)
}


#' Normalize rows of a probability matrix to sum to 1
#' @keywords internal
#' @noRd
normalize_probs <- function(p) {
  # Normalize rows to sum to 1
  # Avoid division by zero for rows summing to 0
  rs <- rowSums(p, na.rm = TRUE)
  
  # Handle rows where all values are zero or negative
  zero_rows <- rs <= 0
  if (any(zero_rows)) {
      warning("Some rows in probability matrix had zero or negative sum; converting to uniform probability.")
      # Make truly zero rows uniform probability 1/ncol
      p[zero_rows,] <- 1
      rs[zero_rows] <- ncol(p) # Will lead to 1/ncol below
  }
  # Use pmax to avoid division by zero
  sweep(p, 1, pmax(rs, .Machine$double.eps), "/")
}

#' Calculate Average Probabilities per Class from Similarity Matrix
#'
#' This function calculates the average similarity (or inverse distance) of test samples
#' to each training class, based on a similarity/distance matrix between test and training samples.
#'
#' @param sim_mat A matrix of similarities (or inverse distances), with training samples as rows
#'                and test samples as columns (n_train x n_test).
#' @param train_labels A factor vector of labels corresponding to the rows (training samples) of `sim_mat`.
#' @return A matrix of average probabilities (n_test x n_classes), where rows sum to 1.
#' @keywords internal
#' @noRd
avg_probs <- function(sim_mat, train_labels) {
  # Ensure train_labels is a factor
  if (!is.factor(train_labels)) train_labels <- factor(train_labels)
  all_levels <- levels(train_labels)
  n_classes <- length(all_levels)
  
  # Use rowsum to sum similarities *column-wise* for each class
  # We want (n_classes x n_test) matrix where entry (c, j) is sum of sims between class c and test sample j
  summed_sims_by_class <- rowsum(sim_mat, train_labels, reorder = FALSE, na.rm = TRUE)
  
  # Count occurrences of each label in the training set
  label_counts <- table(train_labels)
  
  # Divide summed similarities by counts to get averages
  # Ensure alignment and handle classes with 0 counts (though unlikely if they are in levels)
  # Need to align counts to the row names of summed_sims_by_class (which are the class levels)
  avg_sims <- summed_sims_by_class / as.numeric(label_counts[rownames(summed_sims_by_class)])
  
  # Transpose to get (n_test x n_classes)
  avg_sims_t <- t(avg_sims)
  
  # Ensure all original classes are present as columns, even if they had 0 similarity sums
  if (ncol(avg_sims_t) != n_classes) {
      missing_cols <- setdiff(all_levels, colnames(avg_sims_t))
      if (length(missing_cols) > 0) {
          add_mat <- matrix(0, nrow = nrow(avg_sims_t), ncol = length(missing_cols),
                            dimnames = list(rownames(avg_sims_t), missing_cols))
          avg_sims_t <- cbind(avg_sims_t, add_mat)
      }
      # Reorder columns to match original levels
      avg_sims_t <- avg_sims_t[, all_levels, drop = FALSE]
  }
  
  # Normalize rows (test samples) to sum to 1 to get probabilities
  normalize_probs(avg_sims_t) # Reuse the improved normalize_probs
}

#' Find Nearest Class(es) based on Similarity/Distance
#'
#' Determines the predicted class for each test sample based on the k-nearest neighbors
#' in the training set using a precomputed similarity or distance matrix.
#'
#' @param sim_mat A matrix of similarities (higher is better) or inverse distances,
#'                with training samples as rows and test samples as columns (n_train x n_test).
#' @param train_labels A factor vector of labels corresponding to the rows (training samples) of `sim_mat`.
#' @param knn The number of nearest neighbors to consider.
#' @param higher_is_better Logical indicating if higher values in `sim_mat` mean closer/better.
#' @return A factor vector of predicted class labels for the test samples.
#' @keywords internal
#' @noRd
nearest_class <- function(sim_mat, train_labels, knn=1, higher_is_better = TRUE) {
  # Ensure labels is factor
  if(!is.factor(train_labels)) train_labels <- factor(train_labels)
  all_levels <- levels(train_labels)
  
  # Apply per column (test sample)
  predicted_labels <- apply(sim_mat, 2, function(scores_for_one_test_sample) {
    # Find the indices of the top knn training samples
    neighbor_indices <- order(scores_for_one_test_sample, decreasing = higher_is_better)[1:knn]
    
    # Get the labels of these neighbors
    neighbor_labels <- train_labels[neighbor_indices]
    
    # Find the most frequent label among neighbors (majority vote)
    # Use table and which.max, handling ties by picking the first alphabetically
    tab <- table(factor(neighbor_labels, levels=all_levels)) # Ensure all levels present
    if (length(tab) == 0) return(NA) # Should not happen if knn >= 1
    # Handle ties explicitly: which.max returns the first max
    names(which.max(tab))
  })
  
  # Return as factor with all original levels
  factor(predicted_labels, levels = all_levels)
}

#' @export
project.classifier <- function(x, new_data, ...) {
  # Retrieve potentially stored projector arguments
  proj_args <- list(...)
  if (!is.null(x$projector_args)) { # Assuming constructor stores relevant args
      proj_args <- utils::modifyList(x$projector_args, proj_args)
  }
  
  # Use colind stored in classifier if not overridden in ...
  use_colind <- proj_args$colind
  if (is.null(use_colind)) {
      use_colind <- x$colind
  }
  
  # Project using appropriate method based on colind
  scores <- if (!is.null(use_colind)) {
    rlang::exec(partial_project, x$projector, new_data = new_data, colind = use_colind, !!!proj_args)
  } else {
    rlang::exec(project, x$projector, new_data = new_data, !!!proj_args)
  }
  
  scores
}

#' @noRd
prepare_predict <- function(object, colind=NULL, ncomp=NULL, new_data,...) {
  # Determine the colind to use for projection
  use_colind <- colind
  if (is.null(use_colind)) {
    use_colind <- object$colind # Default to colind stored in classifier
  }
  
  # C2 & C3: Robust handling of new_data dimensions and type
  expected_cols_original <- shape(object$projector)[1] # Expect cols matching original data space
  if (!is.null(use_colind)) {
      # If colind is used, new_data should have columns matching the length of colind
      expected_cols_subset <- length(use_colind)
  } else {
      expected_cols_subset <- expected_cols_original
  }

  if (is.vector(new_data)) {
    # Check length against expected columns (subset or full)
    chk::chk_equal(length(new_data), expected_cols_subset)
    # Reshape as a single row matrix
    new_data <- matrix(new_data, nrow = 1, ncol = expected_cols_subset)
  } else if (is.matrix(new_data) || is.data.frame(new_data)) {
      # Ensure matrix/data.frame input has correct columns for the projection type
      chk::chk_equal(ncol(new_data), expected_cols_subset)
  } else {
      stop("'new_data' must be a numeric vector, matrix, or data frame.")
  }
  
  # Determine number of components
  max_comps <- shape(object$projector)[2]
  if (is.null(ncomp)) {
    ncomp <- max_comps
  } else {
    chk::chk_whole_number(ncomp)
    chk::chk_range(ncomp, c(1, max_comps))
  }
  
  # Project the data
  proj <- if (!is.null(use_colind)) {
    partial_project(object$projector, new_data, colind = use_colind, ...) # Pass ...
  } else {
    project(object$projector, new_data, ...) # Pass ...
  }
  
  # Return list with projected data and determined parameters
  list(proj=proj, new_data=new_data, colind=use_colind, ncomp=ncomp)
}


#' Predict Class Labels using a Classifier Object
#'
#' Predicts class labels and probabilities for new data using a fitted `classifier` object.
#' It performs k-Nearest Neighbors (k-NN) classification in the projected component space.
#'
#' The function first projects the `new_data` into the component space defined by the
#' classifier's internal projector. If `colind` is specified, a partial projection using
#' only those features is performed. This projection is then compared to the reference scores
#' stored within the `classifier` object (`object$scores`) using the specified `metric`.
#' The k-NN algorithm identifies the `k` nearest reference samples (based on similarity or distance)
#' and predicts the class via majority vote. Probabilities are estimated based on the average
#' similarity/distance to each class among the neighbors or all reference points.
#'
#' @param object A fitted object of class `classifier`.
#' @param new_data A numeric matrix or vector of new observations to classify. Rows are observations,
#'   columns are variables matching the original data space used by the projector OR matching `colind` if provided.
#' @param ncomp Optional integer; the number of components to use from the projector for classification (default: all components used during classifier creation).
#' @param colind Optional numeric vector specifying column indices from the original data space.
#'   If provided, `new_data` is projected using only these features (`partial_project`). This overrides any
#'   `colind` stored default in the `object`. The resulting projection is compared against the
#'   reference scores (`object$scores`) stored in the classifier.
#' @param metric Character string specifying the similarity or distance metric for k-NN.
#'   Choices: "euclidean", "cosine", "ejaccard".
#' @param normalize_probs Logical; **DEPRECATED** Normalization behavior is now implicit in `prob_type="avg_similarity"`.
#' @param prob_type Character string; method for calculating probabilities:
#'   - "knn_proportion" (default): Calculates the proportion of each class among the `k` nearest neighbors.
#'   - "avg_similarity": Calculates average similarity to all training points per class (uses `avg_probs` helper).
#' @param ... Extra arguments passed down to projection methods (`project`, `partial_project`)
#'   or potentially to distance/similarity calculations (e.g., for `proxy::simil` if used with `ejaccard`).
#' @return A list containing:
#'   \item{class}{A factor vector of predicted class labels for `new_data`.} 
#'   \item{prob}{A numeric matrix (rows corresponding to `new_data`, columns to classes) of estimated class probabilities.}
#' @export
#' @family classifier predict
#' @seealso \code{\link{classifier.projector}}, \code{\link{classifier.multiblock_biprojector}}, \code{\link{partial_project}}
#' @examples
#' # Assume clf is a fitted classifier object (e.g., from classifier.projector)
#' # Assume new_dat is a matrix of new observations
#' # preds <- predict(clf, new_data = new_dat, metric = "cosine")
#' # print(preds$class)
#' # print(preds$prob)
predict.classifier <- function(object, new_data, ncomp=NULL,
                               colind=NULL, 
                               metric=c("euclidean", "cosine", "ejaccard"), 
                               normalize_probs=FALSE, # Deprecated
                               prob_type = c("knn_proportion", "avg_similarity"), 
                               ...) {
  
  metric <- match.arg(metric)
  prob_type <- match.arg(prob_type)
  
  # Deprecate normalize_probs
  if (!missing(normalize_probs) && normalize_probs) {
      warning("'normalize_probs' argument is deprecated and ignored. Normalization is implicit in prob_type='avg_similarity'.")
  }
  
  # Prepare data: project new_data, determine ncomp etc.
  prep <- prepare_predict(object, colind, ncomp, new_data,...) 
  proj_test <- prep$proj # Projected test data (n_test x d)
  ncomp <- prep$ncomp    # Number of components to use (d)
  
  # Reference scores (training data in projected space)
  scores_train <- as.matrix(object$scores) # (n_train x D_full)
  train <- scores_train[, 1:ncomp, drop=FALSE] # (n_train x d)
  test <- as.matrix(proj_test)[, 1:ncomp, drop=FALSE] # (n_test x d)
  
  # Training labels and levels
  train_labels <- object$labels
  if (!is.factor(train_labels)) train_labels <- factor(train_labels)
  all_levels <- levels(train_labels)
  n_classes <- length(all_levels)
  n_test <- nrow(test)
  knn <- object$knn
  
  # Compute similarities or distances (train vs test)
  # Resulting matrix `sim_dist_mat` will be (n_train x n_test)
  if (metric == "cosine") {
    sim_dist_mat <- proxy::simil(train, test, method="cosine")
    higher_is_better <- TRUE
  } else if (metric == "euclidean") {
    sim_dist_mat <- proxy::dist(train, test, method="euclidean")
    higher_is_better <- FALSE
  } else if (metric == "ejaccard") {
    sim_dist_mat <- proxy::simil(train, test, method="ejaccard")
    higher_is_better <- TRUE
  } else {
    # Should not happen due to match.arg
    stop("Unsupported metric")
  }
  sim_dist_mat <- as.matrix(sim_dist_mat)
  
  # --- Predict Classes (k-NN Majority Vote) ---
  predicted_classes <- nearest_class(sim_dist_mat, train_labels, knn, higher_is_better)

  # --- Calculate Probabilities --- 
  prob_mat <- matrix(0.0, nrow = n_test, ncol = n_classes,
                     dimnames = list(rownames(test), all_levels))

  if (prob_type == "knn_proportion") {
      # Calculate proportion of each class among k neighbors for each test sample
      for (j in 1:n_test) {
          scores_for_one_test_sample <- sim_dist_mat[, j]
          neighbor_indices <- order(scores_for_one_test_sample, decreasing = higher_is_better)[1:knn]
          neighbor_labels <- train_labels[neighbor_indices]
          # Use table to count classes among neighbors, ensuring all levels are included
          counts <- table(factor(neighbor_labels, levels = all_levels))
          prob_mat[j, ] <- counts / knn # Simple proportion
      }
  } else if (prob_type == "avg_similarity") {
      # Use the older avg_probs logic (average similarity to all training points per class)
      # Note: This doesn't directly use knn for probabilities
      if (!higher_is_better) {
          # Convert distance to similarity using 1 / (1 + D) for stability
          sim_mat_for_avg <- 1 / (1 + sim_dist_mat)
      } else {
          sim_mat_for_avg <- sim_dist_mat
          # Shift similarities to be non-negative if they were originally similarities
          # This prevents issues in avg_probs -> normalize_probs with negative sums
          min_sim <- min(sim_mat_for_avg, na.rm = TRUE)
          if (min_sim < 0) {
              sim_mat_for_avg <- sim_mat_for_avg - min_sim
          }
      }
      # Calculate average similarity per class and normalize
      # Ensure the matrix passed to avg_probs is (n_train x n_test)
      prob_mat <- avg_probs(sim_mat_for_avg, train_labels)
  } else {
      stop("Unsupported prob_type")
  }

  # Ensure prob_mat has correct dimensions and names
  if (nrow(prob_mat) != n_test || ncol(prob_mat) != n_classes) {
      stop("Internal error: Probability matrix has incorrect dimensions.")
  }
  if (!identical(colnames(prob_mat), all_levels)) {
       # Reorder columns if necessary (should be handled by avg_probs now)
       prob_mat <- prob_mat[, all_levels, drop = FALSE]
  }
  
  list(class = predicted_classes, prob = prob_mat)
}


#' Predict Class Labels using a Random Forest Classifier Object
#'
#' Predicts class labels and probabilities for new data using a fitted `rf_classifier` object.
#' This method projects the `new_data` into the component space and then uses the stored
#' `randomForest` model to predict outcomes.
#'
#' @inheritParams predict.classifier
#' @param object A fitted object of class `rf_classifier`.
#' @param ... Extra arguments passed to `predict.randomForest`.
#' @return A list containing:
#'   \item{class}{Predicted class labels (typically factor) from the random forest model.} 
#'   \item{prob}{A numeric matrix of predicted class probabilities from the random forest model.}
#' @family classifier predict
#' @seealso \code{\link{rf_classifier.projector}}, \code{\link[randomForest]{predict.randomForest}}
#' @export
predict.rf_classifier <- function(object, new_data, ncomp=NULL,
                                  colind=NULL, ...) {
  
  prep <- prepare_predict(object, colind, ncomp, new_data,...) 
  proj <- prep$proj[, 1:prep$ncomp, drop=FALSE] # Ensure correct number of components
  
  # Ensure proj is a data frame with same colnames as used during training if possible
  train_scores <- object$scores
  if (!is.null(colnames(train_scores))) {
    if (ncol(proj) == ncol(train_scores)) {
      colnames(proj) <- colnames(train_scores)
    } else if (ncol(proj) <= ncol(train_scores)) {
        # Use first ncomp names if names exist and dimensions match subset
         colnames(proj) <- colnames(train_scores)[1:ncol(proj)]
    } else {
      warning("Projection has more components than original scores used for naming.")
      colnames(proj) <- paste0("Comp", 1:ncol(proj)) # Fallback naming
    }
  } else {
       colnames(proj) <- paste0("Comp", 1:ncol(proj)) # Fallback naming if original scores lacked names
  }
  
  # Coerce proj to data.frame as expected by randomForest predict
  proj_df <- as.data.frame(proj)
  
  # Ensure randomForest package is available
  if (!requireNamespace("randomForest", quietly = TRUE)) {
    stop("Package 'randomForest' needed but is not available.")
  }
  
  # Predict class and probabilities
  cls <- predict(object$rfres, newdata = proj_df, type = "response", ...)
  prob <- predict(object$rfres, newdata = proj_df, type = "prob", ...)
  
  list(class=cls, prob=prob)
}


#' Evaluate Feature Importance for a Classifier
#'
#' Estimates the importance of features or blocks of features for the classification performance
#' using either a "marginal" (leave-one-block-out) or "standalone" (use-only-one-block) approach.
#'
#' Importance is measured by the change in a performance metric (`fun`) when features are
#' removed (marginal) or used exclusively (standalone).
#'
#' @inheritParams predict.classifier
#' @param x A fitted `classifier` object.
#' @param new_data The data matrix used for evaluating importance (typically validation or test data).
#' @param true_labels The true class labels corresponding to the rows of `new_data`.
#' @param blocks A list where each element is a numeric vector of feature indices (columns in the original
#'   data space) defining a block. If `NULL`, each feature is treated as its own block.
#' @param fun A function to compute the performance metric (e.g., `rank_score`, `topk`, or a custom function).
#'   The function should take a probability matrix and observed labels and return a data frame
#'   where the first column is the metric value per observation.
#' @param fun_direction Character string, either "lower_is_better" or "higher_is_better", indicating
#'   whether lower or higher values of the metric calculated by `fun` signify better performance.
#'   This is used to interpret the importance score correctly.
#' @param approach Character string: "marginal" (calculates importance as change from baseline when block is removed)
#'   or "standalone" (calculates importance as performance using only the block).
#' @param ... Additional arguments passed to `predict.classifier` during internal predictions.
#' @return A `data.frame` with columns `block` (character representation of feature indices in the block)
#'   and `importance` (numeric importance score). Higher importance values generally indicate more influential blocks,
#'   considering `fun_direction`.
#' @export
#' @family feature_importance classifier
#' @seealso \code{\link{rank_score}}, \code{\link{topk}}
#' @examples
#' # Assume clf is a fitted classifier object, dat is new data, true_lbls are correct labels for dat
#' # Assume blocks_list defines feature groups e.g., list(1:5, 6:10)
#' # feature_importance(clf, new_data = dat, true_labels = true_lbls, blocks = blocks_list)
feature_importance.classifier <- function(x, new_data, 
                                          true_labels, # Added true_labels argument
                                          ncomp = NULL,
                                          blocks = NULL, 
                                          metric = c("cosine", "euclidean", "ejaccard"), 
                                          fun = rank_score,
                                          fun_direction = c("lower_is_better", "higher_is_better"), 
                                          approach = c("marginal", "standalone"),
                                          ...) {
  metric <- match.arg(metric)
  approach <- match.arg(approach)
  fun_direction <- match.arg(fun_direction) 
  
  # Check true_labels length
  chk::chk_equal(nrow(new_data), length(true_labels))
  # Ensure true_labels are factor if not already, using levels from training data for consistency
  if (!is.factor(true_labels)) {
      true_labels <- factor(true_labels, levels = levels(x$labels))
  } else {
      # If already factor, ensure levels match training levels
      if (!identical(levels(true_labels), levels(x$labels))) {
          warning("Levels of provided 'true_labels' do not match training labels. Attempting to relevel.")
          true_labels <- factor(true_labels, levels = levels(x$labels))
      }
  }
  
  # Determine the full set of features available to the projector
  full_feature_indices <- 1:shape(x$projector)[1]
  
  if (is.null(blocks)) {
    # If blocks is NULL, treat each feature as a block
    blocks <- as.list(full_feature_indices)
  }
  
  # Check blocks validity
  all_block_indices <- unlist(blocks)
  if (any(all_block_indices <= 0) || any(all_block_indices > length(full_feature_indices))) {
      stop("Block indices are out of range for the features defined by the projector.")
  }
  if (any(duplicated(all_block_indices)) && approach == "marginal") {
      warning("Some features belong to multiple blocks; marginal importance calculation might be ambiguous.")
  }
  
  # Baseline performance with all features
  predict_args <- list(object = x, new_data = new_data, ncomp = ncomp,
                       colind = NULL, # Force use of all features for baseline
                       metric = metric, 
                       ...) 
  predict_args$normalize_probs <- NULL 
  base_pred <- do.call(predict, predict_args)
  
  # Use true_labels here
  base_accuracy <- fun(base_pred$prob, true_labels) 
  base_score <- mean(base_accuracy[,1], na.rm = TRUE)
  
  results <- lapply(seq_along(blocks), function(i) {
    block_indices_original <- blocks[[i]]
    current_score <- NA # Initialize
    importance <- NA # Initialize importance

    tryCatch({ 
        predict_args_block <- list(object = x, ncomp = ncomp,
                                   metric = metric,  
                                   ...) 
        predict_args_block$normalize_probs <- NULL
        
        if (approach == "marginal") {
          remaining_features_original <- setdiff(full_feature_indices, block_indices_original)
          
          if (length(remaining_features_original) == 0) {
            warning("Marginal approach: Removing block ", i, " (", paste(block_indices_original, collapse=","), ") leaves no features. Importance set to NA.")
            return(data.frame(block = paste(block_indices_original, collapse = ","), importance = NA))
          }
          
          subset_data <- new_data[, remaining_features_original, drop=FALSE]
          predict_args_block$new_data <- subset_data
          # Use the original indices for the remaining features when calling partial_project
          predict_args_block$colind <- remaining_features_original 
          
          preds <- do.call(predict, predict_args_block)
          # Use true_labels here
          accuracy <- fun(preds$prob, true_labels) 
          current_score <- mean(accuracy[,1], na.rm = TRUE)

          if (fun_direction == "lower_is_better") {
             importance <- current_score - base_score
          } else { 
             importance <- base_score - current_score
          }

        } else { # standalone
          if (length(block_indices_original) == 0) {
             warning("Standalone approach: Block ", i, " has no features. Importance set to NA.")
             return(data.frame(block = paste(block_indices_original, collapse = ","), importance = NA))
          }
          
          subset_data <- new_data[, block_indices_original, drop=FALSE]
          predict_args_block$new_data <- subset_data
          # Use the original indices for this block when calling partial_project
          predict_args_block$colind <- block_indices_original 
          
          preds <- do.call(predict, predict_args_block)
          # Use true_labels here
          accuracy <- fun(preds$prob, true_labels) 
          current_score <- mean(accuracy[,1], na.rm = TRUE)
          importance <- current_score 

           if (fun_direction == "lower_is_better") {
                importance <- 1 - current_score 
           }
           
        }
     }, error = function(e) {
        warning("Error calculating importance for block ", i, " (", paste(block_indices_original, collapse=","), "): ", e$message)
        importance <<- NA 
     }) 

    if (is.na(current_score) && is.na(importance)) {
         importance <- NA
    }
    
    data.frame(block = paste(block_indices_original, collapse = ","), importance = importance)
  })
  
  ret <- do.call(rbind, results)
  ret <- ret[order(ret$importance, decreasing = TRUE, na.last = TRUE), ]
  ret
}


#' Pretty Print Method for `classifier` Objects
#'
#' Display a human-readable summary of a `classifier` object.
#'
#' @param x A `classifier` object.
#' @param ... Additional arguments.
#' @return `classifier` object.
#' @export
#' @examples
#' # Assume clf is a fitted classifier object
#' # print(clf)
print.classifier <- function(x, ...) {
  cat("k-NN Classifier object:\n")
  cat(crayon::cyan("  k-NN Neighbors (k):"), x$knn, "\n")
  cat(crayon::cyan("  Number of Training Samples:"), nrow(x$scores), "\n")
  cat(crayon::cyan("  Number of Classes:"), length(levels(x$labels)), "\n")
  
  if (!is.null(x$colind)) {
    cat(crayon::cyan("  Default Feature Subset (colind):"), paste(x$colind, collapse=", "), "\n")
  }
  if (inherits(x, "multiblock_classifier") && !is.null(x$block)){
      cat(crayon::cyan("  Default Block Subset:"), x$block, "\n")
  }
  
  cat(crayon::cyan("  Underlying Projector Details:"), "\n")
  # Indent projector print output
  proj_output <- utils::capture.output(print(x$projector))
  cat(paste("    ", proj_output), sep = "\n")
  
  invisible(x)
}


#' Pretty Print Method for `rf_classifier` Objects
#'
#' Display a human-readable summary of an `rf_classifier` object.
#'
#' @param x An `rf_classifier` object.
#' @param ... Additional arguments passed to `print.randomForest`.
#' @return `rf_classifier` object.
#' @export
#' @examples
#' # Assume rf_clf is a fitted rf_classifier object
#' # print(rf_clf)
print.rf_classifier <- function(x, ...) {
  cat("Random Forest Classifier object:\n")
  
  # Print details about the underlying projector
  cat(crayon::cyan("  Underlying Projector Details:"), "\n")
  proj_output <- utils::capture.output(print(x$projector))
  cat(paste("    ", proj_output), sep = "\n")
  
  # Print details about the Random Forest model
  cat(crayon::cyan("  Random Forest Model Details (from randomForest package):"), "\n")
  # Indent RF print output
  rf_output <- utils::capture.output(print(x$rfres, ...))
  cat(paste("    ", rf_output), sep = "\n")
  
  if (!is.null(x$colind)) {
    cat(crayon::cyan("  Default Feature Subset (colind) for Projection:"), paste(x$colind, collapse=", "), "\n")
  }
  
  invisible(x)
}
</file>

</files>
